{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-09 22:51:16.910495: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-09-09 22:51:16.935636: I tensorflow/core/platform/cpu_feature_guard.cc:183] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE3 SSE4.1 SSE4.2 AVX, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Please fix your imports. Module tensorflow.python.training.tracking.data_structures has been moved to tensorflow.python.trackable.data_structures. The old module will be deleted in version 2.11.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-09 22:51:17.940085: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:1013] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-09-09 22:51:17.940309: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:1013] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-09-09 22:51:17.940358: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:1013] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]: sparse_operation_kit is imported\n",
      "WARNING:tensorflow:Please fix your imports. Module tensorflow.python.training.tracking.base has been moved to tensorflow.python.trackable.base. The old module will be deleted in version 2.11.\n",
      "[SOK INFO] Import /usr/local/lib/python3.8/dist-packages/merlin_sok-1.2.0-py3.8-linux-x86_64.egg/sparse_operation_kit/lib/libsok_experiment.so\n",
      "[SOK INFO] Import /usr/local/lib/python3.8/dist-packages/merlin_sok-1.2.0-py3.8-linux-x86_64.egg/sparse_operation_kit/lib/libsok_experiment.so\n",
      "[SOK INFO] Initialize finished, communication tool: horovod\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-09 22:51:18.325273: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:1013] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-09-09 22:51:18.325402: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:1013] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-09-09 22:51:18.325449: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:1013] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-09-09 22:51:18.624341: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:1013] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-09-09 22:51:18.624449: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:1013] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-09-09 22:51:18.624502: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:1013] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-09-09 22:51:18.624535: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2023-09-09 22:51:18.624554: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1638] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6011 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import nvtabular as nvt\n",
    "import cudf\n",
    "import config\n",
    "import os\n",
    "# ignore warnings\n",
    "from merlin.core.dispatch import get_lib\n",
    "from merlin.schema.tags import Tags\n",
    "\n",
    "import time\n",
    "import merlin.models.tf as mm\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "from merlin.schema.tags import Tags\n",
    "from nvtabular.ops import *\n",
    "\n",
    "from merlin.schema.tags import Tags\n",
    "from merlin.io.dataset import Dataset\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = nvt.Dataset(os.path.join(config.data_processed_dir,'train.parquet'),engine=\"parquet\",  part_mem_fraction=0.1)\n",
    "valid = nvt.Dataset(os.path.join(config.data_processed_dir,'valid.parquet'),engine=\"parquet\", part_mem_fraction=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# customer_features = ['customer_id', 'age', 'FN', 'Active', 'club_member_status','fashion_news_frequency']\n",
    "customer_features = ['customer_id', 'age']\n",
    "article_features = ['article_id', 'product_code', 'product_type_no', 'graphical_appearance_no', 'colour_group_code', 'perceived_colour_value_id', 'perceived_colour_master_id', 'department_no', 'index_code', 'index_group_no', 'section_no', 'garment_group_no']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = cudf.read_parquet(os.path.join(config.data_raw_dir,'articles.parquet'))[article_features]\n",
    "customers = cudf.read_parquet(os.path.join(config.data_processed_dir,'customers_enc.parquet'))[customer_features]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = nvt.Dataset(articles)\n",
    "customers = nvt.Dataset(customers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8231156 4453834\n"
     ]
    }
   ],
   "source": [
    "train = Dataset.merge(train, articles, on=\"article_id\", how=\"inner\")\n",
    "train = Dataset.merge(train, customers, on=\"customer_id\", how=\"inner\")\n",
    "train.to_parquet(os.path.join(config.data_processed_dir,'transformer4rec', 'train.parquet'))\n",
    "valid = Dataset.merge(valid, articles, on=\"article_id\", how=\"inner\")\n",
    "valid = Dataset.merge(valid, customers, on=\"customer_id\", how=\"inner\")\n",
    "valid.to_parquet(os.path.join(config.data_processed_dir,'transformer4rec', 'valid.parquet'))\n",
    "print(len(train.to_ddf()), len(valid.to_ddf()))\n",
    "del articles, customers, train, valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ['t_dat', 'customer_id', 'article_id', 'price', 'sales_channel_id','week']\n",
    "\n",
    "article_id = [\"article_id\"] >> Categorify() \n",
    "customer_id = [\"customer_id\"] >> Categorify() >> TagAsUserID()\n",
    "price = [\"price\"] >> LogOp() >> Normalize() >> TagAsItemFeatures() \n",
    "week_split = [\"week\"] \n",
    "time_features = [\"t_dat\"]\n",
    "session_time = (\n",
    "    time_features >> \n",
    "    nvt.ops.LambdaOp(lambda col: cudf.to_datetime(col, unit='s')) >> \n",
    "    nvt.ops.Rename(name = 'event_time_dt')\n",
    ")\n",
    "\n",
    "day = (session_time\n",
    "        >> nvt.ops.LambdaOp(lambda col: col.dt.day)\n",
    "        >> nvt.ops.Rename(name = 'day')\n",
    "        >> Categorify()\n",
    "        >> TagAsUserFeatures())\n",
    "week = (session_time\n",
    "        >> nvt.ops.LambdaOp(lambda col: col.dt.weekday)\n",
    "        >> nvt.ops.Rename(name = 'week_day')\n",
    "        >> Categorify()\n",
    "        >> TagAsUserFeatures())\n",
    "month = (session_time\n",
    "        >> nvt.ops.LambdaOp(lambda col: col.dt.month)\n",
    "        >> nvt.ops.Rename(name = 'month')\n",
    "        >> Categorify()\n",
    "        >> TagAsUserFeatures())\n",
    "year = (session_time\n",
    "        >> nvt.ops.LambdaOp(lambda col: col.dt.year)\n",
    "        >> nvt.ops.Rename(name = 'year')\n",
    "        >> Categorify()\n",
    "        >> TagAsUserFeatures())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "age =  ['age'] >>  LogOp() >> Normalize() >> TagAsUserFeatures() >> AddTags([Tags.USER,Tags.CONTINUOUS])\n",
    "# cat_customer_columns = (['FN', 'Active', 'club_member_status','fashion_news_frequency'] \n",
    "#             >> Categorify() \n",
    "#             >> TagAsUserFeatures()\n",
    "#             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_article_columns = (article_features \n",
    "              >> Categorify()  \n",
    "              >> TagAsItemFeatures())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "SESSIONS_MAX_LENGTH =30\n",
    "SESSIONS_MIN_LENGTH = 2\n",
    "\n",
    "filtered_articles = (article_id + customer_id + ['t_dat'] + price + day + week + month + year + age  + cat_article_columns + week_split\n",
    "                        >> Filter(f=lambda df: df[\"article_id\"]!=0)\n",
    ")\n",
    "groupby_features = (filtered_articles\n",
    "                    >> Groupby(\n",
    "                        groupby_cols=['customer_id', 'week'],\n",
    "                        aggs={\n",
    "                            'article_id': ['list', 'count'],\n",
    "                            'price': ['list','mean', 'std', 'max'],\n",
    "                            'product_code': ['list'],\n",
    "                            'product_type_no': ['list'],\n",
    "                            'graphical_appearance_no': ['list'],\n",
    "                            'colour_group_code': ['list'],\n",
    "                            'perceived_colour_value_id': ['list'],\n",
    "                            'perceived_colour_master_id': ['list'],\n",
    "                            'department_no': ['list'],\n",
    "                            'index_code': ['list'],\n",
    "                            'index_group_no': ['list'],\n",
    "                            'section_no': ['list'],\n",
    "                            'garment_group_no': ['list'],\n",
    "                            \n",
    "                            'day': ['list'],\n",
    "                            'week_day': ['list'],\n",
    "                            'month': ['list'],\n",
    "                            'year': ['list'],\n",
    "                        },\n",
    "                        sort_cols=[\"t_dat\"],\n",
    "                        name_sep=\"_\"\n",
    "                    )\n",
    "                )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_columns = ['price_list',\n",
    " 'day_list',\n",
    " 'week_day_list',\n",
    " 'month_list',\n",
    " 'year_list',\n",
    " 'product_code_list',\n",
    " 'product_type_no_list',\n",
    " 'graphical_appearance_no_list',\n",
    " 'colour_group_code_list',\n",
    " 'perceived_colour_value_id_list',\n",
    " 'perceived_colour_master_id_list',\n",
    " 'department_no_list',\n",
    " 'index_code_list',\n",
    " 'index_group_no_list',\n",
    " 'section_no_list',\n",
    " 'garment_group_no_list']\n",
    "\n",
    "groupby_features_articles_id= (groupby_features['article_id_list'] \n",
    "                         >> AddTags([Tags.ITEM, Tags.ITEM_ID, Tags.SEQUENCE])\n",
    "                        )\n",
    "groupby_features_articles_features= (groupby_features[list_columns]\n",
    "                         >> AddTags([Tags.ITEM, Tags.SEQUENCE])\n",
    "                        )\n",
    "truncated_features = (groupby_features_articles_id + groupby_features_articles_features\n",
    "                      >> ListSlice(-SESSIONS_MAX_LENGTH) \n",
    "                     )\n",
    "stat_features = (groupby_features['article_id_count','price_max','price_mean','price_std']\n",
    "               >> LogOp()\n",
    "               >> Normalize()\n",
    "               >> Rename(postfix=\"_norm\")\n",
    "               >> AddTags([Tags.ITEM,Tags.CONTINUOUS])\n",
    "              )\n",
    "\n",
    "output = (groupby_features['customer_id',\"article_id_count\"] + truncated_features + stat_features + week_split\n",
    "                     >> Filter(f=lambda df: df[\"article_id_count\"] >= SESSIONS_MIN_LENGTH)\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8231156 4453834\n"
     ]
    }
   ],
   "source": [
    "train = nvt.Dataset(os.path.join(config.data_processed_dir,'transformer4rec', 'train.parquet'),engine=\"parquet\",)\n",
    "valid = nvt.Dataset(os.path.join(config.data_processed_dir,'transformer4rec', 'valid.parquet'),engine=\"parquet\")\n",
    "print(len(train.to_ddf()), len(valid.to_ddf()))\n",
    "wf = nvt.Workflow(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1576278 868810\n"
     ]
    }
   ],
   "source": [
    "train = wf.fit_transform(train)\n",
    "valid = wf.transform(valid)\n",
    "print(len(train.to_ddf()), len(valid.to_ddf()))\n",
    "# train.regenerate_dataset(os.path.join(config.data_final,'transformer4rec', 'train.parquet'),part_size=\"500MiB\", file_size=\"500MiB\")\n",
    "# valid.regenerate_dataset(os.path.join(config.data_final,'transformer4rec', 'valid.parquet'),part_size=\"500MiB\", file_size=\"500MiB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_parquet(os.path.join(config.data_final,'transformer4rec', 'train.parquet'))\n",
    "valid.to_parquet(os.path.join(config.data_final,'transformer4rec', 'valid.parquet'))\n",
    "wf.save(os.path.join(config.data_final,'transformer4rec', 'workflow'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating time-based splits: 100%|██████████| 8/8 [00:03<00:00,  2.57it/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers4rec.utils.data_utils import save_time_based_splits\n",
    "save_time_based_splits(train,\n",
    "                       output_dir=os.path.join(config.data_final,'time'),\n",
    "                       partition_col='week',\n",
    "                       timestamp_col='customer_id', \n",
    "                      )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
