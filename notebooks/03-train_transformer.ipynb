{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-07 16:19:43.755842: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-09-07 16:19:43.779000: I tensorflow/core/platform/cpu_feature_guard.cc:183] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE3 SSE4.1 SSE4.2 AVX, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Please fix your imports. Module tensorflow.python.training.tracking.data_structures has been moved to tensorflow.python.trackable.data_structures. The old module will be deleted in version 2.11.\n",
      "[INFO]: sparse_operation_kit is imported\n",
      "WARNING:tensorflow:Please fix your imports. Module tensorflow.python.training.tracking.base has been moved to tensorflow.python.trackable.base. The old module will be deleted in version 2.11.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-07 16:19:46.207493: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:1013] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-09-07 16:19:46.207735: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:1013] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-09-07 16:19:46.207780: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:1013] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SOK INFO] Import /usr/local/lib/python3.8/dist-packages/merlin_sok-1.2.0-py3.8-linux-x86_64.egg/sparse_operation_kit/lib/libsok_experiment.so\n",
      "[SOK INFO] Import /usr/local/lib/python3.8/dist-packages/merlin_sok-1.2.0-py3.8-linux-x86_64.egg/sparse_operation_kit/lib/libsok_experiment.so\n",
      "[SOK INFO] Initialize finished, communication tool: horovod\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-07 16:19:46.419221: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:1013] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-09-07 16:19:46.419311: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:1013] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-09-07 16:19:46.419352: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:1013] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-09-07 16:19:46.427324: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:1013] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-09-07 16:19:46.427406: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:1013] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-09-07 16:19:46.427449: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:1013] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-09-07 16:19:46.427481: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2023-09-07 16:19:46.427486: I tensorflow/core/common_runtime/gpu/gpu_process_state.cc:226] Using CUDA malloc Async allocator for GPU: 0\n",
      "2023-09-07 16:19:46.427555: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1638] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6011 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
      "/usr/local/lib/python3.8/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"TF_GPU_ALLOCATOR\"]=\"cuda_malloc_async\"\n",
    "import gc\n",
    "import numpy as np\n",
    "import config\n",
    "import tensorflow as tf\n",
    "from merlin.schema.tags import Tags\n",
    "from merlin.io.dataset import Dataset\n",
    "import merlin.models.tf as mm\n",
    "from merlin.models.tf.core.aggregation import SequenceAggregator\n",
    "\n",
    "\n",
    "dmodel = 64\n",
    "BATCH_SIZE = 1024\n",
    "LEARNING_RATE = 0.005\n",
    "TOP = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = Dataset(os.path.join(config.data_final,'transformer4rec', 'train.parquet/*.parquet'),engine=\"parquet\",  part_size=\"1GB\")\n",
    "valid = Dataset(os.path.join(config.data_final,'transformer4rec', 'valid.parquet/*.parquet'),engine=\"parquet\", part_size=\"1GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_schema = train.schema.select_by_tag(Tags.ITEM_ID).column_names[0]\n",
    "\n",
    "\n",
    "train.schema = train.schema.select_by_tag(Tags.SEQUENCE).select_by_tag(Tags.CATEGORICAL)\n",
    "valid.schema = train.schema\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'article_id_list'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target = train.schema.select_by_tag(Tags.ITEM_ID).column_names[0]\n",
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_block = mm.InputBlockV2(\n",
    "    train.schema,    \n",
    "    embeddings=mm.Embeddings(\n",
    "        train.schema, \n",
    "        sequence_combiner=None,\n",
    "        dim=dmodel\n",
    "        ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, None, 1280)\n"
     ]
    }
   ],
   "source": [
    "batch = mm.sample_batch(train, batch_size=BATCH_SIZE, include_targets=False)\n",
    "print(input_block(batch).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([28, None, 1280])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_block(batch).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_block = mm.MLPBlock(\n",
    "                [128,dmodel],\n",
    "                activation='relu',\n",
    "                no_activation_last_layer=True,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-07 16:19:47.840828: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:655] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TensorShape([28, None, 64])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_block(input_block(batch)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "xlnet_block = mm.XLNetBlock(d_model=dmodel, n_head=4, n_layer=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_block = mm.SequentialBlock(\n",
    "    input_block,\n",
    "    mlp_block,\n",
    "    xlnet_block\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "article_id\n"
     ]
    }
   ],
   "source": [
    "item_id_name = train.schema.select_by_tag(Tags.ITEM_ID).first.properties['domain']['name']\n",
    "print(item_id_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_task = mm.CategoricalOutput(\n",
    "    to_call=input_block[\"categorical\"][item_id_name],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_block2 = mm.MLPBlock(\n",
    "                [128,dmodel],\n",
    "                activation='relu',\n",
    "                no_activation_last_layer=True,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_transformer = mm.Model(dense_block, mlp_block2, prediction_task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(\n",
    "    learning_rate=LEARNING_RATE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-07 16:21:23.356687: I tensorflow/core/common_runtime/executor.cc:1209] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n",
      "\t [[{{node Placeholder/_0}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['model/mask_emb:0', 'transformer/layer_._0/rel_attn/r_s_bias:0', 'transformer/layer_._0/rel_attn/seg_embed:0', 'transformer/layer_._1/rel_attn/r_s_bias:0', 'transformer/layer_._1/rel_attn/seg_embed:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['model/mask_emb:0', 'transformer/layer_._0/rel_attn/r_s_bias:0', 'transformer/layer_._0/rel_attn/seg_embed:0', 'transformer/layer_._1/rel_attn/r_s_bias:0', 'transformer/layer_._1/rel_attn/seg_embed:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "1/1 [==============================] - 4s 4s/step - loss: 10.6722 - recall_at_10: 0.0282 - mrr_at_10: 0.0115 - ndcg_at_10: 0.0154 - map_at_10: 0.0115 - precision_at_10: 0.0028 - regularization_loss: 0.0000e+00 - loss_batch: 10.6722\n",
      "Epoch 2/30\n",
      "1/1 [==============================] - 0s 176ms/step - loss: 9.9656 - recall_at_10: 0.0865 - mrr_at_10: 0.0428 - ndcg_at_10: 0.0530 - map_at_10: 0.0428 - precision_at_10: 0.0086 - regularization_loss: 0.0000e+00 - loss_batch: 9.9656\n",
      "Epoch 3/30\n",
      "1/1 [==============================] - 0s 180ms/step - loss: 8.7319 - recall_at_10: 0.0846 - mrr_at_10: 0.0386 - ndcg_at_10: 0.0494 - map_at_10: 0.0386 - precision_at_10: 0.0085 - regularization_loss: 0.0000e+00 - loss_batch: 8.7319\n",
      "Epoch 4/30\n",
      "1/1 [==============================] - 0s 178ms/step - loss: 7.0438 - recall_at_10: 0.0620 - mrr_at_10: 0.0364 - ndcg_at_10: 0.0424 - map_at_10: 0.0364 - precision_at_10: 0.0062 - regularization_loss: 0.0000e+00 - loss_batch: 7.0438\n",
      "Epoch 5/30\n",
      "1/1 [==============================] - 0s 185ms/step - loss: 6.1332 - recall_at_10: 0.0677 - mrr_at_10: 0.0343 - ndcg_at_10: 0.0422 - map_at_10: 0.0343 - precision_at_10: 0.0068 - regularization_loss: 0.0000e+00 - loss_batch: 6.1332\n",
      "Epoch 6/30\n",
      "1/1 [==============================] - 0s 203ms/step - loss: 5.8598 - recall_at_10: 0.1335 - mrr_at_10: 0.0537 - ndcg_at_10: 0.0725 - map_at_10: 0.0537 - precision_at_10: 0.0133 - regularization_loss: 0.0000e+00 - loss_batch: 5.8598\n",
      "Epoch 7/30\n",
      "1/1 [==============================] - 0s 183ms/step - loss: 5.7858 - recall_at_10: 0.1297 - mrr_at_10: 0.0671 - ndcg_at_10: 0.0816 - map_at_10: 0.0671 - precision_at_10: 0.0130 - regularization_loss: 0.0000e+00 - loss_batch: 5.7858\n",
      "Epoch 8/30\n",
      "1/1 [==============================] - 0s 176ms/step - loss: 5.8419 - recall_at_10: 0.1316 - mrr_at_10: 0.0699 - ndcg_at_10: 0.0840 - map_at_10: 0.0699 - precision_at_10: 0.0132 - regularization_loss: 0.0000e+00 - loss_batch: 5.8419\n",
      "Epoch 9/30\n",
      "1/1 [==============================] - 0s 183ms/step - loss: 5.8002 - recall_at_10: 0.1241 - mrr_at_10: 0.0778 - ndcg_at_10: 0.0887 - map_at_10: 0.0778 - precision_at_10: 0.0124 - regularization_loss: 0.0000e+00 - loss_batch: 5.8002\n",
      "Epoch 10/30\n",
      "1/1 [==============================] - 0s 172ms/step - loss: 5.6481 - recall_at_10: 0.1579 - mrr_at_10: 0.0767 - ndcg_at_10: 0.0960 - map_at_10: 0.0767 - precision_at_10: 0.0158 - regularization_loss: 0.0000e+00 - loss_batch: 5.6481\n",
      "Epoch 11/30\n",
      "1/1 [==============================] - 0s 179ms/step - loss: 5.5731 - recall_at_10: 0.1805 - mrr_at_10: 0.0885 - ndcg_at_10: 0.1100 - map_at_10: 0.0885 - precision_at_10: 0.0180 - regularization_loss: 0.0000e+00 - loss_batch: 5.5731\n",
      "Epoch 12/30\n",
      "1/1 [==============================] - 0s 171ms/step - loss: 5.4444 - recall_at_10: 0.1917 - mrr_at_10: 0.1022 - ndcg_at_10: 0.1233 - map_at_10: 0.1022 - precision_at_10: 0.0192 - regularization_loss: 0.0000e+00 - loss_batch: 5.4444\n",
      "Epoch 13/30\n",
      "1/1 [==============================] - 0s 179ms/step - loss: 5.3302 - recall_at_10: 0.2011 - mrr_at_10: 0.1107 - ndcg_at_10: 0.1322 - map_at_10: 0.1107 - precision_at_10: 0.0201 - regularization_loss: 0.0000e+00 - loss_batch: 5.3302\n",
      "Epoch 14/30\n",
      "1/1 [==============================] - 0s 187ms/step - loss: 5.1841 - recall_at_10: 0.2218 - mrr_at_10: 0.1153 - ndcg_at_10: 0.1406 - map_at_10: 0.1153 - precision_at_10: 0.0222 - regularization_loss: 0.0000e+00 - loss_batch: 5.1841\n",
      "Epoch 15/30\n",
      "1/1 [==============================] - 0s 479ms/step - loss: 5.0143 - recall_at_10: 0.2519 - mrr_at_10: 0.1181 - ndcg_at_10: 0.1494 - map_at_10: 0.1181 - precision_at_10: 0.0252 - regularization_loss: 0.0000e+00 - loss_batch: 5.0143\n",
      "Epoch 16/30\n",
      "1/1 [==============================] - 0s 177ms/step - loss: 4.9160 - recall_at_10: 0.2669 - mrr_at_10: 0.1274 - ndcg_at_10: 0.1596 - map_at_10: 0.1274 - precision_at_10: 0.0267 - regularization_loss: 0.0000e+00 - loss_batch: 4.9160\n",
      "Epoch 17/30\n",
      "1/1 [==============================] - 0s 171ms/step - loss: 4.7602 - recall_at_10: 0.2970 - mrr_at_10: 0.1493 - ndcg_at_10: 0.1839 - map_at_10: 0.1493 - precision_at_10: 0.0297 - regularization_loss: 0.0000e+00 - loss_batch: 4.7602\n",
      "Epoch 18/30\n",
      "1/1 [==============================] - 0s 184ms/step - loss: 4.6044 - recall_at_10: 0.3252 - mrr_at_10: 0.1645 - ndcg_at_10: 0.2021 - map_at_10: 0.1645 - precision_at_10: 0.0325 - regularization_loss: 0.0000e+00 - loss_batch: 4.6044\n",
      "Epoch 19/30\n",
      "1/1 [==============================] - 0s 182ms/step - loss: 4.4445 - recall_at_10: 0.3872 - mrr_at_10: 0.1807 - ndcg_at_10: 0.2285 - map_at_10: 0.1807 - precision_at_10: 0.0387 - regularization_loss: 0.0000e+00 - loss_batch: 4.4445\n",
      "Epoch 20/30\n",
      "1/1 [==============================] - 0s 186ms/step - loss: 4.2590 - recall_at_10: 0.4229 - mrr_at_10: 0.2144 - ndcg_at_10: 0.2631 - map_at_10: 0.2144 - precision_at_10: 0.0423 - regularization_loss: 0.0000e+00 - loss_batch: 4.2590\n",
      "Epoch 21/30\n",
      "1/1 [==============================] - 0s 184ms/step - loss: 4.0774 - recall_at_10: 0.4699 - mrr_at_10: 0.2360 - ndcg_at_10: 0.2909 - map_at_10: 0.2360 - precision_at_10: 0.0470 - regularization_loss: 0.0000e+00 - loss_batch: 4.0774\n",
      "Epoch 22/30\n",
      "1/1 [==============================] - 0s 168ms/step - loss: 3.9121 - recall_at_10: 0.5282 - mrr_at_10: 0.2448 - ndcg_at_10: 0.3103 - map_at_10: 0.2448 - precision_at_10: 0.0528 - regularization_loss: 0.0000e+00 - loss_batch: 3.9121\n",
      "Epoch 23/30\n",
      "1/1 [==============================] - 0s 167ms/step - loss: 3.7291 - recall_at_10: 0.5789 - mrr_at_10: 0.2834 - ndcg_at_10: 0.3527 - map_at_10: 0.2834 - precision_at_10: 0.0579 - regularization_loss: 0.0000e+00 - loss_batch: 3.7291\n",
      "Epoch 24/30\n",
      "1/1 [==============================] - 0s 175ms/step - loss: 3.5265 - recall_at_10: 0.6729 - mrr_at_10: 0.3409 - ndcg_at_10: 0.4187 - map_at_10: 0.3409 - precision_at_10: 0.0673 - regularization_loss: 0.0000e+00 - loss_batch: 3.5265\n",
      "Epoch 25/30\n",
      "1/1 [==============================] - 0s 176ms/step - loss: 3.3410 - recall_at_10: 0.7105 - mrr_at_10: 0.3773 - ndcg_at_10: 0.4562 - map_at_10: 0.3773 - precision_at_10: 0.0711 - regularization_loss: 0.0000e+00 - loss_batch: 3.3410\n",
      "Epoch 26/30\n",
      "1/1 [==============================] - 0s 174ms/step - loss: 3.1640 - recall_at_10: 0.7744 - mrr_at_10: 0.4081 - ndcg_at_10: 0.4942 - map_at_10: 0.4081 - precision_at_10: 0.0774 - regularization_loss: 0.0000e+00 - loss_batch: 3.1640\n",
      "Epoch 27/30\n",
      "1/1 [==============================] - 0s 180ms/step - loss: 2.9787 - recall_at_10: 0.8158 - mrr_at_10: 0.4504 - ndcg_at_10: 0.5371 - map_at_10: 0.4504 - precision_at_10: 0.0816 - regularization_loss: 0.0000e+00 - loss_batch: 2.9787\n",
      "Epoch 28/30\n",
      "1/1 [==============================] - 0s 172ms/step - loss: 2.7836 - recall_at_10: 0.8553 - mrr_at_10: 0.4817 - ndcg_at_10: 0.5705 - map_at_10: 0.4817 - precision_at_10: 0.0855 - regularization_loss: 0.0000e+00 - loss_batch: 2.7836\n",
      "Epoch 29/30\n",
      "1/1 [==============================] - 0s 177ms/step - loss: 2.6482 - recall_at_10: 0.8703 - mrr_at_10: 0.5131 - ndcg_at_10: 0.5989 - map_at_10: 0.5131 - precision_at_10: 0.0870 - regularization_loss: 0.0000e+00 - loss_batch: 2.6482\n",
      "Epoch 30/30\n",
      "1/1 [==============================] - 0s 181ms/step - loss: 2.4057 - recall_at_10: 0.9305 - mrr_at_10: 0.5542 - ndcg_at_10: 0.6450 - map_at_10: 0.5542 - precision_at_10: 0.0930 - regularization_loss: 0.0000e+00 - loss_batch: 2.4057\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f00646aa520>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model_transformer.compile(run_eagerly=False, optimizer=optimizer, loss=\"categorical_crossentropy\",\n",
    "              metrics=mm.TopKMetricsAggregator.default_metrics(top_ks=[10])\n",
    "             )\n",
    "model_transformer.fit(train, batch_size=512, epochs=30, pre=mm.SequencePredictNext(schema=train.schema, target=target, transformer=xlnet_block))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-07 16:17:18.018682: I tensorflow/core/common_runtime/executor.cc:1209] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n",
      "\t [[{{node Placeholder/_0}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 4s 4s/step - loss: 1.6604 - recall_at_10: 0.9605 - mrr_at_10: 0.8917 - ndcg_at_10: 0.9096 - map_at_10: 0.8917 - precision_at_10: 0.0961 - regularization_loss: 0.0000e+00 - loss_batch: 1.6604\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'loss': 1.6604058742523193,\n",
       " 'recall_at_10': 0.9605262875556946,\n",
       " 'mrr_at_10': 0.8917292952537537,\n",
       " 'ndcg_at_10': 0.9095906019210815,\n",
       " 'map_at_10': 0.8917292952537537,\n",
       " 'precision_at_10': 0.0960526391863823,\n",
       " 'regularization_loss': 0.0,\n",
       " 'loss_batch': 1.6604058742523193}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_transformer.evaluate(\n",
    "    valid,\n",
    "    batch_size=1024,\n",
    "    pre=mm.SequencePredictNext(schema=train.schema, target=target, transformer=xlnet_block),\n",
    "    return_dict=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Column price_min does not exist in schema'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train\u001b[39m.\u001b[39;49mto_ddf()\u001b[39m.\u001b[39;49mcompute()\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/dask/base.py:314\u001b[0m, in \u001b[0;36mDaskMethodsMixin.compute\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    290\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcompute\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    291\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Compute this dask collection\u001b[39;00m\n\u001b[1;32m    292\u001b[0m \n\u001b[1;32m    293\u001b[0m \u001b[39m    This turns a lazy Dask collection into its in-memory equivalent.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    312\u001b[0m \u001b[39m    dask.base.compute\u001b[39;00m\n\u001b[1;32m    313\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 314\u001b[0m     (result,) \u001b[39m=\u001b[39m compute(\u001b[39mself\u001b[39;49m, traverse\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    315\u001b[0m     \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/dask/base.py:599\u001b[0m, in \u001b[0;36mcompute\u001b[0;34m(traverse, optimize_graph, scheduler, get, *args, **kwargs)\u001b[0m\n\u001b[1;32m    596\u001b[0m     keys\u001b[39m.\u001b[39mappend(x\u001b[39m.\u001b[39m__dask_keys__())\n\u001b[1;32m    597\u001b[0m     postcomputes\u001b[39m.\u001b[39mappend(x\u001b[39m.\u001b[39m__dask_postcompute__())\n\u001b[0;32m--> 599\u001b[0m results \u001b[39m=\u001b[39m schedule(dsk, keys, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    600\u001b[0m \u001b[39mreturn\u001b[39;00m repack([f(r, \u001b[39m*\u001b[39ma) \u001b[39mfor\u001b[39;00m r, (f, a) \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(results, postcomputes)])\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/dask/threaded.py:89\u001b[0m, in \u001b[0;36mget\u001b[0;34m(dsk, keys, cache, num_workers, pool, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(pool, multiprocessing\u001b[39m.\u001b[39mpool\u001b[39m.\u001b[39mPool):\n\u001b[1;32m     87\u001b[0m         pool \u001b[39m=\u001b[39m MultiprocessingPoolExecutor(pool)\n\u001b[0;32m---> 89\u001b[0m results \u001b[39m=\u001b[39m get_async(\n\u001b[1;32m     90\u001b[0m     pool\u001b[39m.\u001b[39;49msubmit,\n\u001b[1;32m     91\u001b[0m     pool\u001b[39m.\u001b[39;49m_max_workers,\n\u001b[1;32m     92\u001b[0m     dsk,\n\u001b[1;32m     93\u001b[0m     keys,\n\u001b[1;32m     94\u001b[0m     cache\u001b[39m=\u001b[39;49mcache,\n\u001b[1;32m     95\u001b[0m     get_id\u001b[39m=\u001b[39;49m_thread_get_id,\n\u001b[1;32m     96\u001b[0m     pack_exception\u001b[39m=\u001b[39;49mpack_exception,\n\u001b[1;32m     97\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m     98\u001b[0m )\n\u001b[1;32m    100\u001b[0m \u001b[39m# Cleanup pools associated to dead threads\u001b[39;00m\n\u001b[1;32m    101\u001b[0m \u001b[39mwith\u001b[39;00m pools_lock:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/dask/local.py:511\u001b[0m, in \u001b[0;36mget_async\u001b[0;34m(submit, num_workers, dsk, result, cache, get_id, rerun_exceptions_locally, pack_exception, raise_exception, callbacks, dumps, loads, chunksize, **kwargs)\u001b[0m\n\u001b[1;32m    509\u001b[0m         _execute_task(task, data)  \u001b[39m# Re-execute locally\u001b[39;00m\n\u001b[1;32m    510\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 511\u001b[0m         raise_exception(exc, tb)\n\u001b[1;32m    512\u001b[0m res, worker_id \u001b[39m=\u001b[39m loads(res_info)\n\u001b[1;32m    513\u001b[0m state[\u001b[39m\"\u001b[39m\u001b[39mcache\u001b[39m\u001b[39m\"\u001b[39m][key] \u001b[39m=\u001b[39m res\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/dask/local.py:319\u001b[0m, in \u001b[0;36mreraise\u001b[0;34m(exc, tb)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[39mif\u001b[39;00m exc\u001b[39m.\u001b[39m__traceback__ \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m tb:\n\u001b[1;32m    318\u001b[0m     \u001b[39mraise\u001b[39;00m exc\u001b[39m.\u001b[39mwith_traceback(tb)\n\u001b[0;32m--> 319\u001b[0m \u001b[39mraise\u001b[39;00m exc\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/dask/local.py:224\u001b[0m, in \u001b[0;36mexecute_task\u001b[0;34m(key, task_info, dumps, loads, get_id, pack_exception)\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    223\u001b[0m     task, data \u001b[39m=\u001b[39m loads(task_info)\n\u001b[0;32m--> 224\u001b[0m     result \u001b[39m=\u001b[39m _execute_task(task, data)\n\u001b[1;32m    225\u001b[0m     \u001b[39mid\u001b[39m \u001b[39m=\u001b[39m get_id()\n\u001b[1;32m    226\u001b[0m     result \u001b[39m=\u001b[39m dumps((result, \u001b[39mid\u001b[39m))\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/dask/core.py:119\u001b[0m, in \u001b[0;36m_execute_task\u001b[0;34m(arg, cache, dsk)\u001b[0m\n\u001b[1;32m    115\u001b[0m     func, args \u001b[39m=\u001b[39m arg[\u001b[39m0\u001b[39m], arg[\u001b[39m1\u001b[39m:]\n\u001b[1;32m    116\u001b[0m     \u001b[39m# Note: Don't assign the subtask results to a variable. numpy detects\u001b[39;00m\n\u001b[1;32m    117\u001b[0m     \u001b[39m# temporaries by their reference count and can execute certain\u001b[39;00m\n\u001b[1;32m    118\u001b[0m     \u001b[39m# operations in-place.\u001b[39;00m\n\u001b[0;32m--> 119\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49m(_execute_task(a, cache) \u001b[39mfor\u001b[39;49;00m a \u001b[39min\u001b[39;49;00m args))\n\u001b[1;32m    120\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m ishashable(arg):\n\u001b[1;32m    121\u001b[0m     \u001b[39mreturn\u001b[39;00m arg\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/dask/optimization.py:990\u001b[0m, in \u001b[0;36mSubgraphCallable.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    988\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m==\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39minkeys):\n\u001b[1;32m    989\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mExpected \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m args, got \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39minkeys), \u001b[39mlen\u001b[39m(args)))\n\u001b[0;32m--> 990\u001b[0m \u001b[39mreturn\u001b[39;00m core\u001b[39m.\u001b[39;49mget(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdsk, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moutkey, \u001b[39mdict\u001b[39;49m(\u001b[39mzip\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minkeys, args)))\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/dask/core.py:149\u001b[0m, in \u001b[0;36mget\u001b[0;34m(dsk, out, cache)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m toposort(dsk):\n\u001b[1;32m    148\u001b[0m     task \u001b[39m=\u001b[39m dsk[key]\n\u001b[0;32m--> 149\u001b[0m     result \u001b[39m=\u001b[39m _execute_task(task, cache)\n\u001b[1;32m    150\u001b[0m     cache[key] \u001b[39m=\u001b[39m result\n\u001b[1;32m    151\u001b[0m result \u001b[39m=\u001b[39m _execute_task(out, cache)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/dask/core.py:119\u001b[0m, in \u001b[0;36m_execute_task\u001b[0;34m(arg, cache, dsk)\u001b[0m\n\u001b[1;32m    115\u001b[0m     func, args \u001b[39m=\u001b[39m arg[\u001b[39m0\u001b[39m], arg[\u001b[39m1\u001b[39m:]\n\u001b[1;32m    116\u001b[0m     \u001b[39m# Note: Don't assign the subtask results to a variable. numpy detects\u001b[39;00m\n\u001b[1;32m    117\u001b[0m     \u001b[39m# temporaries by their reference count and can execute certain\u001b[39;00m\n\u001b[1;32m    118\u001b[0m     \u001b[39m# operations in-place.\u001b[39;00m\n\u001b[0;32m--> 119\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49m(_execute_task(a, cache) \u001b[39mfor\u001b[39;49;00m a \u001b[39min\u001b[39;49;00m args))\n\u001b[1;32m    120\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m ishashable(arg):\n\u001b[1;32m    121\u001b[0m     \u001b[39mreturn\u001b[39;00m arg\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/dask/dataframe/io/parquet/core.py:97\u001b[0m, in \u001b[0;36mParquetFunctionWrapper.__call__\u001b[0;34m(self, part)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(part, \u001b[39mlist\u001b[39m):\n\u001b[1;32m     95\u001b[0m     part \u001b[39m=\u001b[39m [part]\n\u001b[0;32m---> 97\u001b[0m \u001b[39mreturn\u001b[39;00m read_parquet_part(\n\u001b[1;32m     98\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfs,\n\u001b[1;32m     99\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mengine,\n\u001b[1;32m    100\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmeta,\n\u001b[1;32m    101\u001b[0m     [\n\u001b[1;32m    102\u001b[0m         \u001b[39m# Temporary workaround for HLG serialization bug\u001b[39;49;00m\n\u001b[1;32m    103\u001b[0m         \u001b[39m# (see: https://github.com/dask/dask/issues/8581)\u001b[39;49;00m\n\u001b[1;32m    104\u001b[0m         (p\u001b[39m.\u001b[39;49mdata[\u001b[39m\"\u001b[39;49m\u001b[39mpiece\u001b[39;49m\u001b[39m\"\u001b[39;49m], p\u001b[39m.\u001b[39;49mdata\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mkwargs\u001b[39;49m\u001b[39m\"\u001b[39;49m, {}))\n\u001b[1;32m    105\u001b[0m         \u001b[39mif\u001b[39;49;00m \u001b[39mhasattr\u001b[39;49m(p, \u001b[39m\"\u001b[39;49m\u001b[39mdata\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m    106\u001b[0m         \u001b[39melse\u001b[39;49;00m (p[\u001b[39m\"\u001b[39;49m\u001b[39mpiece\u001b[39;49m\u001b[39m\"\u001b[39;49m], p\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mkwargs\u001b[39;49m\u001b[39m\"\u001b[39;49m, {}))\n\u001b[1;32m    107\u001b[0m         \u001b[39mfor\u001b[39;49;00m p \u001b[39min\u001b[39;49;00m part\n\u001b[1;32m    108\u001b[0m     ],\n\u001b[1;32m    109\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns,\n\u001b[1;32m    110\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindex,\n\u001b[1;32m    111\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49muse_nullable_dtypes,\n\u001b[1;32m    112\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcommon_kwargs,\n\u001b[1;32m    113\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/dask/dataframe/io/parquet/core.py:644\u001b[0m, in \u001b[0;36mread_parquet_part\u001b[0;34m(fs, engine, meta, part, columns, index, use_nullable_dtypes, kwargs)\u001b[0m\n\u001b[1;32m    641\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(part) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m \u001b[39mor\u001b[39;00m part[\u001b[39m0\u001b[39m][\u001b[39m1\u001b[39m] \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m check_multi_support(engine):\n\u001b[1;32m    642\u001b[0m     \u001b[39m# Part kwargs expected\u001b[39;00m\n\u001b[1;32m    643\u001b[0m     func \u001b[39m=\u001b[39m engine\u001b[39m.\u001b[39mread_partition\n\u001b[0;32m--> 644\u001b[0m     dfs \u001b[39m=\u001b[39m [\n\u001b[1;32m    645\u001b[0m         func(\n\u001b[1;32m    646\u001b[0m             fs,\n\u001b[1;32m    647\u001b[0m             rg,\n\u001b[1;32m    648\u001b[0m             columns\u001b[39m.\u001b[39mcopy(),\n\u001b[1;32m    649\u001b[0m             index,\n\u001b[1;32m    650\u001b[0m             use_nullable_dtypes\u001b[39m=\u001b[39muse_nullable_dtypes,\n\u001b[1;32m    651\u001b[0m             \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mtoolz\u001b[39m.\u001b[39mmerge(kwargs, kw),\n\u001b[1;32m    652\u001b[0m         )\n\u001b[1;32m    653\u001b[0m         \u001b[39mfor\u001b[39;00m (rg, kw) \u001b[39min\u001b[39;00m part\n\u001b[1;32m    654\u001b[0m     ]\n\u001b[1;32m    655\u001b[0m     df \u001b[39m=\u001b[39m concat(dfs, axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m) \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(dfs) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m \u001b[39melse\u001b[39;00m dfs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    656\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    657\u001b[0m     \u001b[39m# No part specific kwargs, let engine read\u001b[39;00m\n\u001b[1;32m    658\u001b[0m     \u001b[39m# list of parts at once\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/dask/dataframe/io/parquet/core.py:645\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    641\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(part) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m \u001b[39mor\u001b[39;00m part[\u001b[39m0\u001b[39m][\u001b[39m1\u001b[39m] \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m check_multi_support(engine):\n\u001b[1;32m    642\u001b[0m     \u001b[39m# Part kwargs expected\u001b[39;00m\n\u001b[1;32m    643\u001b[0m     func \u001b[39m=\u001b[39m engine\u001b[39m.\u001b[39mread_partition\n\u001b[1;32m    644\u001b[0m     dfs \u001b[39m=\u001b[39m [\n\u001b[0;32m--> 645\u001b[0m         func(\n\u001b[1;32m    646\u001b[0m             fs,\n\u001b[1;32m    647\u001b[0m             rg,\n\u001b[1;32m    648\u001b[0m             columns\u001b[39m.\u001b[39;49mcopy(),\n\u001b[1;32m    649\u001b[0m             index,\n\u001b[1;32m    650\u001b[0m             use_nullable_dtypes\u001b[39m=\u001b[39;49muse_nullable_dtypes,\n\u001b[1;32m    651\u001b[0m             \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mtoolz\u001b[39m.\u001b[39;49mmerge(kwargs, kw),\n\u001b[1;32m    652\u001b[0m         )\n\u001b[1;32m    653\u001b[0m         \u001b[39mfor\u001b[39;00m (rg, kw) \u001b[39min\u001b[39;00m part\n\u001b[1;32m    654\u001b[0m     ]\n\u001b[1;32m    655\u001b[0m     df \u001b[39m=\u001b[39m concat(dfs, axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m) \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(dfs) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m \u001b[39melse\u001b[39;00m dfs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    656\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    657\u001b[0m     \u001b[39m# No part specific kwargs, let engine read\u001b[39;00m\n\u001b[1;32m    658\u001b[0m     \u001b[39m# list of parts at once\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/merlin/io/parquet.py:127\u001b[0m, in \u001b[0;36mGPUParquetEngine.read_partition\u001b[0;34m(cls, fs, pieces, *args, **kwargs)\u001b[0m\n\u001b[1;32m    118\u001b[0m cudf_optimized_remote \u001b[39m=\u001b[39m (cudf_version\u001b[39m.\u001b[39mmajor, cudf_version\u001b[39m.\u001b[39mminor) \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m (\u001b[39m22\u001b[39m, \u001b[39m2\u001b[39m)\n\u001b[1;32m    119\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    120\u001b[0m     cudf_optimized_remote\n\u001b[1;32m    121\u001b[0m     \u001b[39mor\u001b[39;00m cudf\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mioutils\u001b[39m.\u001b[39m_is_local_filesystem(fs)\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    125\u001b[0m     \u001b[39m# or if the version of cudf is optimized for remote storage.\u001b[39;00m\n\u001b[1;32m    126\u001b[0m     \u001b[39m# We also fall back to cudf for multi-file aggregation.\u001b[39;00m\n\u001b[0;32m--> 127\u001b[0m     \u001b[39mreturn\u001b[39;00m CudfEngine\u001b[39m.\u001b[39;49mread_partition(fs, pieces, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    129\u001b[0m \u001b[39m# This version of cudf does not include optimized\u001b[39;00m\n\u001b[1;32m    130\u001b[0m \u001b[39m# fsspec usage for remote storage - Use custom code path.\u001b[39;00m\n\u001b[1;32m    131\u001b[0m \u001b[39m# TODO: Remove `_optimized_read_partition_remote` once the\u001b[39;00m\n\u001b[1;32m    132\u001b[0m \u001b[39m# earliest supported cudf version is 22.02\u001b[39;00m\n\u001b[1;32m    133\u001b[0m \u001b[39mreturn\u001b[39;00m _optimized_read_partition_remote(fs, pieces, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/dask_cudf/io/parquet.py:260\u001b[0m, in \u001b[0;36mCudfEngine.read_partition\u001b[0;34m(cls, fs, pieces, columns, index, categories, partitions, partitioning, schema, open_file_options, **kwargs)\u001b[0m\n\u001b[1;32m    257\u001b[0m df \u001b[39m=\u001b[39m cudf\u001b[39m.\u001b[39mconcat(dfs) \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(dfs) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m \u001b[39melse\u001b[39;00m dfs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    259\u001b[0m \u001b[39m# Re-set \"object\" dtypes align with pa schema\u001b[39;00m\n\u001b[0;32m--> 260\u001b[0m set_object_dtypes_from_pa_schema(df, schema)\n\u001b[1;32m    262\u001b[0m \u001b[39mif\u001b[39;00m index \u001b[39mand\u001b[39;00m (index[\u001b[39m0\u001b[39m] \u001b[39min\u001b[39;00m df\u001b[39m.\u001b[39mcolumns):\n\u001b[1;32m    263\u001b[0m     df \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39mset_index(index[\u001b[39m0\u001b[39m])\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/dask_cudf/io/parquet.py:403\u001b[0m, in \u001b[0;36mset_object_dtypes_from_pa_schema\u001b[0;34m(df, schema)\u001b[0m\n\u001b[1;32m    398\u001b[0m \u001b[39mif\u001b[39;00m col_name \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    399\u001b[0m     \u001b[39m# Pyarrow cannot handle `None` as a field name.\u001b[39;00m\n\u001b[1;32m    400\u001b[0m     \u001b[39m# However, this should be a simple range index that\u001b[39;00m\n\u001b[1;32m    401\u001b[0m     \u001b[39m# we can ignore anyway\u001b[39;00m\n\u001b[1;32m    402\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n\u001b[0;32m--> 403\u001b[0m typ \u001b[39m=\u001b[39m cudf_dtype_from_pa_type(schema\u001b[39m.\u001b[39;49mfield(col_name)\u001b[39m.\u001b[39mtype)\n\u001b[1;32m    404\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    405\u001b[0m     col_name \u001b[39min\u001b[39;00m schema\u001b[39m.\u001b[39mnames\n\u001b[1;32m    406\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(typ, (cudf\u001b[39m.\u001b[39mListDtype, cudf\u001b[39m.\u001b[39mStructDtype))\n\u001b[1;32m    407\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(col, cudf\u001b[39m.\u001b[39mcore\u001b[39m.\u001b[39mcolumn\u001b[39m.\u001b[39mStringColumn)\n\u001b[1;32m    408\u001b[0m ):\n\u001b[1;32m    409\u001b[0m     df\u001b[39m.\u001b[39m_data[col_name] \u001b[39m=\u001b[39m col\u001b[39m.\u001b[39mastype(typ)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pyarrow/types.pxi:1780\u001b[0m, in \u001b[0;36mpyarrow.lib.Schema.field\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Column price_min does not exist in schema'"
     ]
    }
   ],
   "source": [
    "train.to_ddf().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
