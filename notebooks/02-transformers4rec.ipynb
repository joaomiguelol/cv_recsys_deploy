{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-31 12:12:06.446844: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-08-31 12:12:06.470660: I tensorflow/core/platform/cpu_feature_guard.cc:183] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE3 SSE4.1 SSE4.2 AVX, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Please fix your imports. Module tensorflow.python.training.tracking.data_structures has been moved to tensorflow.python.trackable.data_structures. The old module will be deleted in version 2.11.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-31 12:12:07.458687: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:1013] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-08-31 12:12:07.458907: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:1013] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-08-31 12:12:07.458956: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:1013] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]: sparse_operation_kit is imported\n",
      "WARNING:tensorflow:Please fix your imports. Module tensorflow.python.training.tracking.base has been moved to tensorflow.python.trackable.base. The old module will be deleted in version 2.11.\n",
      "[SOK INFO] Import /usr/local/lib/python3.8/dist-packages/merlin_sok-1.2.0-py3.8-linux-x86_64.egg/sparse_operation_kit/lib/libsok_experiment.so\n",
      "[SOK INFO] Import /usr/local/lib/python3.8/dist-packages/merlin_sok-1.2.0-py3.8-linux-x86_64.egg/sparse_operation_kit/lib/libsok_experiment.so\n",
      "[SOK INFO] Initialize finished, communication tool: horovod\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-31 12:12:07.710936: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:1013] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-08-31 12:12:07.711056: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:1013] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-08-31 12:12:07.711111: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:1013] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-08-31 12:12:07.718824: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:1013] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-08-31 12:12:07.718920: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:1013] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-08-31 12:12:07.718969: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:1013] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-08-31 12:12:07.719002: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2023-08-31 12:12:07.719018: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1638] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6011 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import nvtabular as nvt\n",
    "import cudf\n",
    "import config\n",
    "import os\n",
    "# ignore warnings\n",
    "from merlin.core.dispatch import get_lib\n",
    "from merlin.schema.tags import Tags\n",
    "\n",
    "import time\n",
    "import merlin.models.tf as mm\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "from merlin.schema.tags import Tags\n",
    "from nvtabular.ops import *\n",
    "\n",
    "from merlin.schema.tags import Tags\n",
    "from merlin.io.dataset import Dataset\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = nvt.Dataset(os.path.join(config.data_processed_dir,'train', 'transactions.parquet'),engine=\"parquet\",  part_mem_fraction=0.1)\n",
    "valid = nvt.Dataset(os.path.join(config.data_processed_dir,'valid', 'transactions.parquet'),engine=\"parquet\", part_mem_fraction=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_features = ['customer_id', 'age', 'FN', 'Active', 'club_member_status','fashion_news_frequency']\n",
    "article_features = ['article_id', 'product_code', 'product_type_no', 'graphical_appearance_no', 'colour_group_code', 'perceived_colour_value_id', 'perceived_colour_master_id', 'department_no', 'index_code', 'index_group_no', 'section_no', 'garment_group_no']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = cudf.read_parquet(os.path.join(config.data_raw_dir,'articles.parquet'))[article_features]\n",
    "customers = cudf.read_parquet(os.path.join(config.data_raw_dir,'customers_enc.parquet'))[customer_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = nvt.Dataset(articles)\n",
    "customers = nvt.Dataset(customers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = Dataset.merge(train, articles, on=\"article_id\", how=\"inner\")\n",
    "train = Dataset.merge(train, customers, on=\"customer_id\", how=\"inner\")\n",
    "valid = Dataset.merge(valid, articles, on=\"article_id\", how=\"inner\")\n",
    "valid = Dataset.merge(valid, customers, on=\"customer_id\", how=\"inner\")\n",
    "del articles, customers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ['t_dat', 'customer_id', 'article_id', 'price', 'sales_channel_id','week']\n",
    "\n",
    "article_id = [\"article_id\"] >> Categorify() >> TagAsItemID()\n",
    "customer_id = [\"customer_id\"] >> Categorify() >> TagAsUserID()\n",
    "\n",
    "\n",
    "price = [\"price\"] >> FillMissing(fill_val=0) >> Normalize() >> TagAsItemFeatures() >> Rename(postfix='_norm')\n",
    "\n",
    "time_features = [\"t_dat\"]\n",
    "session_time = (\n",
    "    time_features >> \n",
    "    nvt.ops.LambdaOp(lambda col: cudf.to_datetime(col, unit='s')) >> \n",
    "    nvt.ops.Rename(name = 'event_time_dt')\n",
    ")\n",
    "\n",
    "day = (session_time\n",
    "        >> nvt.ops.LambdaOp(lambda col: col.dt.day)\n",
    "        >> nvt.ops.Rename(name = 'day')\n",
    "        >> Categorify()\n",
    "        >> TagAsUserFeatures())\n",
    "week = (session_time\n",
    "        >> nvt.ops.LambdaOp(lambda col: col.dt.weekday)\n",
    "        >> nvt.ops.Rename(name = 'week')\n",
    "        >> Categorify()\n",
    "        >> TagAsUserFeatures())\n",
    "month = (session_time\n",
    "        >> nvt.ops.LambdaOp(lambda col: col.dt.month)\n",
    "        >> nvt.ops.Rename(name = 'month')\n",
    "        >> Categorify()\n",
    "        >> TagAsUserFeatures())\n",
    "year = (session_time\n",
    "        >> nvt.ops.LambdaOp(lambda col: col.dt.year)\n",
    "        >> nvt.ops.Rename(name = 'year')\n",
    "        >> Categorify()\n",
    "        >> TagAsUserFeatures())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_boundaries = list(np.arange(0,100,1))\n",
    "age =  ['age'] >>  FillMissing(0) >> Bucketize(age_boundaries) >> Categorify() >> TagAsUserFeatures() \n",
    "# cat_customer_columns = (['FN', 'Active', 'club_member_status','fashion_news_frequency'] \n",
    "#             >> Categorify() \n",
    "#             >> TagAsUserFeatures()\n",
    "#             )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# article_features = ['article_id', 'product_code', 'prod_name', 'product_type_no',\n",
    "#        'product_type_name', 'product_group_name', 'graphical_appearance_no',\n",
    "#        'graphical_appearance_name', 'colour_group_code', 'colour_group_name',\n",
    "#        'perceived_colour_value_id', 'perceived_colour_value_name',\n",
    "#        'perceived_colour_master_id', 'perceived_colour_master_name',\n",
    "#        'department_no', 'department_name', 'index_code', 'index_name',\n",
    "#        'index_group_no', 'index_group_name', 'section_no', 'section_name',\n",
    "#        'garment_group_no', 'garment_group_name', 'detail_desc']\n",
    "\n",
    "# cat_article_columns = (article_features \n",
    "#               >> Categorify()  \n",
    "#               >> TagAsItemFeatures()\n",
    "#               >> Rename(postfix='_cat'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features = article_id + customer_id + price + day + week + month + year + cat_customer_columns + age + cat_article_columns + time_features\n",
    "# wf = nvt.Workflow(features)\n",
    "# train = wf.fit_transform(train)\n",
    "# valid = wf.transform(valid)\n",
    "# train.to_parquet(os.path.join(config.data_processed_dir,'train_processed.parquet'))\n",
    "# valid.to_parquet(os.path.join(config.data_processed_dir,'validation_processed.parquet'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "SESSIONS_MAX_LENGTH =2\n",
    "\n",
    "# groupby_features = features >> Groupby(\n",
    "#     groupby_cols=[\"customer_id\"], \n",
    "#     sort_cols=[\"t_dat\"],\n",
    "#     aggs={'article_id': ['list', 'count'],\n",
    "#         'price_norm': ['list', 'mean'],\n",
    "#         'day': ['list'],\n",
    "#         'week': ['list'],\n",
    "#         'month': ['list'],\n",
    "#         'year': ['list'],\n",
    "#         'FN': ['list'],\n",
    "#         'Active': ['list'],\n",
    "#         'club_member_status': ['list'],\n",
    "#         'fashion_news_frequency': ['list'],\n",
    "#         'article_id_cat': ['list'],\n",
    "#         'product_code_cat': ['list'],\n",
    "#         'product_type_no_cat': ['list'],\n",
    "#         'graphical_appearance_no_cat': ['list'],\n",
    "#         'colour_group_code_cat': ['list'],\n",
    "#         'perceived_colour_value_id_cat': ['list'],\n",
    "#         'perceived_colour_master_id_cat': ['list'],\n",
    "#         'department_no_cat': ['list'],\n",
    "#         'index_code_cat': ['list'],\n",
    "#         'index_group_no_cat': ['list'],\n",
    "#         'section_no_cat': ['list'],\n",
    "#         'garment_group_no_cat': ['list']},\n",
    "#     name_sep=\"-\")\n",
    "features = article_id + customer_id + price + day + week + month + year + time_features\n",
    "groupby_features = features >> Groupby(\n",
    "    groupby_cols=[\"customer_id\"], \n",
    "    sort_cols=[\"t_dat\"],\n",
    "    aggs={'article_id': ['list', 'count'],\n",
    "\n",
    "        },\n",
    "    name_sep=\"_\")\n",
    "groupby_features = groupby_features[] >> ListSlice(-SESSIONS_MAX_LENGTH) \n",
    "groupby_features = groupby_features + age + customer_id \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to transform operator <nvtabular.ops.list_slice.ListSlice object at 0x7f6b48376f10>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/merlin/dag/executors.py\", line 237, in _run_node_transform\n",
      "    transformed_data = node.op.transform(selection, input_data)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/nvtx/nvtx.py\", line 101, in inner\n",
      "    result = func(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/nvtabular/ops/list_slice.py\", line 106, in transform\n",
      "    offsets = c.offsets.values\n",
      "AttributeError: 'NumericalColumn' object has no attribute 'offsets'\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NumericalColumn' object has no attribute 'offsets'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m wf \u001b[39m=\u001b[39m nvt\u001b[39m.\u001b[39mWorkflow(groupby_features)\n\u001b[0;32m----> 2\u001b[0m train \u001b[39m=\u001b[39m wf\u001b[39m.\u001b[39;49mfit_transform(train)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/nvtabular/workflow/workflow.py:236\u001b[0m, in \u001b[0;36mWorkflow.fit_transform\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfit_transform\u001b[39m(\u001b[39mself\u001b[39m, dataset: Dataset) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Dataset:\n\u001b[1;32m    217\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Convenience method to both fit the workflow and transform the dataset in a single\u001b[39;00m\n\u001b[1;32m    218\u001b[0m \u001b[39m    call. Equivalent to calling ``workflow.fit(dataset)`` followed by\u001b[39;00m\n\u001b[1;32m    219\u001b[0m \u001b[39m    ``workflow.transform(dataset)``\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[39m    transform\u001b[39;00m\n\u001b[1;32m    235\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 236\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit(dataset)\n\u001b[1;32m    237\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform(dataset)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/nvtabular/workflow/workflow.py:213\u001b[0m, in \u001b[0;36mWorkflow.fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfit\u001b[39m(\u001b[39mself\u001b[39m, dataset: Dataset) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mWorkflow\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    200\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Calculates statistics for this workflow on the input dataset\u001b[39;00m\n\u001b[1;32m    201\u001b[0m \n\u001b[1;32m    202\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[39m        This Workflow with statistics calculated on it\u001b[39;00m\n\u001b[1;32m    212\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 213\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mexecutor\u001b[39m.\u001b[39;49mfit(dataset, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgraph)\n\u001b[1;32m    214\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/merlin/dag/executors.py:479\u001b[0m, in \u001b[0;36mDaskExecutor.fit\u001b[0;34m(self, dataset, graph, refit)\u001b[0m\n\u001b[1;32m    473\u001b[0m         dependencies\u001b[39m.\u001b[39mdifference_update(current_phase)\n\u001b[1;32m    475\u001b[0m \u001b[39m# This captures the output dtypes of operators like LambdaOp where\u001b[39;00m\n\u001b[1;32m    476\u001b[0m \u001b[39m# the dtype can't be determined without running the transform\u001b[39;00m\n\u001b[1;32m    477\u001b[0m \u001b[39m# self._transform_impl(dataset, capture_dtypes=True).sample_dtypes()\u001b[39;00m\n\u001b[1;32m    478\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[0;32m--> 479\u001b[0m Dataset(\n\u001b[1;32m    480\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransform(\n\u001b[1;32m    481\u001b[0m         dataset\u001b[39m.\u001b[39;49mto_ddf(),\n\u001b[1;32m    482\u001b[0m         graph\u001b[39m.\u001b[39;49moutput_node,\n\u001b[1;32m    483\u001b[0m         graph\u001b[39m.\u001b[39;49moutput_dtypes,\n\u001b[1;32m    484\u001b[0m         capture_dtypes\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    485\u001b[0m     ),\n\u001b[1;32m    486\u001b[0m     cpu\u001b[39m=\u001b[39;49mdataset\u001b[39m.\u001b[39;49mcpu,\n\u001b[1;32m    487\u001b[0m     base_dataset\u001b[39m=\u001b[39;49mdataset\u001b[39m.\u001b[39;49mbase_dataset,\n\u001b[1;32m    488\u001b[0m     schema\u001b[39m=\u001b[39;49mgraph\u001b[39m.\u001b[39;49moutput_schema,\n\u001b[1;32m    489\u001b[0m )\u001b[39m.\u001b[39;49msample_dtypes()\n\u001b[1;32m    490\u001b[0m graph\u001b[39m.\u001b[39mconstruct_schema(dataset\u001b[39m.\u001b[39mschema, preserve_dtypes\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    492\u001b[0m \u001b[39mreturn\u001b[39;00m graph\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/merlin/io/dataset.py:1262\u001b[0m, in \u001b[0;36mDataset.sample_dtypes\u001b[0;34m(self, n, annotate_lists)\u001b[0m\n\u001b[1;32m   1255\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Return the real dtypes of the Dataset\u001b[39;00m\n\u001b[1;32m   1256\u001b[0m \n\u001b[1;32m   1257\u001b[0m \u001b[39mUse cached metadata if this operation was\u001b[39;00m\n\u001b[1;32m   1258\u001b[0m \u001b[39malready performed. Otherwise, call down to the\u001b[39;00m\n\u001b[1;32m   1259\u001b[0m \u001b[39munderlying engine for sampling logic.\u001b[39;00m\n\u001b[1;32m   1260\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1261\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_real_meta\u001b[39m.\u001b[39mget(n, \u001b[39mNone\u001b[39;00m) \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1262\u001b[0m     _real_meta \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mengine\u001b[39m.\u001b[39;49msample_data(n\u001b[39m=\u001b[39;49mn)\n\u001b[1;32m   1263\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdtypes:\n\u001b[1;32m   1264\u001b[0m         _real_meta \u001b[39m=\u001b[39m _set_dtypes(_real_meta, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdtypes)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/merlin/io/dataset_engine.py:71\u001b[0m, in \u001b[0;36mDatasetEngine.sample_data\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m     69\u001b[0m _ddf \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mto_ddf()\n\u001b[1;32m     70\u001b[0m \u001b[39mfor\u001b[39;00m partition_index \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(_ddf\u001b[39m.\u001b[39mnpartitions):\n\u001b[0;32m---> 71\u001b[0m     _head \u001b[39m=\u001b[39m _ddf\u001b[39m.\u001b[39;49mpartitions[partition_index]\u001b[39m.\u001b[39;49mhead(n)\n\u001b[1;32m     72\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(_head):\n\u001b[1;32m     73\u001b[0m         \u001b[39mreturn\u001b[39;00m _head\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/dask/dataframe/core.py:1268\u001b[0m, in \u001b[0;36m_Frame.head\u001b[0;34m(self, n, npartitions, compute)\u001b[0m\n\u001b[1;32m   1266\u001b[0m \u001b[39m# No need to warn if we're already looking at all partitions\u001b[39;00m\n\u001b[1;32m   1267\u001b[0m safe \u001b[39m=\u001b[39m npartitions \u001b[39m!=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnpartitions\n\u001b[0;32m-> 1268\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_head(n\u001b[39m=\u001b[39;49mn, npartitions\u001b[39m=\u001b[39;49mnpartitions, compute\u001b[39m=\u001b[39;49mcompute, safe\u001b[39m=\u001b[39;49msafe)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/dask/dataframe/core.py:1302\u001b[0m, in \u001b[0;36m_Frame._head\u001b[0;34m(self, n, npartitions, compute, safe)\u001b[0m\n\u001b[1;32m   1297\u001b[0m result \u001b[39m=\u001b[39m new_dd_object(\n\u001b[1;32m   1298\u001b[0m     graph, name, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_meta, [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdivisions[\u001b[39m0\u001b[39m], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdivisions[npartitions]]\n\u001b[1;32m   1299\u001b[0m )\n\u001b[1;32m   1301\u001b[0m \u001b[39mif\u001b[39;00m compute:\n\u001b[0;32m-> 1302\u001b[0m     result \u001b[39m=\u001b[39m result\u001b[39m.\u001b[39;49mcompute()\n\u001b[1;32m   1303\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/dask/base.py:314\u001b[0m, in \u001b[0;36mDaskMethodsMixin.compute\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    290\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcompute\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    291\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Compute this dask collection\u001b[39;00m\n\u001b[1;32m    292\u001b[0m \n\u001b[1;32m    293\u001b[0m \u001b[39m    This turns a lazy Dask collection into its in-memory equivalent.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    312\u001b[0m \u001b[39m    dask.base.compute\u001b[39;00m\n\u001b[1;32m    313\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 314\u001b[0m     (result,) \u001b[39m=\u001b[39m compute(\u001b[39mself\u001b[39;49m, traverse\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    315\u001b[0m     \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/dask/base.py:599\u001b[0m, in \u001b[0;36mcompute\u001b[0;34m(traverse, optimize_graph, scheduler, get, *args, **kwargs)\u001b[0m\n\u001b[1;32m    596\u001b[0m     keys\u001b[39m.\u001b[39mappend(x\u001b[39m.\u001b[39m__dask_keys__())\n\u001b[1;32m    597\u001b[0m     postcomputes\u001b[39m.\u001b[39mappend(x\u001b[39m.\u001b[39m__dask_postcompute__())\n\u001b[0;32m--> 599\u001b[0m results \u001b[39m=\u001b[39m schedule(dsk, keys, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    600\u001b[0m \u001b[39mreturn\u001b[39;00m repack([f(r, \u001b[39m*\u001b[39ma) \u001b[39mfor\u001b[39;00m r, (f, a) \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(results, postcomputes)])\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/dask/threaded.py:89\u001b[0m, in \u001b[0;36mget\u001b[0;34m(dsk, keys, cache, num_workers, pool, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(pool, multiprocessing\u001b[39m.\u001b[39mpool\u001b[39m.\u001b[39mPool):\n\u001b[1;32m     87\u001b[0m         pool \u001b[39m=\u001b[39m MultiprocessingPoolExecutor(pool)\n\u001b[0;32m---> 89\u001b[0m results \u001b[39m=\u001b[39m get_async(\n\u001b[1;32m     90\u001b[0m     pool\u001b[39m.\u001b[39;49msubmit,\n\u001b[1;32m     91\u001b[0m     pool\u001b[39m.\u001b[39;49m_max_workers,\n\u001b[1;32m     92\u001b[0m     dsk,\n\u001b[1;32m     93\u001b[0m     keys,\n\u001b[1;32m     94\u001b[0m     cache\u001b[39m=\u001b[39;49mcache,\n\u001b[1;32m     95\u001b[0m     get_id\u001b[39m=\u001b[39;49m_thread_get_id,\n\u001b[1;32m     96\u001b[0m     pack_exception\u001b[39m=\u001b[39;49mpack_exception,\n\u001b[1;32m     97\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m     98\u001b[0m )\n\u001b[1;32m    100\u001b[0m \u001b[39m# Cleanup pools associated to dead threads\u001b[39;00m\n\u001b[1;32m    101\u001b[0m \u001b[39mwith\u001b[39;00m pools_lock:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/dask/local.py:511\u001b[0m, in \u001b[0;36mget_async\u001b[0;34m(submit, num_workers, dsk, result, cache, get_id, rerun_exceptions_locally, pack_exception, raise_exception, callbacks, dumps, loads, chunksize, **kwargs)\u001b[0m\n\u001b[1;32m    509\u001b[0m         _execute_task(task, data)  \u001b[39m# Re-execute locally\u001b[39;00m\n\u001b[1;32m    510\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 511\u001b[0m         raise_exception(exc, tb)\n\u001b[1;32m    512\u001b[0m res, worker_id \u001b[39m=\u001b[39m loads(res_info)\n\u001b[1;32m    513\u001b[0m state[\u001b[39m\"\u001b[39m\u001b[39mcache\u001b[39m\u001b[39m\"\u001b[39m][key] \u001b[39m=\u001b[39m res\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/dask/local.py:319\u001b[0m, in \u001b[0;36mreraise\u001b[0;34m(exc, tb)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[39mif\u001b[39;00m exc\u001b[39m.\u001b[39m__traceback__ \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m tb:\n\u001b[1;32m    318\u001b[0m     \u001b[39mraise\u001b[39;00m exc\u001b[39m.\u001b[39mwith_traceback(tb)\n\u001b[0;32m--> 319\u001b[0m \u001b[39mraise\u001b[39;00m exc\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/dask/local.py:224\u001b[0m, in \u001b[0;36mexecute_task\u001b[0;34m(key, task_info, dumps, loads, get_id, pack_exception)\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    223\u001b[0m     task, data \u001b[39m=\u001b[39m loads(task_info)\n\u001b[0;32m--> 224\u001b[0m     result \u001b[39m=\u001b[39m _execute_task(task, data)\n\u001b[1;32m    225\u001b[0m     \u001b[39mid\u001b[39m \u001b[39m=\u001b[39m get_id()\n\u001b[1;32m    226\u001b[0m     result \u001b[39m=\u001b[39m dumps((result, \u001b[39mid\u001b[39m))\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/dask/core.py:119\u001b[0m, in \u001b[0;36m_execute_task\u001b[0;34m(arg, cache, dsk)\u001b[0m\n\u001b[1;32m    115\u001b[0m     func, args \u001b[39m=\u001b[39m arg[\u001b[39m0\u001b[39m], arg[\u001b[39m1\u001b[39m:]\n\u001b[1;32m    116\u001b[0m     \u001b[39m# Note: Don't assign the subtask results to a variable. numpy detects\u001b[39;00m\n\u001b[1;32m    117\u001b[0m     \u001b[39m# temporaries by their reference count and can execute certain\u001b[39;00m\n\u001b[1;32m    118\u001b[0m     \u001b[39m# operations in-place.\u001b[39;00m\n\u001b[0;32m--> 119\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49m(_execute_task(a, cache) \u001b[39mfor\u001b[39;49;00m a \u001b[39min\u001b[39;49;00m args))\n\u001b[1;32m    120\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m ishashable(arg):\n\u001b[1;32m    121\u001b[0m     \u001b[39mreturn\u001b[39;00m arg\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/dask/optimization.py:990\u001b[0m, in \u001b[0;36mSubgraphCallable.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    988\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m==\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39minkeys):\n\u001b[1;32m    989\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mExpected \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m args, got \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39minkeys), \u001b[39mlen\u001b[39m(args)))\n\u001b[0;32m--> 990\u001b[0m \u001b[39mreturn\u001b[39;00m core\u001b[39m.\u001b[39;49mget(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdsk, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moutkey, \u001b[39mdict\u001b[39;49m(\u001b[39mzip\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minkeys, args)))\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/dask/core.py:149\u001b[0m, in \u001b[0;36mget\u001b[0;34m(dsk, out, cache)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m toposort(dsk):\n\u001b[1;32m    148\u001b[0m     task \u001b[39m=\u001b[39m dsk[key]\n\u001b[0;32m--> 149\u001b[0m     result \u001b[39m=\u001b[39m _execute_task(task, cache)\n\u001b[1;32m    150\u001b[0m     cache[key] \u001b[39m=\u001b[39m result\n\u001b[1;32m    151\u001b[0m result \u001b[39m=\u001b[39m _execute_task(out, cache)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/dask/core.py:119\u001b[0m, in \u001b[0;36m_execute_task\u001b[0;34m(arg, cache, dsk)\u001b[0m\n\u001b[1;32m    115\u001b[0m     func, args \u001b[39m=\u001b[39m arg[\u001b[39m0\u001b[39m], arg[\u001b[39m1\u001b[39m:]\n\u001b[1;32m    116\u001b[0m     \u001b[39m# Note: Don't assign the subtask results to a variable. numpy detects\u001b[39;00m\n\u001b[1;32m    117\u001b[0m     \u001b[39m# temporaries by their reference count and can execute certain\u001b[39;00m\n\u001b[1;32m    118\u001b[0m     \u001b[39m# operations in-place.\u001b[39;00m\n\u001b[0;32m--> 119\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49m(_execute_task(a, cache) \u001b[39mfor\u001b[39;49;00m a \u001b[39min\u001b[39;49;00m args))\n\u001b[1;32m    120\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m ishashable(arg):\n\u001b[1;32m    121\u001b[0m     \u001b[39mreturn\u001b[39;00m arg\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/dask/utils.py:72\u001b[0m, in \u001b[0;36mapply\u001b[0;34m(func, args, kwargs)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Apply a function given its positional and keyword arguments.\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \n\u001b[1;32m     43\u001b[0m \u001b[39mEquivalent to ``func(*args, **kwargs)``\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[39m>>> dsk = {'task-name': task}  # adds the task to a low level Dask task graph\u001b[39;00m\n\u001b[1;32m     70\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[39mif\u001b[39;00m kwargs:\n\u001b[0;32m---> 72\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     73\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     74\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/merlin/dag/executors.py:102\u001b[0m, in \u001b[0;36mLocalExecutor.transform\u001b[0;34m(self, transformable, graph, output_dtypes, additional_columns, capture_dtypes, strict, target_format)\u001b[0m\n\u001b[1;32m    100\u001b[0m output_data \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    101\u001b[0m \u001b[39mfor\u001b[39;00m node \u001b[39min\u001b[39;00m nodes:\n\u001b[0;32m--> 102\u001b[0m     transformed_data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_execute_node(node, transformable, capture_dtypes, strict)\n\u001b[1;32m    103\u001b[0m     output_data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_combine_node_outputs(node, transformed_data, output_data)\n\u001b[1;32m    105\u001b[0m \u001b[39m# If there are any additional columns that weren't produced by one of the supplied nodes\u001b[39;00m\n\u001b[1;32m    106\u001b[0m \u001b[39m# we grab them directly from the supplied input data. Normally this would happen on a\u001b[39;00m\n\u001b[1;32m    107\u001b[0m \u001b[39m# per-node basis, but offers a safety net for the multi-node case\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/merlin/dag/executors.py:116\u001b[0m, in \u001b[0;36mLocalExecutor._execute_node\u001b[0;34m(self, node, transformable, capture_dtypes, strict)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_execute_node\u001b[39m(\u001b[39mself\u001b[39m, node, transformable, capture_dtypes\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, strict\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m--> 116\u001b[0m     upstream_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_upstream_transforms(\n\u001b[1;32m    117\u001b[0m         node, transformable, capture_dtypes, strict\n\u001b[1;32m    118\u001b[0m     )\n\u001b[1;32m    119\u001b[0m     upstream_columns \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_append_addl_root_columns(node, transformable, upstream_outputs)\n\u001b[1;32m    120\u001b[0m     formatted_columns \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_standardize_formats(node, upstream_columns)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/merlin/dag/executors.py:130\u001b[0m, in \u001b[0;36mLocalExecutor._run_upstream_transforms\u001b[0;34m(self, node, transformable, capture_dtypes, strict)\u001b[0m\n\u001b[1;32m    127\u001b[0m upstream_outputs \u001b[39m=\u001b[39m []\n\u001b[1;32m    129\u001b[0m \u001b[39mfor\u001b[39;00m upstream_node \u001b[39min\u001b[39;00m node\u001b[39m.\u001b[39mparents_with_dependencies:\n\u001b[0;32m--> 130\u001b[0m     node_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_execute_node(\n\u001b[1;32m    131\u001b[0m         upstream_node,\n\u001b[1;32m    132\u001b[0m         transformable,\n\u001b[1;32m    133\u001b[0m         capture_dtypes\u001b[39m=\u001b[39;49mcapture_dtypes,\n\u001b[1;32m    134\u001b[0m         strict\u001b[39m=\u001b[39;49mstrict,\n\u001b[1;32m    135\u001b[0m     )\n\u001b[1;32m    136\u001b[0m     \u001b[39mif\u001b[39;00m node_output \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(node_output) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    137\u001b[0m         upstream_outputs\u001b[39m.\u001b[39mappend(node_output)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/merlin/dag/executors.py:122\u001b[0m, in \u001b[0;36mLocalExecutor._execute_node\u001b[0;34m(self, node, transformable, capture_dtypes, strict)\u001b[0m\n\u001b[1;32m    120\u001b[0m formatted_columns \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_standardize_formats(node, upstream_columns)\n\u001b[1;32m    121\u001b[0m transform_input \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_merge_upstream_columns(formatted_columns)\n\u001b[0;32m--> 122\u001b[0m transform_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_node_transform(node, transform_input, capture_dtypes, strict)\n\u001b[1;32m    123\u001b[0m transform_output \u001b[39m=\u001b[39m _convert_format(transform_output, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_format)\n\u001b[1;32m    124\u001b[0m \u001b[39mreturn\u001b[39;00m transform_output\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/merlin/dag/executors.py:250\u001b[0m, in \u001b[0;36mLocalExecutor._run_node_transform\u001b[0;34m(self, node, input_data, capture_dtypes, strict)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m exc:\n\u001b[1;32m    249\u001b[0m     LOG\u001b[39m.\u001b[39mexception(\u001b[39m\"\u001b[39m\u001b[39mFailed to transform operator \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m, node\u001b[39m.\u001b[39mop)\n\u001b[0;32m--> 250\u001b[0m     \u001b[39mraise\u001b[39;00m exc\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/merlin/dag/executors.py:237\u001b[0m, in \u001b[0;36mLocalExecutor._run_node_transform\u001b[0;34m(self, node, input_data, capture_dtypes, strict)\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    235\u001b[0m     \u001b[39m# use input_columns to ensure correct grouping (subgroups)\u001b[39;00m\n\u001b[1;32m    236\u001b[0m     selection \u001b[39m=\u001b[39m node\u001b[39m.\u001b[39minput_columns\u001b[39m.\u001b[39mresolve(node\u001b[39m.\u001b[39minput_schema)\n\u001b[0;32m--> 237\u001b[0m     transformed_data \u001b[39m=\u001b[39m node\u001b[39m.\u001b[39;49mop\u001b[39m.\u001b[39;49mtransform(selection, input_data)\n\u001b[1;32m    239\u001b[0m     \u001b[39mif\u001b[39;00m transformed_data \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    240\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mOperator \u001b[39m\u001b[39m{\u001b[39;00mnode\u001b[39m.\u001b[39mop\u001b[39m}\u001b[39;00m\u001b[39m didn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt return a value during transform\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/nvtx/nvtx.py:101\u001b[0m, in \u001b[0;36mannotate.__call__.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[39m@wraps\u001b[39m(func)\n\u001b[1;32m     99\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minner\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    100\u001b[0m     libnvtx_push_range(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mattributes, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdomain\u001b[39m.\u001b[39mhandle)\n\u001b[0;32m--> 101\u001b[0m     result \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    102\u001b[0m     libnvtx_pop_range(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdomain\u001b[39m.\u001b[39mhandle)\n\u001b[1;32m    103\u001b[0m     \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/nvtabular/ops/list_slice.py:106\u001b[0m, in \u001b[0;36mListSlice.transform\u001b[0;34m(self, col_selector, df)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    104\u001b[0m     \u001b[39m# figure out the size of each row from the list offsets\u001b[39;00m\n\u001b[1;32m    105\u001b[0m     c \u001b[39m=\u001b[39m df[col]\u001b[39m.\u001b[39m_column\n\u001b[0;32m--> 106\u001b[0m     offsets \u001b[39m=\u001b[39m c\u001b[39m.\u001b[39;49moffsets\u001b[39m.\u001b[39mvalues\n\u001b[1;32m    107\u001b[0m     elements \u001b[39m=\u001b[39m c\u001b[39m.\u001b[39melements\u001b[39m.\u001b[39mvalues\n\u001b[1;32m    109\u001b[0m     threads \u001b[39m=\u001b[39m \u001b[39m32\u001b[39m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NumericalColumn' object has no attribute 'offsets'"
     ]
    }
   ],
   "source": [
    "wf = nvt.Workflow(groupby_features)\n",
    "train = wf.fit_transform(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id-list</th>\n",
       "      <th>age</th>\n",
       "      <th>customer_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[151, 2055]</td>\n",
       "      <td>4</td>\n",
       "      <td>97302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[193, 25]</td>\n",
       "      <td>4</td>\n",
       "      <td>97302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[9712, 12785]</td>\n",
       "      <td>4</td>\n",
       "      <td>97302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[4425, 6450]</td>\n",
       "      <td>4</td>\n",
       "      <td>97302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[4046, 5260]</td>\n",
       "      <td>4</td>\n",
       "      <td>97302</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  article_id-list  age  customer_id\n",
       "0     [151, 2055]    4        97302\n",
       "1       [193, 25]    4        97302\n",
       "2   [9712, 12785]    4        97302\n",
       "3    [4425, 6450]    4        97302\n",
       "4    [4046, 5260]    4        97302"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>tags</th>\n",
       "      <th>dtype</th>\n",
       "      <th>is_list</th>\n",
       "      <th>is_ragged</th>\n",
       "      <th>properties.num_buckets</th>\n",
       "      <th>properties.freq_threshold</th>\n",
       "      <th>properties.max_size</th>\n",
       "      <th>properties.cat_path</th>\n",
       "      <th>properties.domain.min</th>\n",
       "      <th>properties.domain.max</th>\n",
       "      <th>properties.domain.name</th>\n",
       "      <th>properties.embedding_sizes.cardinality</th>\n",
       "      <th>properties.embedding_sizes.dimension</th>\n",
       "      <th>properties.value_count.min</th>\n",
       "      <th>properties.value_count.max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>article_id-list</td>\n",
       "      <td>(Tags.LIST, Tags.ITEM, Tags.CATEGORICAL, Tags.ID)</td>\n",
       "      <td>DType(name='int64', element_type=&lt;ElementType....</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>.//categories/unique.article_id.parquet</td>\n",
       "      <td>0</td>\n",
       "      <td>55240</td>\n",
       "      <td>article_id</td>\n",
       "      <td>55241</td>\n",
       "      <td>512</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>age</td>\n",
       "      <td>(Tags.CATEGORICAL, Tags.USER)</td>\n",
       "      <td>DType(name='int64', element_type=&lt;ElementType....</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>.//categories/unique.age.parquet</td>\n",
       "      <td>0</td>\n",
       "      <td>87</td>\n",
       "      <td>age</td>\n",
       "      <td>88</td>\n",
       "      <td>20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>customer_id</td>\n",
       "      <td>(Tags.ID, Tags.CATEGORICAL, Tags.USER)</td>\n",
       "      <td>DType(name='int64', element_type=&lt;ElementType....</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>.//categories/unique.customer_id.parquet</td>\n",
       "      <td>0</td>\n",
       "      <td>780123</td>\n",
       "      <td>customer_id</td>\n",
       "      <td>780124</td>\n",
       "      <td>512</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "[{'name': 'article_id-list', 'tags': {<Tags.LIST: 'list'>, <Tags.ITEM: 'item'>, <Tags.CATEGORICAL: 'categorical'>, <Tags.ID: 'id'>}, 'properties': {'num_buckets': None, 'freq_threshold': 0, 'max_size': 0, 'cat_path': './/categories/unique.article_id.parquet', 'domain': {'min': 0, 'max': 55240, 'name': 'article_id'}, 'embedding_sizes': {'cardinality': 55241, 'dimension': 512}, 'value_count': {'min': 0, 'max': 2}}, 'dtype': DType(name='int64', element_type=<ElementType.Int: 'int'>, element_size=64, element_unit=None, signed=True, shape=Shape(dims=(Dimension(min=0, max=None), Dimension(min=0, max=2)))), 'is_list': True, 'is_ragged': True}, {'name': 'age', 'tags': {<Tags.CATEGORICAL: 'categorical'>, <Tags.USER: 'user'>}, 'properties': {'num_buckets': None, 'freq_threshold': 0, 'max_size': 0, 'cat_path': './/categories/unique.age.parquet', 'domain': {'min': 0, 'max': 87, 'name': 'age'}, 'embedding_sizes': {'cardinality': 88, 'dimension': 20}}, 'dtype': DType(name='int64', element_type=<ElementType.Int: 'int'>, element_size=64, element_unit=None, signed=True, shape=Shape(dims=(Dimension(min=0, max=None),))), 'is_list': False, 'is_ragged': False}, {'name': 'customer_id', 'tags': {<Tags.ID: 'id'>, <Tags.CATEGORICAL: 'categorical'>, <Tags.USER: 'user'>}, 'properties': {'num_buckets': None, 'freq_threshold': 0, 'max_size': 0, 'cat_path': './/categories/unique.customer_id.parquet', 'domain': {'min': 0, 'max': 780123, 'name': 'customer_id'}, 'embedding_sizes': {'cardinality': 780124, 'dimension': 512}}, 'dtype': DType(name='int64', element_type=<ElementType.Int: 'int'>, element_size=64, element_unit=None, signed=True, shape=Shape(dims=(Dimension(min=0, max=None),))), 'is_list': False, 'is_ragged': False}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wf.output_schema\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
