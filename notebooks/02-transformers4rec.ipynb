{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-05 12:59:07.234838: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-09-05 12:59:07.259235: I tensorflow/core/platform/cpu_feature_guard.cc:183] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE3 SSE4.1 SSE4.2 AVX, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Please fix your imports. Module tensorflow.python.training.tracking.data_structures has been moved to tensorflow.python.trackable.data_structures. The old module will be deleted in version 2.11.\n",
      "[INFO]: sparse_operation_kit is imported\n",
      "WARNING:tensorflow:Please fix your imports. Module tensorflow.python.training.tracking.base has been moved to tensorflow.python.trackable.base. The old module will be deleted in version 2.11.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-05 12:59:08.251676: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:1013] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-09-05 12:59:08.251898: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:1013] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-09-05 12:59:08.251946: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:1013] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SOK INFO] Import /usr/local/lib/python3.8/dist-packages/merlin_sok-1.2.0-py3.8-linux-x86_64.egg/sparse_operation_kit/lib/libsok_experiment.so\n",
      "[SOK INFO] Import /usr/local/lib/python3.8/dist-packages/merlin_sok-1.2.0-py3.8-linux-x86_64.egg/sparse_operation_kit/lib/libsok_experiment.so\n",
      "[SOK INFO] Initialize finished, communication tool: horovod\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-05 12:59:08.479261: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:1013] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-09-05 12:59:08.479381: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:1013] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-09-05 12:59:08.479429: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:1013] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-09-05 12:59:08.487393: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:1013] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-09-05 12:59:08.487493: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:1013] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-09-05 12:59:08.487544: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:1013] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-09-05 12:59:08.487577: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2023-09-05 12:59:08.487593: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1638] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6011 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import nvtabular as nvt\n",
    "import cudf\n",
    "import config\n",
    "import os\n",
    "# ignore warnings\n",
    "from merlin.core.dispatch import get_lib\n",
    "from merlin.schema.tags import Tags\n",
    "\n",
    "import time\n",
    "import merlin.models.tf as mm\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "from merlin.schema.tags import Tags\n",
    "from nvtabular.ops import *\n",
    "\n",
    "from merlin.schema.tags import Tags\n",
    "from merlin.io.dataset import Dataset\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = nvt.Dataset(os.path.join(config.data_processed_dir,'train', 'transactions.parquet'),engine=\"parquet\",  part_mem_fraction=0.1)\n",
    "valid = nvt.Dataset(os.path.join(config.data_processed_dir,'valid', 'transactions.parquet'),engine=\"parquet\", part_mem_fraction=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_features = ['customer_id', 'age', 'FN', 'Active', 'club_member_status','fashion_news_frequency']\n",
    "article_features = ['article_id', 'product_code', 'product_type_no', 'graphical_appearance_no', 'colour_group_code', 'perceived_colour_value_id', 'perceived_colour_master_id', 'department_no', 'index_code', 'index_group_no', 'section_no', 'garment_group_no']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = cudf.read_parquet(os.path.join(config.data_raw_dir,'articles.parquet'))[article_features]\n",
    "customers = cudf.read_parquet(os.path.join(config.data_raw_dir,'customers_enc.parquet'))[customer_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = nvt.Dataset(articles)\n",
    "customers = nvt.Dataset(customers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = Dataset.merge(train, articles, on=\"article_id\", how=\"inner\")\n",
    "train = Dataset.merge(train, customers, on=\"customer_id\", how=\"inner\")\n",
    "# train.regenerate_dataset(os.path.join(config.data_processed_dir,'transformer4rec', 'train.parquet'),part_size=\"500MiB\", file_size=\"500MiB\")\n",
    "valid = Dataset.merge(valid, articles, on=\"article_id\", how=\"inner\")\n",
    "valid = Dataset.merge(valid, customers, on=\"customer_id\", how=\"inner\")\n",
    "# train.regenerate_dataset(os.path.join(config.data_processed_dir,'transformer4rec', 'valid.parquet'),part_size=\"500MiB\", file_size=\"500MiB\")\n",
    "del articles, customers, train, valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ['t_dat', 'customer_id', 'article_id', 'price', 'sales_channel_id','week']\n",
    "\n",
    "article_id = [\"article_id\"] >> Categorify() >> TagAsItemID()\n",
    "customer_id = [\"customer_id\"] >> Categorify() >> TagAsUserID()\n",
    "\n",
    "\n",
    "price = [\"price\"] >> FillMissing(fill_val=0) >> Normalize() >> TagAsItemFeatures() \n",
    "\n",
    "time_features = [\"t_dat\"]\n",
    "session_time = (\n",
    "    time_features >> \n",
    "    nvt.ops.LambdaOp(lambda col: cudf.to_datetime(col, unit='s')) >> \n",
    "    nvt.ops.Rename(name = 'event_time_dt')\n",
    ")\n",
    "\n",
    "day = (session_time\n",
    "        >> nvt.ops.LambdaOp(lambda col: col.dt.day)\n",
    "        >> nvt.ops.Rename(name = 'day')\n",
    "        >> Categorify()\n",
    "        >> TagAsUserFeatures())\n",
    "week = (session_time\n",
    "        >> nvt.ops.LambdaOp(lambda col: col.dt.weekday)\n",
    "        >> nvt.ops.Rename(name = 'week')\n",
    "        >> Categorify()\n",
    "        >> TagAsUserFeatures())\n",
    "month = (session_time\n",
    "        >> nvt.ops.LambdaOp(lambda col: col.dt.month)\n",
    "        >> nvt.ops.Rename(name = 'month')\n",
    "        >> Categorify()\n",
    "        >> TagAsUserFeatures())\n",
    "year = (session_time\n",
    "        >> nvt.ops.LambdaOp(lambda col: col.dt.year)\n",
    "        >> nvt.ops.Rename(name = 'year')\n",
    "        >> Categorify()\n",
    "        >> TagAsUserFeatures())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_boundaries = list(np.arange(0,100,1))\n",
    "age =  ['age'] >>  FillMissing(0) >> Bucketize(age_boundaries) >> Categorify() >> TagAsUserFeatures() \n",
    "cat_customer_columns = (['FN', 'Active', 'club_member_status','fashion_news_frequency'] \n",
    "            >> Categorify() \n",
    "            >> TagAsUserFeatures()\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# article_features = ['article_id', 'product_code', 'prod_name', 'product_type_no',\n",
    "#        'product_type_name', 'product_group_name', 'graphical_appearance_no',\n",
    "#        'graphical_appearance_name', 'colour_group_code', 'colour_group_name',\n",
    "#        'perceived_colour_value_id', 'perceived_colour_value_name',\n",
    "#        'perceived_colour_master_id', 'perceived_colour_master_name',\n",
    "#        'department_no', 'department_name', 'index_code', 'index_name',\n",
    "#        'index_group_no', 'index_group_name', 'section_no', 'section_name',\n",
    "#        'garment_group_no', 'garment_group_name', 'detail_desc']\n",
    "\n",
    "cat_article_columns = (article_features \n",
    "              >> Categorify()  \n",
    "              >> TagAsItemFeatures())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features = article_id + customer_id + price + day + week + month + year + cat_customer_columns + age + cat_article_columns + time_features\n",
    "# wf = nvt.Workflow(features)\n",
    "# train = wf.fit_transform(train)\n",
    "# valid = wf.transform(valid)\n",
    "# train.to_parquet(os.path.join(config.data_processed_dir,'train_processed.parquet'))\n",
    "# valid.to_parquet(os.path.join(config.data_processed_dir,'validation_processed.parquet'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "SESSIONS_MAX_LENGTH =20\n",
    "SESSIONS_MIN_LENGTH = 5\n",
    "features = article_id + customer_id + price + day + week + month + year + cat_customer_columns + age + cat_article_columns + time_features\n",
    "groupby_features = features >> Groupby(\n",
    "    groupby_cols=[\"customer_id\"], \n",
    "    sort_cols=[\"t_dat\"],\n",
    "    aggs={'article_id': ['list'],\n",
    "        'price': ['list'],\n",
    "        'day': ['list'],\n",
    "        'week': ['list'],\n",
    "        'month': ['list'],\n",
    "        'year': ['list'],\n",
    "        'FN': ['list'],\n",
    "        'Active': ['list'],\n",
    "        'club_member_status': ['list'],\n",
    "        'fashion_news_frequency': ['list'],\n",
    "        'article_id': ['list'],\n",
    "        'product_code': ['list'],\n",
    "        'product_type_no': ['list'],\n",
    "        'graphical_appearance_no': ['list'],\n",
    "        'colour_group_code': ['list'],\n",
    "        'perceived_colour_value_id': ['list'],\n",
    "        'perceived_colour_master_id': ['list'],\n",
    "        'department_no': ['list'],\n",
    "        'index_code': ['list'],\n",
    "        'index_group_no': ['list'],\n",
    "        'section_no': ['list'],\n",
    "        'garment_group_no': ['list']},\n",
    "    name_sep=\"_\")\n",
    "list_columns = ['article_id_list',\n",
    " 'price_list',\n",
    " 'day_list',\n",
    " 'week_list',\n",
    " 'month_list',\n",
    " 'year_list',\n",
    " 'FN_list',\n",
    " 'Active_list',\n",
    " 'club_member_status_list',\n",
    " 'fashion_news_frequency_list',\n",
    " 'product_code_list',\n",
    " 'product_type_no_list',\n",
    " 'graphical_appearance_no_list',\n",
    " 'colour_group_code_list',\n",
    " 'perceived_colour_value_id_list',\n",
    " 'perceived_colour_master_id_list',\n",
    " 'department_no_list',\n",
    " 'index_code_list',\n",
    " 'index_group_no_list',\n",
    " 'section_no_list',\n",
    " 'garment_group_no_list']\n",
    "groupby_features = groupby_features[list_columns] >> ListSlice(-SESSIONS_MAX_LENGTH,pad=False) \n",
    "\n",
    "stat_features = features >> Groupby(\n",
    "    groupby_cols=[\"customer_id\"], \n",
    "    sort_cols=[\"t_dat\"],\n",
    "    aggs={'article_id': ['list','count'],\n",
    "        'price': ['mean', 'std', 'min', 'max']},\n",
    "    name_sep=\"_\")\n",
    "\n",
    "groupby_features = (groupby_features + \n",
    "                stat_features['article_id_count','price_max','price_min','price_mean','price_std'] + \n",
    "                age + \n",
    "                customer_id )\n",
    "\n",
    "filtered = groupby_features >> Filter(f=lambda df: df[\"article_id_count\"] >=SESSIONS_MIN_LENGTH)\n",
    "\n",
    "\n",
    "\n",
    "# filtered = groupby_features[list_columns] >> Filter(f=lambda df: df['article_id_count'] >= SESSIONS_MIN_LENGTH)\n",
    "# groupby_features = groupby_features + age + customer_id \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "wf = nvt.Workflow(filtered)\n",
    "train = nvt.Dataset(os.path.join(config.data_processed_dir,'transformer4rec', 'train.parquet'),engine=\"parquet\",  part_size=\"1GB\")\n",
    "valid = nvt.Dataset(os.path.join(config.data_processed_dir,'transformer4rec', 'valid.parquet'),engine=\"parquet\", part_size=\"1GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = wf.fit_transform(train)\n",
    "valid = wf.transform(valid)\n",
    "train.regenerate_dataset(os.path.join(config.data_final,'transformer4rec', 'train.parquet'),part_size=\"500MiB\", file_size=\"500MiB\")\n",
    "valid.regenerate_dataset(os.path.join(config.data_final,'transformer4rec', 'valid.parquet'),part_size=\"500MiB\", file_size=\"500MiB\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
