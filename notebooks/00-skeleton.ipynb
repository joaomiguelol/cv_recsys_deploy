{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NoSuchModuleError",
     "evalue": "Can't load plugin: sqlalchemy.dialects:db",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNoSuchModuleError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m table_name \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mtest_table\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m      8\u001b[0m connection_string \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mdb://$\u001b[39m\u001b[39m{\u001b[39;00mMYSQL_USER\u001b[39m}\u001b[39;00m\u001b[39m:$\u001b[39m\u001b[39m{\u001b[39;00mMYSQL_PASSWORD\u001b[39m}\u001b[39;00m\u001b[39m@db:3306/$\u001b[39m\u001b[39m{\u001b[39;00mMYSQL_DATABASE\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m----> 9\u001b[0m engine \u001b[39m=\u001b[39m create_engine(connection_string)\n\u001b[1;32m     11\u001b[0m \u001b[39m# Example DataFrame to save\u001b[39;00m\n\u001b[1;32m     12\u001b[0m data \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame({\u001b[39m'\u001b[39m\u001b[39mcolumn1\u001b[39m\u001b[39m'\u001b[39m: [\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m3\u001b[39m], \u001b[39m'\u001b[39m\u001b[39mcolumn2\u001b[39m\u001b[39m'\u001b[39m: [\u001b[39m'\u001b[39m\u001b[39mA\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mB\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mC\u001b[39m\u001b[39m'\u001b[39m]})\n",
      "File \u001b[0;32m<string>:2\u001b[0m, in \u001b[0;36mcreate_engine\u001b[0;34m(url, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/sqlalchemy/util/deprecations.py:281\u001b[0m, in \u001b[0;36mdeprecated_params.<locals>.decorate.<locals>.warned\u001b[0;34m(fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    274\u001b[0m     \u001b[39mif\u001b[39;00m m \u001b[39min\u001b[39;00m kwargs:\n\u001b[1;32m    275\u001b[0m         _warn_with_version(\n\u001b[1;32m    276\u001b[0m             messages[m],\n\u001b[1;32m    277\u001b[0m             versions[m],\n\u001b[1;32m    278\u001b[0m             version_warnings[m],\n\u001b[1;32m    279\u001b[0m             stacklevel\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m,\n\u001b[1;32m    280\u001b[0m         )\n\u001b[0;32m--> 281\u001b[0m \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/sqlalchemy/engine/create.py:552\u001b[0m, in \u001b[0;36mcreate_engine\u001b[0;34m(url, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m u \u001b[39m=\u001b[39m _url\u001b[39m.\u001b[39mmake_url(url)\n\u001b[1;32m    550\u001b[0m u, plugins, kwargs \u001b[39m=\u001b[39m u\u001b[39m.\u001b[39m_instantiate_plugins(kwargs)\n\u001b[0;32m--> 552\u001b[0m entrypoint \u001b[39m=\u001b[39m u\u001b[39m.\u001b[39;49m_get_entrypoint()\n\u001b[1;32m    553\u001b[0m _is_async \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39m_is_async\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m    554\u001b[0m \u001b[39mif\u001b[39;00m _is_async:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/sqlalchemy/engine/url.py:754\u001b[0m, in \u001b[0;36mURL._get_entrypoint\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    752\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    753\u001b[0m     name \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdrivername\u001b[39m.\u001b[39mreplace(\u001b[39m\"\u001b[39m\u001b[39m+\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 754\u001b[0m \u001b[39mcls\u001b[39m \u001b[39m=\u001b[39m registry\u001b[39m.\u001b[39;49mload(name)\n\u001b[1;32m    755\u001b[0m \u001b[39m# check for legacy dialects that\u001b[39;00m\n\u001b[1;32m    756\u001b[0m \u001b[39m# would return a module with 'dialect' as the\u001b[39;00m\n\u001b[1;32m    757\u001b[0m \u001b[39m# actual class\u001b[39;00m\n\u001b[1;32m    758\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    759\u001b[0m     \u001b[39mhasattr\u001b[39m(\u001b[39mcls\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mdialect\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    760\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mdialect, \u001b[39mtype\u001b[39m)\n\u001b[1;32m    761\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39missubclass\u001b[39m(\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mdialect, Dialect)\n\u001b[1;32m    762\u001b[0m ):\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/sqlalchemy/util/langhelpers.py:368\u001b[0m, in \u001b[0;36mPluginLoader.load\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    365\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimpls[name] \u001b[39m=\u001b[39m impl\u001b[39m.\u001b[39mload\n\u001b[1;32m    366\u001b[0m         \u001b[39mreturn\u001b[39;00m impl\u001b[39m.\u001b[39mload()\n\u001b[0;32m--> 368\u001b[0m \u001b[39mraise\u001b[39;00m exc\u001b[39m.\u001b[39mNoSuchModuleError(\n\u001b[1;32m    369\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mCan\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt load plugin: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m:\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroup, name)\n\u001b[1;32m    370\u001b[0m )\n",
      "\u001b[0;31mNoSuchModuleError\u001b[0m: Can't load plugin: sqlalchemy.dialects:db"
     ]
    }
   ],
   "source": [
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "import os\n",
    "MYSQL_USER = os.getenv('MYSQL_USER')\n",
    "MYSQL_PASSWORD = os.getenv('MYSQL_PASSWORD')\n",
    "MYSQL_DATABASE = os.getenv('MYSQL_DATABASE')\n",
    "table_name = 'test_table'\n",
    "connection_string = f\"mysql+mysqlconnector://${MYSQL_USER}:${MYSQL_PASSWORD}@db:3306/${MYSQL_DATABASE}\"\n",
    "engine = create_engine(connection_string)\n",
    "\n",
    "# Example DataFrame to save\n",
    "data = pd.DataFrame({'column1': [1, 2, 3], 'column2': ['A', 'B', 'C']})\n",
    "\n",
    "# Save the DataFrame to the MySQL table\n",
    "data.to_sql(table_name, engine, index=False, if_exists='replace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] minio url:  s3:9000\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from minio import Minio\n",
    "from minio.error import InvalidResponseError\n",
    "\n",
    "accessID = os.environ.get('AWS_ACCESS_KEY_ID')\n",
    "accessSecret =  os.environ.get('AWS_SECRET_ACCESS_KEY')\n",
    "minioUrl =  os.environ.get('MLFLOW_S3_ENDPOINT_URL')\n",
    "minioUrl = 'http://s3:9000'\n",
    "bucketName =  'ree'\n",
    "\n",
    "if accessID == None:\n",
    "    print('[!] AWS_ACCESS_KEY_ID environment variable is empty! run \\'source .env\\' to load it from the .env file')\n",
    "    exit(1)\n",
    "\n",
    "if accessSecret == None:\n",
    "    print('[!] AWS_SECRET_ACCESS_KEY environment variable is empty! run \\'source .env\\' to load it from the .env file')\n",
    "    exit(1)\n",
    "\n",
    "if minioUrl == None:\n",
    "    print('[!] MLFLOW_S3_ENDPOINT_URL environment variable is empty! run \\'source .env\\' to load it from the .env file')\n",
    "    exit(1)\n",
    "\n",
    "    \n",
    "if bucketName == None:\n",
    "    print('[!] AWS_BUCKET_NAME environment variable is empty! run \\'source .env\\' to load it from the .env file')\n",
    "    exit(1)\n",
    "\n",
    "minioUrlHostWithPort = minioUrl.split('//')[1]\n",
    "print('[*] minio url: ',minioUrlHostWithPort)\n",
    "\n",
    "s3Client = Minio(\n",
    "    minioUrlHostWithPort,\n",
    "    access_key=accessID,\n",
    "    secret_key=accessSecret,\n",
    "    secure=False\n",
    ")\n",
    "\n",
    "s3Client.make_bucket(bucketName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "EndpointConnectionError",
     "evalue": "Could not connect to the endpoint URL: \"http://s3:9000/mlflow/0/9086adfc1adf44608913bd078e9d6ac6/artifacts/00-skeleton.ipynb\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mgaierror\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/urllib3/connection.py:159\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 159\u001b[0m     conn \u001b[39m=\u001b[39m connection\u001b[39m.\u001b[39;49mcreate_connection(\n\u001b[1;32m    160\u001b[0m         (\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dns_host, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mport), \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtimeout, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mextra_kw\n\u001b[1;32m    161\u001b[0m     )\n\u001b[1;32m    163\u001b[0m \u001b[39mexcept\u001b[39;00m SocketTimeout:\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/urllib3/util/connection.py:61\u001b[0m, in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     59\u001b[0m family \u001b[39m=\u001b[39m allowed_gai_family()\n\u001b[0;32m---> 61\u001b[0m \u001b[39mfor\u001b[39;00m res \u001b[39min\u001b[39;00m socket\u001b[39m.\u001b[39;49mgetaddrinfo(host, port, family, socket\u001b[39m.\u001b[39;49mSOCK_STREAM):\n\u001b[1;32m     62\u001b[0m     af, socktype, proto, canonname, sa \u001b[39m=\u001b[39m res\n",
      "File \u001b[0;32m/usr/lib/python3.8/socket.py:918\u001b[0m, in \u001b[0;36mgetaddrinfo\u001b[0;34m(host, port, family, type, proto, flags)\u001b[0m\n\u001b[1;32m    917\u001b[0m addrlist \u001b[39m=\u001b[39m []\n\u001b[0;32m--> 918\u001b[0m \u001b[39mfor\u001b[39;00m res \u001b[39min\u001b[39;00m _socket\u001b[39m.\u001b[39;49mgetaddrinfo(host, port, family, \u001b[39mtype\u001b[39;49m, proto, flags):\n\u001b[1;32m    919\u001b[0m     af, socktype, proto, canonname, sa \u001b[39m=\u001b[39m res\n",
      "\u001b[0;31mgaierror\u001b[0m: [Errno -2] Name or service not known",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNewConnectionError\u001b[0m                        Traceback (most recent call last)",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/botocore/httpsession.py:465\u001b[0m, in \u001b[0;36mURLLib3Session.send\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    464\u001b[0m request_target \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_request_target(request\u001b[39m.\u001b[39murl, proxy_url)\n\u001b[0;32m--> 465\u001b[0m urllib_response \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49murlopen(\n\u001b[1;32m    466\u001b[0m     method\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mmethod,\n\u001b[1;32m    467\u001b[0m     url\u001b[39m=\u001b[39;49mrequest_target,\n\u001b[1;32m    468\u001b[0m     body\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mbody,\n\u001b[1;32m    469\u001b[0m     headers\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mheaders,\n\u001b[1;32m    470\u001b[0m     retries\u001b[39m=\u001b[39;49mRetry(\u001b[39mFalse\u001b[39;49;00m),\n\u001b[1;32m    471\u001b[0m     assert_same_host\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    472\u001b[0m     preload_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    473\u001b[0m     decode_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    474\u001b[0m     chunked\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_chunked(request\u001b[39m.\u001b[39;49mheaders),\n\u001b[1;32m    475\u001b[0m )\n\u001b[1;32m    477\u001b[0m http_response \u001b[39m=\u001b[39m botocore\u001b[39m.\u001b[39mawsrequest\u001b[39m.\u001b[39mAWSResponse(\n\u001b[1;32m    478\u001b[0m     request\u001b[39m.\u001b[39murl,\n\u001b[1;32m    479\u001b[0m     urllib_response\u001b[39m.\u001b[39mstatus,\n\u001b[1;32m    480\u001b[0m     urllib_response\u001b[39m.\u001b[39mheaders,\n\u001b[1;32m    481\u001b[0m     urllib_response,\n\u001b[1;32m    482\u001b[0m )\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/urllib3/connectionpool.py:719\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    717\u001b[0m     e \u001b[39m=\u001b[39m ProtocolError(\u001b[39m\"\u001b[39m\u001b[39mConnection aborted.\u001b[39m\u001b[39m\"\u001b[39m, e)\n\u001b[0;32m--> 719\u001b[0m retries \u001b[39m=\u001b[39m retries\u001b[39m.\u001b[39;49mincrement(\n\u001b[1;32m    720\u001b[0m     method, url, error\u001b[39m=\u001b[39;49me, _pool\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m, _stacktrace\u001b[39m=\u001b[39;49msys\u001b[39m.\u001b[39;49mexc_info()[\u001b[39m2\u001b[39;49m]\n\u001b[1;32m    721\u001b[0m )\n\u001b[1;32m    722\u001b[0m retries\u001b[39m.\u001b[39msleep()\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/urllib3/util/retry.py:376\u001b[0m, in \u001b[0;36mRetry.increment\u001b[0;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[1;32m    374\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtotal \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m \u001b[39mand\u001b[39;00m error:\n\u001b[1;32m    375\u001b[0m     \u001b[39m# Disabled, indicate to re-raise the error.\u001b[39;00m\n\u001b[0;32m--> 376\u001b[0m     \u001b[39mraise\u001b[39;00m six\u001b[39m.\u001b[39;49mreraise(\u001b[39mtype\u001b[39;49m(error), error, _stacktrace)\n\u001b[1;32m    378\u001b[0m total \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtotal\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/six.py:703\u001b[0m, in \u001b[0;36mreraise\u001b[0;34m(tp, value, tb)\u001b[0m\n\u001b[1;32m    702\u001b[0m         \u001b[39mraise\u001b[39;00m value\u001b[39m.\u001b[39mwith_traceback(tb)\n\u001b[0;32m--> 703\u001b[0m     \u001b[39mraise\u001b[39;00m value\n\u001b[1;32m    704\u001b[0m \u001b[39mfinally\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/urllib3/connectionpool.py:665\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    664\u001b[0m \u001b[39m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[0;32m--> 665\u001b[0m httplib_response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_request(\n\u001b[1;32m    666\u001b[0m     conn,\n\u001b[1;32m    667\u001b[0m     method,\n\u001b[1;32m    668\u001b[0m     url,\n\u001b[1;32m    669\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout_obj,\n\u001b[1;32m    670\u001b[0m     body\u001b[39m=\u001b[39;49mbody,\n\u001b[1;32m    671\u001b[0m     headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    672\u001b[0m     chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[1;32m    673\u001b[0m )\n\u001b[1;32m    675\u001b[0m \u001b[39m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[1;32m    676\u001b[0m \u001b[39m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[1;32m    677\u001b[0m \u001b[39m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[1;32m    678\u001b[0m \u001b[39m# mess.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/urllib3/connectionpool.py:387\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    386\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 387\u001b[0m     conn\u001b[39m.\u001b[39;49mrequest(method, url, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mhttplib_request_kw)\n\u001b[1;32m    389\u001b[0m \u001b[39m# Reset the timeout for the recv() on the socket\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.8/http/client.py:1256\u001b[0m, in \u001b[0;36mHTTPConnection.request\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1255\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Send a complete request to the server.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1256\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_send_request(method, url, body, headers, encode_chunked)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/botocore/awsrequest.py:94\u001b[0m, in \u001b[0;36mAWSConnection._send_request\u001b[0;34m(self, method, url, body, headers, *args, **kwargs)\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresponse_class \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_original_response_cls\n\u001b[0;32m---> 94\u001b[0m rval \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m_send_request(\n\u001b[1;32m     95\u001b[0m     method, url, body, headers, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m     96\u001b[0m )\n\u001b[1;32m     97\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_expect_header_set \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.8/http/client.py:1302\u001b[0m, in \u001b[0;36mHTTPConnection._send_request\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1301\u001b[0m     body \u001b[39m=\u001b[39m _encode(body, \u001b[39m'\u001b[39m\u001b[39mbody\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m-> 1302\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mendheaders(body, encode_chunked\u001b[39m=\u001b[39;49mencode_chunked)\n",
      "File \u001b[0;32m/usr/lib/python3.8/http/client.py:1251\u001b[0m, in \u001b[0;36mHTTPConnection.endheaders\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1250\u001b[0m     \u001b[39mraise\u001b[39;00m CannotSendHeader()\n\u001b[0;32m-> 1251\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_send_output(message_body, encode_chunked\u001b[39m=\u001b[39;49mencode_chunked)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/botocore/awsrequest.py:123\u001b[0m, in \u001b[0;36mAWSConnection._send_output\u001b[0;34m(self, message_body, *args, **kwargs)\u001b[0m\n\u001b[1;32m    122\u001b[0m     message_body \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 123\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend(msg)\n\u001b[1;32m    124\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_expect_header_set:\n\u001b[1;32m    125\u001b[0m     \u001b[39m# This is our custom behavior.  If the Expect header was\u001b[39;00m\n\u001b[1;32m    126\u001b[0m     \u001b[39m# set, it will trigger this custom behavior.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/botocore/awsrequest.py:218\u001b[0m, in \u001b[0;36mAWSConnection.send\u001b[0;34m(self, str)\u001b[0m\n\u001b[1;32m    217\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m--> 218\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49msend(\u001b[39mstr\u001b[39;49m)\n",
      "File \u001b[0;32m/usr/lib/python3.8/http/client.py:951\u001b[0m, in \u001b[0;36mHTTPConnection.send\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    950\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_open:\n\u001b[0;32m--> 951\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconnect()\n\u001b[1;32m    952\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/urllib3/connection.py:187\u001b[0m, in \u001b[0;36mHTTPConnection.connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mconnect\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m--> 187\u001b[0m     conn \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_new_conn()\n\u001b[1;32m    188\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prepare_conn(conn)\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/urllib3/connection.py:171\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[39mexcept\u001b[39;00m SocketError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m--> 171\u001b[0m     \u001b[39mraise\u001b[39;00m NewConnectionError(\n\u001b[1;32m    172\u001b[0m         \u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mFailed to establish a new connection: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m e\n\u001b[1;32m    173\u001b[0m     )\n\u001b[1;32m    175\u001b[0m \u001b[39mreturn\u001b[39;00m conn\n",
      "\u001b[0;31mNewConnectionError\u001b[0m: <botocore.awsrequest.AWSHTTPConnection object at 0x7fc530589e50>: Failed to establish a new connection: [Errno -2] Name or service not known",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mEndpointConnectionError\u001b[0m                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m mlflow\u001b[39m.\u001b[39mlog_metric(\u001b[39m\"\u001b[39m\u001b[39mfoo\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m2\u001b[39m)\n\u001b[1;32m      9\u001b[0m mlflow\u001b[39m.\u001b[39mlog_metric(\u001b[39m\"\u001b[39m\u001b[39mfoo\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m3\u001b[39m)\n\u001b[0;32m---> 10\u001b[0m mlflow\u001b[39m.\u001b[39;49mlog_artifact(\u001b[39m\"\u001b[39;49m\u001b[39m00-skeleton.ipynb\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/mlflow/tracking/fluent.py:877\u001b[0m, in \u001b[0;36mlog_artifact\u001b[0;34m(local_path, artifact_path)\u001b[0m\n\u001b[1;32m    854\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    855\u001b[0m \u001b[39mLog a local file or directory as an artifact of the currently active run. If no run is\u001b[39;00m\n\u001b[1;32m    856\u001b[0m \u001b[39mactive, this method will create a new active run.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    874\u001b[0m \u001b[39m        mlflow.log_artifact(\"features.txt\")\u001b[39;00m\n\u001b[1;32m    875\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    876\u001b[0m run_id \u001b[39m=\u001b[39m _get_or_start_run()\u001b[39m.\u001b[39minfo\u001b[39m.\u001b[39mrun_id\n\u001b[0;32m--> 877\u001b[0m MlflowClient()\u001b[39m.\u001b[39;49mlog_artifact(run_id, local_path, artifact_path)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/mlflow/tracking/client.py:1091\u001b[0m, in \u001b[0;36mMlflowClient.log_artifact\u001b[0;34m(self, run_id, local_path, artifact_path)\u001b[0m\n\u001b[1;32m   1056\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlog_artifact\u001b[39m(\u001b[39mself\u001b[39m, run_id, local_path, artifact_path\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1057\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1058\u001b[0m \u001b[39m    Write a local file or directory to the remote ``artifact_uri``.\u001b[39;00m\n\u001b[1;32m   1059\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1089\u001b[0m \u001b[39m        is_dir: False\u001b[39;00m\n\u001b[1;32m   1090\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1091\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_tracking_client\u001b[39m.\u001b[39;49mlog_artifact(run_id, local_path, artifact_path)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/mlflow/tracking/_tracking_service/client.py:456\u001b[0m, in \u001b[0;36mTrackingServiceClient.log_artifact\u001b[0;34m(self, run_id, local_path, artifact_path)\u001b[0m\n\u001b[1;32m    454\u001b[0m     artifact_repo\u001b[39m.\u001b[39mlog_artifacts(local_path, path_name)\n\u001b[1;32m    455\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 456\u001b[0m     artifact_repo\u001b[39m.\u001b[39;49mlog_artifact(local_path, artifact_path)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/mlflow/store/artifact/s3_artifact_repo.py:153\u001b[0m, in \u001b[0;36mS3ArtifactRepository.log_artifact\u001b[0;34m(self, local_file, artifact_path)\u001b[0m\n\u001b[1;32m    151\u001b[0m     dest_path \u001b[39m=\u001b[39m posixpath\u001b[39m.\u001b[39mjoin(dest_path, artifact_path)\n\u001b[1;32m    152\u001b[0m dest_path \u001b[39m=\u001b[39m posixpath\u001b[39m.\u001b[39mjoin(dest_path, os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mbasename(local_file))\n\u001b[0;32m--> 153\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_upload_file(\n\u001b[1;32m    154\u001b[0m     s3_client\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_s3_client(), local_file\u001b[39m=\u001b[39;49mlocal_file, bucket\u001b[39m=\u001b[39;49mbucket, key\u001b[39m=\u001b[39;49mdest_path\n\u001b[1;32m    155\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/mlflow/store/artifact/s3_artifact_repo.py:146\u001b[0m, in \u001b[0;36mS3ArtifactRepository._upload_file\u001b[0;34m(self, s3_client, local_file, bucket, key)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[39mif\u001b[39;00m environ_extra_args \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    145\u001b[0m     extra_args\u001b[39m.\u001b[39mupdate(environ_extra_args)\n\u001b[0;32m--> 146\u001b[0m s3_client\u001b[39m.\u001b[39;49mupload_file(Filename\u001b[39m=\u001b[39;49mlocal_file, Bucket\u001b[39m=\u001b[39;49mbucket, Key\u001b[39m=\u001b[39;49mkey, ExtraArgs\u001b[39m=\u001b[39;49mextra_args)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/boto3/s3/inject.py:143\u001b[0m, in \u001b[0;36mupload_file\u001b[0;34m(self, Filename, Bucket, Key, ExtraArgs, Callback, Config)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Upload a file to an S3 object.\u001b[39;00m\n\u001b[1;32m    109\u001b[0m \n\u001b[1;32m    110\u001b[0m \u001b[39mUsage::\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[39m    transfer.\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    142\u001b[0m \u001b[39mwith\u001b[39;00m S3Transfer(\u001b[39mself\u001b[39m, Config) \u001b[39mas\u001b[39;00m transfer:\n\u001b[0;32m--> 143\u001b[0m     \u001b[39mreturn\u001b[39;00m transfer\u001b[39m.\u001b[39;49mupload_file(\n\u001b[1;32m    144\u001b[0m         filename\u001b[39m=\u001b[39;49mFilename,\n\u001b[1;32m    145\u001b[0m         bucket\u001b[39m=\u001b[39;49mBucket,\n\u001b[1;32m    146\u001b[0m         key\u001b[39m=\u001b[39;49mKey,\n\u001b[1;32m    147\u001b[0m         extra_args\u001b[39m=\u001b[39;49mExtraArgs,\n\u001b[1;32m    148\u001b[0m         callback\u001b[39m=\u001b[39;49mCallback,\n\u001b[1;32m    149\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/boto3/s3/transfer.py:292\u001b[0m, in \u001b[0;36mS3Transfer.upload_file\u001b[0;34m(self, filename, bucket, key, callback, extra_args)\u001b[0m\n\u001b[1;32m    288\u001b[0m future \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_manager\u001b[39m.\u001b[39mupload(\n\u001b[1;32m    289\u001b[0m     filename, bucket, key, extra_args, subscribers\n\u001b[1;32m    290\u001b[0m )\n\u001b[1;32m    291\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 292\u001b[0m     future\u001b[39m.\u001b[39;49mresult()\n\u001b[1;32m    293\u001b[0m \u001b[39m# If a client error was raised, add the backwards compatibility layer\u001b[39;00m\n\u001b[1;32m    294\u001b[0m \u001b[39m# that raises a S3UploadFailedError. These specific errors were only\u001b[39;00m\n\u001b[1;32m    295\u001b[0m \u001b[39m# ever thrown for upload_parts but now can be thrown for any related\u001b[39;00m\n\u001b[1;32m    296\u001b[0m \u001b[39m# client error.\u001b[39;00m\n\u001b[1;32m    297\u001b[0m \u001b[39mexcept\u001b[39;00m ClientError \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/s3transfer/futures.py:103\u001b[0m, in \u001b[0;36mTransferFuture.result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mresult\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m     99\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    100\u001b[0m         \u001b[39m# Usually the result() method blocks until the transfer is done,\u001b[39;00m\n\u001b[1;32m    101\u001b[0m         \u001b[39m# however if a KeyboardInterrupt is raised we want want to exit\u001b[39;00m\n\u001b[1;32m    102\u001b[0m         \u001b[39m# out of this and propagate the exception.\u001b[39;00m\n\u001b[0;32m--> 103\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_coordinator\u001b[39m.\u001b[39;49mresult()\n\u001b[1;32m    104\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mKeyboardInterrupt\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    105\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcancel()\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/s3transfer/futures.py:266\u001b[0m, in \u001b[0;36mTransferCoordinator.result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[39m# Once done waiting, raise an exception if present or return the\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[39m# final result.\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception:\n\u001b[0;32m--> 266\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception\n\u001b[1;32m    267\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_result\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/s3transfer/tasks.py:139\u001b[0m, in \u001b[0;36mTask.__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[39m# If the task is not done (really only if some other related\u001b[39;00m\n\u001b[1;32m    136\u001b[0m     \u001b[39m# task to the TransferFuture had failed) then execute the task's\u001b[39;00m\n\u001b[1;32m    137\u001b[0m     \u001b[39m# main() method.\u001b[39;00m\n\u001b[1;32m    138\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_transfer_coordinator\u001b[39m.\u001b[39mdone():\n\u001b[0;32m--> 139\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_execute_main(kwargs)\n\u001b[1;32m    140\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    141\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_log_and_set_exception(e)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/s3transfer/tasks.py:162\u001b[0m, in \u001b[0;36mTask._execute_main\u001b[0;34m(self, kwargs)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[39m# Log what is about to be executed.\u001b[39;00m\n\u001b[1;32m    160\u001b[0m logger\u001b[39m.\u001b[39mdebug(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mExecuting task \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m with kwargs \u001b[39m\u001b[39m{\u001b[39;00mkwargs_to_display\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 162\u001b[0m return_value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_main(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    163\u001b[0m \u001b[39m# If the task is the final task, then set the TransferFuture's\u001b[39;00m\n\u001b[1;32m    164\u001b[0m \u001b[39m# value to the return value from main().\u001b[39;00m\n\u001b[1;32m    165\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_is_final:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/s3transfer/upload.py:758\u001b[0m, in \u001b[0;36mPutObjectTask._main\u001b[0;34m(self, client, fileobj, bucket, key, extra_args)\u001b[0m\n\u001b[1;32m    749\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    750\u001b[0m \u001b[39m:param client: The client to use when calling PutObject\u001b[39;00m\n\u001b[1;32m    751\u001b[0m \u001b[39m:param fileobj: The file to upload.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    755\u001b[0m \u001b[39m    used in the upload.\u001b[39;00m\n\u001b[1;32m    756\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    757\u001b[0m \u001b[39mwith\u001b[39;00m fileobj \u001b[39mas\u001b[39;00m body:\n\u001b[0;32m--> 758\u001b[0m     client\u001b[39m.\u001b[39;49mput_object(Bucket\u001b[39m=\u001b[39;49mbucket, Key\u001b[39m=\u001b[39;49mkey, Body\u001b[39m=\u001b[39;49mbody, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mextra_args)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/botocore/client.py:530\u001b[0m, in \u001b[0;36mClientCreator._create_api_method.<locals>._api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    526\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[1;32m    527\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mpy_operation_name\u001b[39m}\u001b[39;00m\u001b[39m() only accepts keyword arguments.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    528\u001b[0m     )\n\u001b[1;32m    529\u001b[0m \u001b[39m# The \"self\" in this scope is referring to the BaseClient.\u001b[39;00m\n\u001b[0;32m--> 530\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_api_call(operation_name, kwargs)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/botocore/client.py:947\u001b[0m, in \u001b[0;36mBaseClient._make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m    945\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    946\u001b[0m     apply_request_checksum(request_dict)\n\u001b[0;32m--> 947\u001b[0m     http, parsed_response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_request(\n\u001b[1;32m    948\u001b[0m         operation_model, request_dict, request_context\n\u001b[1;32m    949\u001b[0m     )\n\u001b[1;32m    951\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmeta\u001b[39m.\u001b[39mevents\u001b[39m.\u001b[39memit(\n\u001b[1;32m    952\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mafter-call.\u001b[39m\u001b[39m{service_id}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{operation_name}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m    953\u001b[0m         service_id\u001b[39m=\u001b[39mservice_id, operation_name\u001b[39m=\u001b[39moperation_name\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    958\u001b[0m     context\u001b[39m=\u001b[39mrequest_context,\n\u001b[1;32m    959\u001b[0m )\n\u001b[1;32m    961\u001b[0m \u001b[39mif\u001b[39;00m http\u001b[39m.\u001b[39mstatus_code \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m300\u001b[39m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/botocore/client.py:970\u001b[0m, in \u001b[0;36mBaseClient._make_request\u001b[0;34m(self, operation_model, request_dict, request_context)\u001b[0m\n\u001b[1;32m    968\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_make_request\u001b[39m(\u001b[39mself\u001b[39m, operation_model, request_dict, request_context):\n\u001b[1;32m    969\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 970\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_endpoint\u001b[39m.\u001b[39;49mmake_request(operation_model, request_dict)\n\u001b[1;32m    971\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    972\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmeta\u001b[39m.\u001b[39mevents\u001b[39m.\u001b[39memit(\n\u001b[1;32m    973\u001b[0m             \u001b[39m'\u001b[39m\u001b[39mafter-call-error.\u001b[39m\u001b[39m{service_id}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{operation_name}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m    974\u001b[0m                 service_id\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_service_model\u001b[39m.\u001b[39mservice_id\u001b[39m.\u001b[39mhyphenize(),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    978\u001b[0m             context\u001b[39m=\u001b[39mrequest_context,\n\u001b[1;32m    979\u001b[0m         )\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/botocore/endpoint.py:119\u001b[0m, in \u001b[0;36mEndpoint.make_request\u001b[0;34m(self, operation_model, request_dict)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmake_request\u001b[39m(\u001b[39mself\u001b[39m, operation_model, request_dict):\n\u001b[1;32m    114\u001b[0m     logger\u001b[39m.\u001b[39mdebug(\n\u001b[1;32m    115\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mMaking request for \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m with params: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    116\u001b[0m         operation_model,\n\u001b[1;32m    117\u001b[0m         request_dict,\n\u001b[1;32m    118\u001b[0m     )\n\u001b[0;32m--> 119\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_send_request(request_dict, operation_model)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/botocore/endpoint.py:202\u001b[0m, in \u001b[0;36mEndpoint._send_request\u001b[0;34m(self, request_dict, operation_model)\u001b[0m\n\u001b[1;32m    198\u001b[0m request \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcreate_request(request_dict, operation_model)\n\u001b[1;32m    199\u001b[0m success_response, exception \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_response(\n\u001b[1;32m    200\u001b[0m     request, operation_model, context\n\u001b[1;32m    201\u001b[0m )\n\u001b[0;32m--> 202\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_needs_retry(\n\u001b[1;32m    203\u001b[0m     attempts,\n\u001b[1;32m    204\u001b[0m     operation_model,\n\u001b[1;32m    205\u001b[0m     request_dict,\n\u001b[1;32m    206\u001b[0m     success_response,\n\u001b[1;32m    207\u001b[0m     exception,\n\u001b[1;32m    208\u001b[0m ):\n\u001b[1;32m    209\u001b[0m     attempts \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    210\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_update_retries_context(context, attempts, success_response)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/botocore/endpoint.py:354\u001b[0m, in \u001b[0;36mEndpoint._needs_retry\u001b[0;34m(self, attempts, operation_model, request_dict, response, caught_exception)\u001b[0m\n\u001b[1;32m    352\u001b[0m service_id \u001b[39m=\u001b[39m operation_model\u001b[39m.\u001b[39mservice_model\u001b[39m.\u001b[39mservice_id\u001b[39m.\u001b[39mhyphenize()\n\u001b[1;32m    353\u001b[0m event_name \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mneeds-retry.\u001b[39m\u001b[39m{\u001b[39;00mservice_id\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00moperation_model\u001b[39m.\u001b[39mname\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 354\u001b[0m responses \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_event_emitter\u001b[39m.\u001b[39;49memit(\n\u001b[1;32m    355\u001b[0m     event_name,\n\u001b[1;32m    356\u001b[0m     response\u001b[39m=\u001b[39;49mresponse,\n\u001b[1;32m    357\u001b[0m     endpoint\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[1;32m    358\u001b[0m     operation\u001b[39m=\u001b[39;49moperation_model,\n\u001b[1;32m    359\u001b[0m     attempts\u001b[39m=\u001b[39;49mattempts,\n\u001b[1;32m    360\u001b[0m     caught_exception\u001b[39m=\u001b[39;49mcaught_exception,\n\u001b[1;32m    361\u001b[0m     request_dict\u001b[39m=\u001b[39;49mrequest_dict,\n\u001b[1;32m    362\u001b[0m )\n\u001b[1;32m    363\u001b[0m handler_response \u001b[39m=\u001b[39m first_non_none_response(responses)\n\u001b[1;32m    364\u001b[0m \u001b[39mif\u001b[39;00m handler_response \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/botocore/hooks.py:412\u001b[0m, in \u001b[0;36mEventAliaser.emit\u001b[0;34m(self, event_name, **kwargs)\u001b[0m\n\u001b[1;32m    410\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39memit\u001b[39m(\u001b[39mself\u001b[39m, event_name, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    411\u001b[0m     aliased_event_name \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_alias_event_name(event_name)\n\u001b[0;32m--> 412\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_emitter\u001b[39m.\u001b[39;49memit(aliased_event_name, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/botocore/hooks.py:256\u001b[0m, in \u001b[0;36mHierarchicalEmitter.emit\u001b[0;34m(self, event_name, **kwargs)\u001b[0m\n\u001b[1;32m    245\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39memit\u001b[39m(\u001b[39mself\u001b[39m, event_name, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    246\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    247\u001b[0m \u001b[39m    Emit an event by name with arguments passed as keyword args.\u001b[39;00m\n\u001b[1;32m    248\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[39m             handlers.\u001b[39;00m\n\u001b[1;32m    255\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 256\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_emit(event_name, kwargs)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/botocore/hooks.py:239\u001b[0m, in \u001b[0;36mHierarchicalEmitter._emit\u001b[0;34m(self, event_name, kwargs, stop_on_response)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[39mfor\u001b[39;00m handler \u001b[39min\u001b[39;00m handlers_to_call:\n\u001b[1;32m    238\u001b[0m     logger\u001b[39m.\u001b[39mdebug(\u001b[39m'\u001b[39m\u001b[39mEvent \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m: calling handler \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m, event_name, handler)\n\u001b[0;32m--> 239\u001b[0m     response \u001b[39m=\u001b[39m handler(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    240\u001b[0m     responses\u001b[39m.\u001b[39mappend((handler, response))\n\u001b[1;32m    241\u001b[0m     \u001b[39mif\u001b[39;00m stop_on_response \u001b[39mand\u001b[39;00m response \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/botocore/retryhandler.py:207\u001b[0m, in \u001b[0;36mRetryHandler.__call__\u001b[0;34m(self, attempts, response, caught_exception, **kwargs)\u001b[0m\n\u001b[1;32m    204\u001b[0m     retries_context \u001b[39m=\u001b[39m kwargs[\u001b[39m'\u001b[39m\u001b[39mrequest_dict\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mcontext\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mretries\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    205\u001b[0m     checker_kwargs\u001b[39m.\u001b[39mupdate({\u001b[39m'\u001b[39m\u001b[39mretries_context\u001b[39m\u001b[39m'\u001b[39m: retries_context})\n\u001b[0;32m--> 207\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_checker(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mchecker_kwargs):\n\u001b[1;32m    208\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_action(attempts\u001b[39m=\u001b[39mattempts)\n\u001b[1;32m    209\u001b[0m     logger\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mRetry needed, action of: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m, result)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/botocore/retryhandler.py:284\u001b[0m, in \u001b[0;36mMaxAttemptsDecorator.__call__\u001b[0;34m(self, attempt_number, response, caught_exception, retries_context)\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[39mif\u001b[39;00m retries_context:\n\u001b[1;32m    280\u001b[0m     retries_context[\u001b[39m'\u001b[39m\u001b[39mmax\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mmax\u001b[39m(\n\u001b[1;32m    281\u001b[0m         retries_context\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mmax\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_max_attempts\n\u001b[1;32m    282\u001b[0m     )\n\u001b[0;32m--> 284\u001b[0m should_retry \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_should_retry(\n\u001b[1;32m    285\u001b[0m     attempt_number, response, caught_exception\n\u001b[1;32m    286\u001b[0m )\n\u001b[1;32m    287\u001b[0m \u001b[39mif\u001b[39;00m should_retry:\n\u001b[1;32m    288\u001b[0m     \u001b[39mif\u001b[39;00m attempt_number \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_max_attempts:\n\u001b[1;32m    289\u001b[0m         \u001b[39m# explicitly set MaxAttemptsReached\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/botocore/retryhandler.py:320\u001b[0m, in \u001b[0;36mMaxAttemptsDecorator._should_retry\u001b[0;34m(self, attempt_number, response, caught_exception)\u001b[0m\n\u001b[1;32m    316\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    317\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    318\u001b[0m     \u001b[39m# If we've exceeded the max attempts we just let the exception\u001b[39;00m\n\u001b[1;32m    319\u001b[0m     \u001b[39m# propogate if one has occurred.\u001b[39;00m\n\u001b[0;32m--> 320\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_checker(attempt_number, response, caught_exception)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/botocore/retryhandler.py:363\u001b[0m, in \u001b[0;36mMultiChecker.__call__\u001b[0;34m(self, attempt_number, response, caught_exception)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, attempt_number, response, caught_exception):\n\u001b[1;32m    362\u001b[0m     \u001b[39mfor\u001b[39;00m checker \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkers:\n\u001b[0;32m--> 363\u001b[0m         checker_response \u001b[39m=\u001b[39m checker(\n\u001b[1;32m    364\u001b[0m             attempt_number, response, caught_exception\n\u001b[1;32m    365\u001b[0m         )\n\u001b[1;32m    366\u001b[0m         \u001b[39mif\u001b[39;00m checker_response:\n\u001b[1;32m    367\u001b[0m             \u001b[39mreturn\u001b[39;00m checker_response\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/botocore/retryhandler.py:247\u001b[0m, in \u001b[0;36mBaseChecker.__call__\u001b[0;34m(self, attempt_number, response, caught_exception)\u001b[0m\n\u001b[1;32m    245\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_response(attempt_number, response)\n\u001b[1;32m    246\u001b[0m \u001b[39melif\u001b[39;00m caught_exception \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 247\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_check_caught_exception(\n\u001b[1;32m    248\u001b[0m         attempt_number, caught_exception\n\u001b[1;32m    249\u001b[0m     )\n\u001b[1;32m    250\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    251\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mBoth response and caught_exception are None.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/botocore/retryhandler.py:416\u001b[0m, in \u001b[0;36mExceptionRaiser._check_caught_exception\u001b[0;34m(self, attempt_number, caught_exception)\u001b[0m\n\u001b[1;32m    408\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_check_caught_exception\u001b[39m(\u001b[39mself\u001b[39m, attempt_number, caught_exception):\n\u001b[1;32m    409\u001b[0m     \u001b[39m# This is implementation specific, but this class is useful by\u001b[39;00m\n\u001b[1;32m    410\u001b[0m     \u001b[39m# coordinating with the MaxAttemptsDecorator.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    414\u001b[0m     \u001b[39m# the MaxAttemptsDecorator is not interested in retrying the exception\u001b[39;00m\n\u001b[1;32m    415\u001b[0m     \u001b[39m# then this exception just propogates out past the retry code.\u001b[39;00m\n\u001b[0;32m--> 416\u001b[0m     \u001b[39mraise\u001b[39;00m caught_exception\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/botocore/endpoint.py:281\u001b[0m, in \u001b[0;36mEndpoint._do_get_response\u001b[0;34m(self, request, operation_model, context)\u001b[0m\n\u001b[1;32m    279\u001b[0m     http_response \u001b[39m=\u001b[39m first_non_none_response(responses)\n\u001b[1;32m    280\u001b[0m     \u001b[39mif\u001b[39;00m http_response \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 281\u001b[0m         http_response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_send(request)\n\u001b[1;32m    282\u001b[0m \u001b[39mexcept\u001b[39;00m HTTPClientError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    283\u001b[0m     \u001b[39mreturn\u001b[39;00m (\u001b[39mNone\u001b[39;00m, e)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/botocore/endpoint.py:377\u001b[0m, in \u001b[0;36mEndpoint._send\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    376\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_send\u001b[39m(\u001b[39mself\u001b[39m, request):\n\u001b[0;32m--> 377\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhttp_session\u001b[39m.\u001b[39;49msend(request)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/botocore/httpsession.py:494\u001b[0m, in \u001b[0;36mURLLib3Session.send\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    492\u001b[0m     \u001b[39mraise\u001b[39;00m SSLError(endpoint_url\u001b[39m=\u001b[39mrequest\u001b[39m.\u001b[39murl, error\u001b[39m=\u001b[39me)\n\u001b[1;32m    493\u001b[0m \u001b[39mexcept\u001b[39;00m (NewConnectionError, socket\u001b[39m.\u001b[39mgaierror) \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m--> 494\u001b[0m     \u001b[39mraise\u001b[39;00m EndpointConnectionError(endpoint_url\u001b[39m=\u001b[39mrequest\u001b[39m.\u001b[39murl, error\u001b[39m=\u001b[39me)\n\u001b[1;32m    495\u001b[0m \u001b[39mexcept\u001b[39;00m ProxyError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    496\u001b[0m     \u001b[39mraise\u001b[39;00m ProxyConnectionError(\n\u001b[1;32m    497\u001b[0m         proxy_url\u001b[39m=\u001b[39mmask_proxy_url(proxy_url), error\u001b[39m=\u001b[39me\n\u001b[1;32m    498\u001b[0m     )\n",
      "\u001b[0;31mEndpointConnectionError\u001b[0m: Could not connect to the endpoint URL: \"http://s3:9000/mlflow/0/9086adfc1adf44608913bd078e9d6ac6/artifacts/00-skeleton.ipynb\""
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "# set url to your mlflow server\n",
    "mlflow.set_tracking_uri(\"http://mlflow:5000\")\n",
    "mlflow.set_experiment(\"Default\")\n",
    "with mlflow.start_run():\n",
    "    mlflow.log_param(\"param1\", 5)\n",
    "    mlflow.log_metric(\"foo\", 1)\n",
    "    mlflow.log_metric(\"foo\", 2)\n",
    "    mlflow.log_metric(\"foo\", 3)\n",
    "    mlflow.log_artifact(\"00-skeleton.ipynb\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-03 15:53:15.892267: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-08-03 15:53:15.916056: I tensorflow/core/platform/cpu_feature_guard.cc:183] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE3 SSE4.1 SSE4.2 AVX, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Please fix your imports. Module tensorflow.python.training.tracking.data_structures has been moved to tensorflow.python.trackable.data_structures. The old module will be deleted in version 2.11.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-03 15:53:17.353971: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:1013] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-08-03 15:53:17.354201: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:1013] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-08-03 15:53:17.354251: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:1013] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]: sparse_operation_kit is imported\n",
      "WARNING:tensorflow:Please fix your imports. Module tensorflow.python.training.tracking.base has been moved to tensorflow.python.trackable.base. The old module will be deleted in version 2.11.\n",
      "[SOK INFO] Import /usr/local/lib/python3.8/dist-packages/merlin_sok-1.2.0-py3.8-linux-x86_64.egg/sparse_operation_kit/lib/libsok_experiment.so\n",
      "[SOK INFO] Import /usr/local/lib/python3.8/dist-packages/merlin_sok-1.2.0-py3.8-linux-x86_64.egg/sparse_operation_kit/lib/libsok_experiment.so\n",
      "[SOK INFO] Initialize finished, communication tool: horovod\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-03 15:53:17.664784: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:1013] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-08-03 15:53:17.664936: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:1013] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-08-03 15:53:17.664985: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:1013] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-08-03 15:53:17.673325: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:1013] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-08-03 15:53:17.673424: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:1013] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-08-03 15:53:17.673474: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:1013] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-08-03 15:53:17.673508: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2023-08-03 15:53:17.673524: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1638] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6011 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import nvtabular as nvt\n",
    "import cudf\n",
    "import config\n",
    "import os\n",
    "# ignore warnings\n",
    "from merlin.core.dispatch import get_lib\n",
    "from merlin.schema.tags import Tags\n",
    "\n",
    "import time\n",
    "import merlin.models.tf as mm\n",
    "# from dask.distributed import Client, wait\n",
    "# from dask_cuda import LocalCUDACluster\n",
    "# from dask.utils import parse_bytes\n",
    "\n",
    "from merlin.schema.tags import Tags\n",
    "from nvtabular.ops import *\n",
    "\n",
    "from merlin.schema.tags import Tags\n",
    "from merlin.io.dataset import Dataset\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_parquet(os.path.join(config.data_raw_dir,'transactions.parquet'))\n",
    "customers = pd.read_parquet(os.path.join(config.data_raw_dir,'customers.parquet'))\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "merged_ids = pd.concat([train['customer_id'], customers['customer_id']])\n",
    "encoder.fit(merged_ids)\n",
    "train['customer_id'] = encoder.transform(train['customer_id'])\n",
    "customers['customer_id'] = encoder.transform(customers['customer_id'])\n",
    "customers.to_parquet(os.path.join(config.data_processed_dir,'customers_enc.parquet'))\n",
    "del customers\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert  t_dat column to datetime\n",
    "train['t_dat'] = pd.to_datetime(train['t_dat'])\n",
    "train['week'] = 104 -(train['t_dat'].max() - train['t_dat']).dt.days // 7\n",
    "start = 60\n",
    "end = 90\n",
    "valid = train[train.week>end]\n",
    "train = train[(train.week>start) & (train.week<=end)]\n",
    "valid = valid[valid['customer_id'].isin(train['customer_id'])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shape (8231156, 6)\n",
      "valid shape (3706105, 6)\n"
     ]
    }
   ],
   "source": [
    "print('train shape', train.shape)\n",
    "print('valid shape', valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of common customers between train and valid:  408937\n",
      "Number of common items between train and valid:  31933\n",
      "Number of customers in train:  780121\n",
      "Number of customers in valid:  408937\n",
      "Number of items in train:  55238\n",
      "Number of items in valid:  42459\n",
      "Number of customers in valid but not in train:  0\n",
      "Number of items in valid but not in train:  10526\n",
      "train shape:  (8231156, 6)\n",
      "valid shape:  (3706105, 6)\n"
     ]
    }
   ],
   "source": [
    "# see how many 'customer_id' are common between the two datasets train and valid and also items 'article_id'\n",
    "common = set(train['customer_id']).intersection(set(valid['customer_id']))\n",
    "common_items = set(train['article_id']).intersection(set(valid['article_id']))\n",
    "print('Number of common customers between train and valid: ', len(common))\n",
    "print('Number of common items between train and valid: ', len(common_items))\n",
    "print('Number of customers in train: ', len(train['customer_id'].unique()))\n",
    "print('Number of customers in valid: ', len(valid['customer_id'].unique()))\n",
    "print('Number of items in train: ', len(train['article_id'].unique()))\n",
    "print('Number of items in valid: ', len(valid['article_id'].unique()))\n",
    "print('Number of customers in valid but not in train: ', len(set(valid['customer_id']) - set(train['customer_id'])))\n",
    "print('Number of items in valid but not in train: ', len(set(valid['article_id']) - set(train['article_id'])))\n",
    "print('train shape: ', train.shape)\n",
    "print('valid shape: ', valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = os.path.join(config.data_processed_dir,'train', 'transactions.parquet')\n",
    "train.to_parquet(save_dir)\n",
    "\n",
    "save_dir = os.path.join(config.data_processed_dir,'valid', 'transactions.parquet')\n",
    "valid.to_parquet(save_dir)\n",
    "del train, valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = nvt.Dataset(os.path.join(config.data_processed_dir,'train', 'transactions.parquet'),engine=\"parquet\",  part_mem_fraction=0.1)\n",
    "valid = nvt.Dataset(os.path.join(config.data_processed_dir,'valid', 'transactions.parquet'),engine=\"parquet\", part_mem_fraction=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_id = [\"article_id\"] >> Categorify() >> TagAsItemID()\n",
    "customer_id = [\"customer_id\"] >> Categorify() >> TagAsUserID()\n",
    "price = [\"price\"] >> FillMissing(fill_val=0) >> Normalize() >> TagAsItemFeatures()\n",
    "len_session = [\"price\"] >> FillMissing(fill_val=0) >> Normalize() >> TagAsUserFeatures()\n",
    "\n",
    "\n",
    "features = article_id + customer_id + price + len_session\n",
    "\n",
    "wf = nvt.Workflow(features)\n",
    "wf.fit_transform(train).to_parquet(os.path.join(config.data_processed_dir, 'train_processed.parquet'))\n",
    "wf.transform(valid).to_parquet(os.path.join(config.data_processed_dir, 'validation_processed.parquet'))\n",
    "\n",
    "wf.save(os.path.join(config.data_processed_dir, 'workflow'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = Dataset(os.path.join(config.data_processed_dir, 'train_processed.parquet'))\n",
    "valid = Dataset(os.path.join(config.data_processed_dir, 'validation_processed.parquet'))\n",
    "schema = train.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>tags</th>\n",
       "      <th>dtype</th>\n",
       "      <th>is_list</th>\n",
       "      <th>is_ragged</th>\n",
       "      <th>properties.num_buckets</th>\n",
       "      <th>properties.freq_threshold</th>\n",
       "      <th>properties.max_size</th>\n",
       "      <th>properties.cat_path</th>\n",
       "      <th>properties.embedding_sizes.cardinality</th>\n",
       "      <th>properties.embedding_sizes.dimension</th>\n",
       "      <th>properties.domain.min</th>\n",
       "      <th>properties.domain.max</th>\n",
       "      <th>properties.domain.name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>article_id</td>\n",
       "      <td>(Tags.CATEGORICAL, Tags.ITEM, Tags.ID)</td>\n",
       "      <td>DType(name='int64', element_type=&lt;ElementType....</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>.//categories/unique.article_id.parquet</td>\n",
       "      <td>55241.0</td>\n",
       "      <td>512.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>55240.0</td>\n",
       "      <td>article_id</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>customer_id</td>\n",
       "      <td>(Tags.CATEGORICAL, Tags.ID, Tags.USER)</td>\n",
       "      <td>DType(name='int64', element_type=&lt;ElementType....</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>.//categories/unique.customer_id.parquet</td>\n",
       "      <td>780124.0</td>\n",
       "      <td>512.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>780123.0</td>\n",
       "      <td>customer_id</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>price</td>\n",
       "      <td>(Tags.ITEM, Tags.CONTINUOUS, Tags.USER)</td>\n",
       "      <td>DType(name='float64', element_type=&lt;ElementTyp...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "[{'name': 'article_id', 'tags': {<Tags.CATEGORICAL: 'categorical'>, <Tags.ITEM: 'item'>, <Tags.ID: 'id'>}, 'properties': {'num_buckets': None, 'freq_threshold': 0.0, 'max_size': 0.0, 'cat_path': './/categories/unique.article_id.parquet', 'embedding_sizes': {'cardinality': 55241.0, 'dimension': 512.0}, 'domain': {'min': 0, 'max': 55240, 'name': 'article_id'}}, 'dtype': DType(name='int64', element_type=<ElementType.Int: 'int'>, element_size=64, element_unit=None, signed=True, shape=Shape(dims=(Dimension(min=0, max=None),))), 'is_list': False, 'is_ragged': False}, {'name': 'customer_id', 'tags': {<Tags.CATEGORICAL: 'categorical'>, <Tags.ID: 'id'>, <Tags.USER: 'user'>}, 'properties': {'num_buckets': None, 'freq_threshold': 0.0, 'max_size': 0.0, 'cat_path': './/categories/unique.customer_id.parquet', 'embedding_sizes': {'cardinality': 780124.0, 'dimension': 512.0}, 'domain': {'min': 0, 'max': 780123, 'name': 'customer_id'}}, 'dtype': DType(name='int64', element_type=<ElementType.Int: 'int'>, element_size=64, element_unit=None, signed=True, shape=Shape(dims=(Dimension(min=0, max=None),))), 'is_list': False, 'is_ragged': False}, {'name': 'price', 'tags': {<Tags.ITEM: 'item'>, <Tags.CONTINUOUS: 'continuous'>, <Tags.USER: 'user'>}, 'properties': {}, 'dtype': DType(name='float64', element_type=<ElementType.Float: 'float'>, element_size=64, element_unit=None, signed=True, shape=Shape(dims=(Dimension(min=0, max=None),))), 'is_list': False, 'is_ragged': False}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schema = schema.select_by_tag([Tags.ITEM_ID, Tags.USER_ID, Tags.ITEM, Tags.USER])\n",
    "label_names = schema.select_by_tag(Tags.TARGET).column_names\n",
    "label_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = mm.TwoTowerModel(\n",
    "    schema,\n",
    "    query_tower=mm.MLPBlock([512, 256, 128, 64], no_activation_last_layer=True),\n",
    "    samplers=[mm.InBatchSampler()],\n",
    "    embedding_options=mm.EmbeddingOptions(infer_embedding_sizes=True),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-03 15:57:15.260244: I tensorflow/core/common_runtime/executor.cc:1209] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n",
      "\t [[{{node Placeholder/_0}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2057/2058 [============================>.] - ETA: 0s - loss: 5.4461 - recall_at_30: 0.3217 - ndcg_at_30: 0.1081 - regularization_loss: 0.0000e+00 - loss_batch: 5.4461"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-03 15:58:34.897051: I tensorflow/core/common_runtime/executor.cc:1209] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n",
      "\t [[{{node Placeholder/_0}}]]\n",
      "The sampler InBatchSampler returned no samples for this batch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2058/2058 [==============================] - 92s 44ms/step - loss: 5.4460 - recall_at_30: 0.3217 - ndcg_at_30: 0.1082 - regularization_loss: 0.0000e+00 - loss_batch: 5.4458 - val_loss: 5.3480 - val_recall_at_30: 0.3605 - val_ndcg_at_30: 0.1344 - val_regularization_loss: 0.0000e+00 - val_loss_batch: 4.1751\n",
      "Epoch 2/15\n",
      "2058/2058 [==============================] - 90s 44ms/step - loss: 5.2267 - recall_at_30: 0.3987 - ndcg_at_30: 0.1390 - regularization_loss: 0.0000e+00 - loss_batch: 5.2265 - val_loss: 5.1810 - val_recall_at_30: 0.3442 - val_ndcg_at_30: 0.1284 - val_regularization_loss: 0.0000e+00 - val_loss_batch: 4.1086\n",
      "Epoch 3/15\n",
      "2058/2058 [==============================] - 90s 44ms/step - loss: 4.8108 - recall_at_30: 0.4268 - ndcg_at_30: 0.1526 - regularization_loss: 0.0000e+00 - loss_batch: 4.8106 - val_loss: 4.9178 - val_recall_at_30: 0.3810 - val_ndcg_at_30: 0.1519 - val_regularization_loss: 0.0000e+00 - val_loss_batch: 3.8806\n",
      "Epoch 4/15\n",
      "2058/2058 [==============================] - 91s 44ms/step - loss: 4.9173 - recall_at_30: 0.4429 - ndcg_at_30: 0.1640 - regularization_loss: 0.0000e+00 - loss_batch: 4.9170 - val_loss: 4.9187 - val_recall_at_30: 0.3938 - val_ndcg_at_30: 0.1699 - val_regularization_loss: 0.0000e+00 - val_loss_batch: 3.8896\n",
      "Epoch 5/15\n",
      "2058/2058 [==============================] - 91s 44ms/step - loss: 4.6716 - recall_at_30: 0.5381 - ndcg_at_30: 0.2173 - regularization_loss: 0.0000e+00 - loss_batch: 4.6713 - val_loss: 4.9451 - val_recall_at_30: 0.4059 - val_ndcg_at_30: 0.1683 - val_regularization_loss: 0.0000e+00 - val_loss_batch: 3.8560\n",
      "Epoch 6/15\n",
      "2058/2058 [==============================] - 90s 44ms/step - loss: 4.6415 - recall_at_30: 0.5823 - ndcg_at_30: 0.2418 - regularization_loss: 0.0000e+00 - loss_batch: 4.6412 - val_loss: 4.9992 - val_recall_at_30: 0.4019 - val_ndcg_at_30: 0.1633 - val_regularization_loss: 0.0000e+00 - val_loss_batch: 3.8835\n",
      "Epoch 7/15\n",
      "2058/2058 [==============================] - 91s 44ms/step - loss: 4.3853 - recall_at_30: 0.6194 - ndcg_at_30: 0.2620 - regularization_loss: 0.0000e+00 - loss_batch: 4.3852 - val_loss: 5.0040 - val_recall_at_30: 0.4131 - val_ndcg_at_30: 0.1641 - val_regularization_loss: 0.0000e+00 - val_loss_batch: 3.8280\n",
      "Epoch 8/15\n",
      "2058/2058 [==============================] - 91s 44ms/step - loss: 4.1637 - recall_at_30: 0.6379 - ndcg_at_30: 0.2703 - regularization_loss: 0.0000e+00 - loss_batch: 4.1636 - val_loss: 5.0674 - val_recall_at_30: 0.4194 - val_ndcg_at_30: 0.1702 - val_regularization_loss: 0.0000e+00 - val_loss_batch: 3.8503\n",
      "Epoch 9/15\n",
      "2058/2058 [==============================] - 91s 44ms/step - loss: 4.3266 - recall_at_30: 0.6311 - ndcg_at_30: 0.2659 - regularization_loss: 0.0000e+00 - loss_batch: 4.3264 - val_loss: 5.1038 - val_recall_at_30: 0.4229 - val_ndcg_at_30: 0.1737 - val_regularization_loss: 0.0000e+00 - val_loss_batch: 3.8605\n",
      "Epoch 10/15\n",
      "2058/2058 [==============================] - 90s 44ms/step - loss: 4.2898 - recall_at_30: 0.6650 - ndcg_at_30: 0.2888 - regularization_loss: 0.0000e+00 - loss_batch: 4.2896 - val_loss: 5.1889 - val_recall_at_30: 0.4078 - val_ndcg_at_30: 0.1633 - val_regularization_loss: 0.0000e+00 - val_loss_batch: 4.0104\n",
      "Epoch 11/15\n",
      "2058/2058 [==============================] - 89s 43ms/step - loss: 4.1804 - recall_at_30: 0.6849 - ndcg_at_30: 0.2991 - regularization_loss: 0.0000e+00 - loss_batch: 4.1804 - val_loss: 5.1410 - val_recall_at_30: 0.4197 - val_ndcg_at_30: 0.1677 - val_regularization_loss: 0.0000e+00 - val_loss_batch: 3.9008\n",
      "Epoch 12/15\n",
      "2058/2058 [==============================] - 90s 43ms/step - loss: 4.0054 - recall_at_30: 0.6899 - ndcg_at_30: 0.2990 - regularization_loss: 0.0000e+00 - loss_batch: 4.0053 - val_loss: 5.3146 - val_recall_at_30: 0.4173 - val_ndcg_at_30: 0.1681 - val_regularization_loss: 0.0000e+00 - val_loss_batch: 4.0605\n",
      "Epoch 13/15\n",
      "2058/2058 [==============================] - 91s 44ms/step - loss: 4.0784 - recall_at_30: 0.6945 - ndcg_at_30: 0.3046 - regularization_loss: 0.0000e+00 - loss_batch: 4.0783 - val_loss: 5.3468 - val_recall_at_30: 0.4082 - val_ndcg_at_30: 0.1686 - val_regularization_loss: 0.0000e+00 - val_loss_batch: 4.0925\n",
      "Epoch 14/15\n",
      "2058/2058 [==============================] - 90s 44ms/step - loss: 4.0249 - recall_at_30: 0.7069 - ndcg_at_30: 0.3141 - regularization_loss: 0.0000e+00 - loss_batch: 4.0247 - val_loss: 5.3654 - val_recall_at_30: 0.4186 - val_ndcg_at_30: 0.1702 - val_regularization_loss: 0.0000e+00 - val_loss_batch: 4.0843\n",
      "Epoch 15/15\n",
      "2058/2058 [==============================] - 90s 44ms/step - loss: 3.8931 - recall_at_30: 0.7189 - ndcg_at_30: 0.3224 - regularization_loss: 0.0000e+00 - loss_batch: 3.8930 - val_loss: 5.4409 - val_recall_at_30: 0.4038 - val_ndcg_at_30: 0.1656 - val_regularization_loss: 0.0000e+00 - val_loss_batch: 4.1505\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f6c2ae8e850>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer=\"adam\", run_eagerly=False, metrics=[mm.RecallAt(30), mm.NDCGAt(30)])\n",
    "model.fit(train, validation_data=valid, batch_size=4000, epochs=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  35/3707 [..............................] - ETA: 11s - loss: 3.7486 - recall_at_30: 0.7791 - ndcg_at_30: 0.3592 - regularization_loss: 0.0000e+00 - loss_batch: 3.7486"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-03 16:21:29.298362: I tensorflow/core/common_runtime/executor.cc:1209] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n",
      "\t [[{{node Placeholder/_0}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3707/3707 [==============================] - 10s 3ms/step - loss: 4.0384 - recall_at_30: 0.7330 - ndcg_at_30: 0.3295 - regularization_loss: 0.0000e+00 - loss_batch: 4.0371\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'loss': 4.038394451141357,\n",
       " 'recall_at_30': 0.7511730194091797,\n",
       " 'ndcg_at_30': 0.3364556133747101,\n",
       " 'regularization_loss': 0.0,\n",
       " 'loss_batch': 1.5547443628311157}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(valid, batch_size=1000, return_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  34/3089 [..............................] - ETA: 9s - loss: 7.0239 - recall_at_30: 0.0353 - ndcg_at_30: 0.0112 - regularization_loss: 0.0000e+00 - loss_batch: 7.0239"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-02 16:03:29.295270: I tensorflow/core/common_runtime/executor.cc:1209] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n",
      "\t [[{{node Placeholder/_0}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3089/3089 [==============================] - 9s 3ms/step - loss: 7.0094 - recall_at_30: 0.0374 - ndcg_at_30: 0.0119 - regularization_loss: 0.0000e+00 - loss_batch: 7.0092\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'loss': 7.009433269500732,\n",
       " 'recall_at_30': 0.03781740367412567,\n",
       " 'ndcg_at_30': 0.012081457301974297,\n",
       " 'regularization_loss': 0.0,\n",
       " 'loss_batch': 6.611358165740967}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(valid, batch_size=1000, return_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
