{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NoSuchModuleError",
     "evalue": "Can't load plugin: sqlalchemy.dialects:db",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNoSuchModuleError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m table_name \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mtest_table\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m      8\u001b[0m connection_string \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mdb://$\u001b[39m\u001b[39m{\u001b[39;00mMYSQL_USER\u001b[39m}\u001b[39;00m\u001b[39m:$\u001b[39m\u001b[39m{\u001b[39;00mMYSQL_PASSWORD\u001b[39m}\u001b[39;00m\u001b[39m@db:3306/$\u001b[39m\u001b[39m{\u001b[39;00mMYSQL_DATABASE\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m----> 9\u001b[0m engine \u001b[39m=\u001b[39m create_engine(connection_string)\n\u001b[1;32m     11\u001b[0m \u001b[39m# Example DataFrame to save\u001b[39;00m\n\u001b[1;32m     12\u001b[0m data \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame({\u001b[39m'\u001b[39m\u001b[39mcolumn1\u001b[39m\u001b[39m'\u001b[39m: [\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m3\u001b[39m], \u001b[39m'\u001b[39m\u001b[39mcolumn2\u001b[39m\u001b[39m'\u001b[39m: [\u001b[39m'\u001b[39m\u001b[39mA\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mB\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mC\u001b[39m\u001b[39m'\u001b[39m]})\n",
      "File \u001b[0;32m<string>:2\u001b[0m, in \u001b[0;36mcreate_engine\u001b[0;34m(url, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/sqlalchemy/util/deprecations.py:281\u001b[0m, in \u001b[0;36mdeprecated_params.<locals>.decorate.<locals>.warned\u001b[0;34m(fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    274\u001b[0m     \u001b[39mif\u001b[39;00m m \u001b[39min\u001b[39;00m kwargs:\n\u001b[1;32m    275\u001b[0m         _warn_with_version(\n\u001b[1;32m    276\u001b[0m             messages[m],\n\u001b[1;32m    277\u001b[0m             versions[m],\n\u001b[1;32m    278\u001b[0m             version_warnings[m],\n\u001b[1;32m    279\u001b[0m             stacklevel\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m,\n\u001b[1;32m    280\u001b[0m         )\n\u001b[0;32m--> 281\u001b[0m \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/sqlalchemy/engine/create.py:552\u001b[0m, in \u001b[0;36mcreate_engine\u001b[0;34m(url, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m u \u001b[39m=\u001b[39m _url\u001b[39m.\u001b[39mmake_url(url)\n\u001b[1;32m    550\u001b[0m u, plugins, kwargs \u001b[39m=\u001b[39m u\u001b[39m.\u001b[39m_instantiate_plugins(kwargs)\n\u001b[0;32m--> 552\u001b[0m entrypoint \u001b[39m=\u001b[39m u\u001b[39m.\u001b[39;49m_get_entrypoint()\n\u001b[1;32m    553\u001b[0m _is_async \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39m_is_async\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m    554\u001b[0m \u001b[39mif\u001b[39;00m _is_async:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/sqlalchemy/engine/url.py:754\u001b[0m, in \u001b[0;36mURL._get_entrypoint\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    752\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    753\u001b[0m     name \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdrivername\u001b[39m.\u001b[39mreplace(\u001b[39m\"\u001b[39m\u001b[39m+\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 754\u001b[0m \u001b[39mcls\u001b[39m \u001b[39m=\u001b[39m registry\u001b[39m.\u001b[39;49mload(name)\n\u001b[1;32m    755\u001b[0m \u001b[39m# check for legacy dialects that\u001b[39;00m\n\u001b[1;32m    756\u001b[0m \u001b[39m# would return a module with 'dialect' as the\u001b[39;00m\n\u001b[1;32m    757\u001b[0m \u001b[39m# actual class\u001b[39;00m\n\u001b[1;32m    758\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    759\u001b[0m     \u001b[39mhasattr\u001b[39m(\u001b[39mcls\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mdialect\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    760\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mdialect, \u001b[39mtype\u001b[39m)\n\u001b[1;32m    761\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39missubclass\u001b[39m(\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mdialect, Dialect)\n\u001b[1;32m    762\u001b[0m ):\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/sqlalchemy/util/langhelpers.py:368\u001b[0m, in \u001b[0;36mPluginLoader.load\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    365\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimpls[name] \u001b[39m=\u001b[39m impl\u001b[39m.\u001b[39mload\n\u001b[1;32m    366\u001b[0m         \u001b[39mreturn\u001b[39;00m impl\u001b[39m.\u001b[39mload()\n\u001b[0;32m--> 368\u001b[0m \u001b[39mraise\u001b[39;00m exc\u001b[39m.\u001b[39mNoSuchModuleError(\n\u001b[1;32m    369\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mCan\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt load plugin: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m:\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroup, name)\n\u001b[1;32m    370\u001b[0m )\n",
      "\u001b[0;31mNoSuchModuleError\u001b[0m: Can't load plugin: sqlalchemy.dialects:db"
     ]
    }
   ],
   "source": [
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "import os\n",
    "MYSQL_USER = os.getenv('MYSQL_USER')\n",
    "MYSQL_PASSWORD = os.getenv('MYSQL_PASSWORD')\n",
    "MYSQL_DATABASE = os.getenv('MYSQL_DATABASE')\n",
    "table_name = 'test_table'\n",
    "connection_string = f\"mysql+mysqlconnector://${MYSQL_USER}:${MYSQL_PASSWORD}@db:3306/${MYSQL_DATABASE}\"\n",
    "engine = create_engine(connection_string)\n",
    "\n",
    "# Example DataFrame to save\n",
    "data = pd.DataFrame({'column1': [1, 2, 3], 'column2': ['A', 'B', 'C']})\n",
    "\n",
    "# Save the DataFrame to the MySQL table\n",
    "data.to_sql(table_name, engine, index=False, if_exists='replace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] minio url:  s3:9000\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from minio import Minio\n",
    "from minio.error import InvalidResponseError\n",
    "\n",
    "accessID = os.environ.get('AWS_ACCESS_KEY_ID')\n",
    "accessSecret =  os.environ.get('AWS_SECRET_ACCESS_KEY')\n",
    "minioUrl =  os.environ.get('MLFLOW_S3_ENDPOINT_URL')\n",
    "minioUrl = 'http://s3:9000'\n",
    "bucketName =  'ree'\n",
    "\n",
    "if accessID == None:\n",
    "    print('[!] AWS_ACCESS_KEY_ID environment variable is empty! run \\'source .env\\' to load it from the .env file')\n",
    "    exit(1)\n",
    "\n",
    "if accessSecret == None:\n",
    "    print('[!] AWS_SECRET_ACCESS_KEY environment variable is empty! run \\'source .env\\' to load it from the .env file')\n",
    "    exit(1)\n",
    "\n",
    "if minioUrl == None:\n",
    "    print('[!] MLFLOW_S3_ENDPOINT_URL environment variable is empty! run \\'source .env\\' to load it from the .env file')\n",
    "    exit(1)\n",
    "\n",
    "    \n",
    "if bucketName == None:\n",
    "    print('[!] AWS_BUCKET_NAME environment variable is empty! run \\'source .env\\' to load it from the .env file')\n",
    "    exit(1)\n",
    "\n",
    "minioUrlHostWithPort = minioUrl.split('//')[1]\n",
    "print('[*] minio url: ',minioUrlHostWithPort)\n",
    "\n",
    "s3Client = Minio(\n",
    "    minioUrlHostWithPort,\n",
    "    access_key=accessID,\n",
    "    secret_key=accessSecret,\n",
    "    secure=False\n",
    ")\n",
    "\n",
    "s3Client.make_bucket(bucketName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "# set url to your mlflow server\n",
    "# mlflow.set_tracking_uri(\"http://mlflow:5000\")\n",
    "mlflow.set_experiment(\"Default\")\n",
    "with mlflow.start_run():\n",
    "    mlflow.log_param(\"param1\", 5)\n",
    "    mlflow.log_metric(\"foo\", 1)\n",
    "    mlflow.log_metric(\"foo\", 2)\n",
    "    mlflow.log_metric(\"foo\", 3)\n",
    "    mlflow.log_artifact(\"00-skeleton.ipynb\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://mlflow:5000\n"
     ]
    }
   ],
   "source": [
    "!echo  $MLFLOW_TRACKING_URI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-05 13:40:15.589748: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-08-05 13:40:15.910564: I tensorflow/core/platform/cpu_feature_guard.cc:183] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE3 SSE4.1 SSE4.2 AVX, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Please fix your imports. Module tensorflow.python.training.tracking.data_structures has been moved to tensorflow.python.trackable.data_structures. The old module will be deleted in version 2.11.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-05 13:40:17.828875: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:1013] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-08-05 13:40:17.829232: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:1013] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-08-05 13:40:17.829330: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:1013] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]: sparse_operation_kit is imported\n",
      "WARNING:tensorflow:Please fix your imports. Module tensorflow.python.training.tracking.base has been moved to tensorflow.python.trackable.base. The old module will be deleted in version 2.11.\n",
      "[SOK INFO] Import /usr/local/lib/python3.8/dist-packages/merlin_sok-1.2.0-py3.8-linux-x86_64.egg/sparse_operation_kit/lib/libsok_experiment.so\n",
      "[SOK INFO] Import /usr/local/lib/python3.8/dist-packages/merlin_sok-1.2.0-py3.8-linux-x86_64.egg/sparse_operation_kit/lib/libsok_experiment.so\n",
      "[SOK INFO] Initialize finished, communication tool: horovod\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-05 13:40:18.377397: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:1013] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-08-05 13:40:18.377519: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:1013] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-08-05 13:40:18.377567: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:1013] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-08-05 13:40:18.410342: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:1013] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-08-05 13:40:18.410441: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:1013] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-08-05 13:40:18.410493: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:1013] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-08-05 13:40:18.410526: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2023-08-05 13:40:18.410543: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1638] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6011 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import nvtabular as nvt\n",
    "import cudf\n",
    "import config\n",
    "import os\n",
    "# ignore warnings\n",
    "from merlin.core.dispatch import get_lib\n",
    "from merlin.schema.tags import Tags\n",
    "\n",
    "import time\n",
    "import merlin.models.tf as mm\n",
    "# from dask.distributed import Client, wait\n",
    "# from dask_cuda import LocalCUDACluster\n",
    "# from dask.utils import parse_bytes\n",
    "\n",
    "from merlin.schema.tags import Tags\n",
    "from nvtabular.ops import *\n",
    "\n",
    "from merlin.schema.tags import Tags\n",
    "from merlin.io.dataset import Dataset\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_parquet(os.path.join(config.data_raw_dir,'transactions.parquet'))\n",
    "customers = pd.read_parquet(os.path.join(config.data_raw_dir,'customers.parquet'))\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "merged_ids = pd.concat([train['customer_id'], customers['customer_id']])\n",
    "encoder.fit(merged_ids)\n",
    "train['customer_id'] = encoder.transform(train['customer_id'])\n",
    "customers['customer_id'] = encoder.transform(customers['customer_id'])\n",
    "customers.to_parquet(os.path.join(config.data_processed_dir,'customers_enc.parquet'))\n",
    "del customers\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert  t_dat column to datetime\n",
    "train['t_dat'] = pd.to_datetime(train['t_dat'])\n",
    "train['week'] = 104 -(train['t_dat'].max() - train['t_dat']).dt.days // 7\n",
    "start = 60\n",
    "end = 90\n",
    "valid = train[train.week>end]\n",
    "train = train[(train.week>start) & (train.week<=end)]\n",
    "valid = valid[valid['customer_id'].isin(train['customer_id'])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shape (8231156, 6)\n",
      "valid shape (3706105, 6)\n"
     ]
    }
   ],
   "source": [
    "print('train shape', train.shape)\n",
    "print('valid shape', valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of common customers between train and valid:  408937\n",
      "Number of common items between train and valid:  31933\n",
      "Number of customers in train:  780121\n",
      "Number of customers in valid:  408937\n",
      "Number of items in train:  55238\n",
      "Number of items in valid:  42459\n",
      "Number of customers in valid but not in train:  0\n",
      "Number of items in valid but not in train:  10526\n",
      "train shape:  (8231156, 6)\n",
      "valid shape:  (3706105, 6)\n"
     ]
    }
   ],
   "source": [
    "# see how many 'customer_id' are common between the two datasets train and valid and also items 'article_id'\n",
    "common = set(train['customer_id']).intersection(set(valid['customer_id']))\n",
    "common_items = set(train['article_id']).intersection(set(valid['article_id']))\n",
    "print('Number of common customers between train and valid: ', len(common))\n",
    "print('Number of common items between train and valid: ', len(common_items))\n",
    "print('Number of customers in train: ', len(train['customer_id'].unique()))\n",
    "print('Number of customers in valid: ', len(valid['customer_id'].unique()))\n",
    "print('Number of items in train: ', len(train['article_id'].unique()))\n",
    "print('Number of items in valid: ', len(valid['article_id'].unique()))\n",
    "print('Number of customers in valid but not in train: ', len(set(valid['customer_id']) - set(train['customer_id'])))\n",
    "print('Number of items in valid but not in train: ', len(set(valid['article_id']) - set(train['article_id'])))\n",
    "print('train shape: ', train.shape)\n",
    "print('valid shape: ', valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = os.path.join(config.data_processed_dir,'train', 'transactions.parquet')\n",
    "train.to_parquet(save_dir)\n",
    "\n",
    "save_dir = os.path.join(config.data_processed_dir,'valid', 'transactions.parquet')\n",
    "valid.to_parquet(save_dir)\n",
    "del train, valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = nvt.Dataset(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = nvt.Dataset(os.path.join(config.data_processed_dir,'train', 'transactions.parquet'),engine=\"parquet\",  part_mem_fraction=0.1)\n",
    "valid = nvt.Dataset(os.path.join(config.data_processed_dir,'valid', 'transactions.parquet'),engine=\"parquet\", part_mem_fraction=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>t_dat</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>article_id</th>\n",
       "      <th>price</th>\n",
       "      <th>sales_channel_id</th>\n",
       "      <th>week</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-11-20</td>\n",
       "      <td>155</td>\n",
       "      <td>828652001</td>\n",
       "      <td>0.016932</td>\n",
       "      <td>1</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-11-20</td>\n",
       "      <td>330</td>\n",
       "      <td>408647009</td>\n",
       "      <td>0.016932</td>\n",
       "      <td>2</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-11-20</td>\n",
       "      <td>330</td>\n",
       "      <td>572797002</td>\n",
       "      <td>0.010153</td>\n",
       "      <td>2</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-11-20</td>\n",
       "      <td>330</td>\n",
       "      <td>634426008</td>\n",
       "      <td>0.042356</td>\n",
       "      <td>2</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-11-20</td>\n",
       "      <td>330</td>\n",
       "      <td>753421001</td>\n",
       "      <td>0.067780</td>\n",
       "      <td>2</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       t_dat  customer_id  article_id     price  sales_channel_id  week\n",
       "0 2019-11-20          155   828652001  0.016932                 1    61\n",
       "1 2019-11-20          330   408647009  0.016932                 2    61\n",
       "2 2019-11-20          330   572797002  0.010153                 2    61\n",
       "3 2019-11-20          330   634426008  0.042356                 2    61\n",
       "4 2019-11-20          330   753421001  0.067780                 2    61"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_id = [\"article_id\"] >> Categorify() >> TagAsItemID()\n",
    "customer_id = [\"customer_id\"] >> Categorify() >> TagAsUserID()\n",
    "price = [\"price\"] >> FillMissing(fill_val=0) >> Normalize() >> TagAsItemFeatures()\n",
    "len_session = [\"price\"] >> FillMissing(fill_val=0) >> Normalize() >> TagAsUserFeatures()\n",
    "\n",
    "\n",
    "features = article_id + customer_id + price + len_session\n",
    "\n",
    "wf = nvt.Workflow(features)\n",
    "wf.fit_transform(train).to_parquet(os.path.join(config.data_processed_dir, 'train_processed.parquet'))\n",
    "wf.transform(valid).to_parquet(os.path.join(config.data_processed_dir, 'validation_processed.parquet'))\n",
    "\n",
    "wf.save(os.path.join(config.data_processed_dir, 'workflow'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = Dataset(os.path.join(config.data_processed_dir, 'train_processed.parquet'))\n",
    "valid = Dataset(os.path.join(config.data_processed_dir, 'validation_processed.parquet'))\n",
    "schema = train.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>tags</th>\n",
       "      <th>dtype</th>\n",
       "      <th>is_list</th>\n",
       "      <th>is_ragged</th>\n",
       "      <th>properties.num_buckets</th>\n",
       "      <th>properties.freq_threshold</th>\n",
       "      <th>properties.max_size</th>\n",
       "      <th>properties.cat_path</th>\n",
       "      <th>properties.embedding_sizes.cardinality</th>\n",
       "      <th>properties.embedding_sizes.dimension</th>\n",
       "      <th>properties.domain.min</th>\n",
       "      <th>properties.domain.max</th>\n",
       "      <th>properties.domain.name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>article_id</td>\n",
       "      <td>(Tags.ID, Tags.CATEGORICAL, Tags.ITEM)</td>\n",
       "      <td>DType(name='int64', element_type=&lt;ElementType....</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>.//categories/unique.article_id.parquet</td>\n",
       "      <td>55241.0</td>\n",
       "      <td>512.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>55240.0</td>\n",
       "      <td>article_id</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>customer_id</td>\n",
       "      <td>(Tags.USER, Tags.CATEGORICAL, Tags.ID)</td>\n",
       "      <td>DType(name='int64', element_type=&lt;ElementType....</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>.//categories/unique.customer_id.parquet</td>\n",
       "      <td>780124.0</td>\n",
       "      <td>512.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>780123.0</td>\n",
       "      <td>customer_id</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>price</td>\n",
       "      <td>(Tags.CONTINUOUS, Tags.USER, Tags.ITEM)</td>\n",
       "      <td>DType(name='float64', element_type=&lt;ElementTyp...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "[{'name': 'article_id', 'tags': {<Tags.ID: 'id'>, <Tags.CATEGORICAL: 'categorical'>, <Tags.ITEM: 'item'>}, 'properties': {'num_buckets': None, 'freq_threshold': 0.0, 'max_size': 0.0, 'cat_path': './/categories/unique.article_id.parquet', 'embedding_sizes': {'cardinality': 55241.0, 'dimension': 512.0}, 'domain': {'min': 0, 'max': 55240, 'name': 'article_id'}}, 'dtype': DType(name='int64', element_type=<ElementType.Int: 'int'>, element_size=64, element_unit=None, signed=True, shape=Shape(dims=(Dimension(min=0, max=None),))), 'is_list': False, 'is_ragged': False}, {'name': 'customer_id', 'tags': {<Tags.USER: 'user'>, <Tags.CATEGORICAL: 'categorical'>, <Tags.ID: 'id'>}, 'properties': {'num_buckets': None, 'freq_threshold': 0.0, 'max_size': 0.0, 'cat_path': './/categories/unique.customer_id.parquet', 'embedding_sizes': {'cardinality': 780124.0, 'dimension': 512.0}, 'domain': {'min': 0, 'max': 780123, 'name': 'customer_id'}}, 'dtype': DType(name='int64', element_type=<ElementType.Int: 'int'>, element_size=64, element_unit=None, signed=True, shape=Shape(dims=(Dimension(min=0, max=None),))), 'is_list': False, 'is_ragged': False}, {'name': 'price', 'tags': {<Tags.CONTINUOUS: 'continuous'>, <Tags.USER: 'user'>, <Tags.ITEM: 'item'>}, 'properties': {}, 'dtype': DType(name='float64', element_type=<ElementType.Float: 'float'>, element_size=64, element_unit=None, signed=True, shape=Shape(dims=(Dimension(min=0, max=None),))), 'is_list': False, 'is_ragged': False}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schema = schema.select_by_tag([Tags.ITEM_ID, Tags.USER_ID, Tags.ITEM, Tags.USER])\n",
    "label_names = schema.select_by_tag(Tags.TARGET).column_names\n",
    "label_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = mm.TwoTowerModel(\n",
    "    schema,\n",
    "    query_tower=mm.MLPBlock([512, 256, 128, 64], no_activation_last_layer=True),\n",
    "    samplers=[mm.InBatchSampler()],\n",
    "    embedding_options=mm.EmbeddingOptions(infer_embedding_sizes=True),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-03 15:57:15.260244: I tensorflow/core/common_runtime/executor.cc:1209] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n",
      "\t [[{{node Placeholder/_0}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2057/2058 [============================>.] - ETA: 0s - loss: 5.4461 - recall_at_30: 0.3217 - ndcg_at_30: 0.1081 - regularization_loss: 0.0000e+00 - loss_batch: 5.4461"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-03 15:58:34.897051: I tensorflow/core/common_runtime/executor.cc:1209] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n",
      "\t [[{{node Placeholder/_0}}]]\n",
      "The sampler InBatchSampler returned no samples for this batch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2058/2058 [==============================] - 92s 44ms/step - loss: 5.4460 - recall_at_30: 0.3217 - ndcg_at_30: 0.1082 - regularization_loss: 0.0000e+00 - loss_batch: 5.4458 - val_loss: 5.3480 - val_recall_at_30: 0.3605 - val_ndcg_at_30: 0.1344 - val_regularization_loss: 0.0000e+00 - val_loss_batch: 4.1751\n",
      "Epoch 2/15\n",
      "2058/2058 [==============================] - 90s 44ms/step - loss: 5.2267 - recall_at_30: 0.3987 - ndcg_at_30: 0.1390 - regularization_loss: 0.0000e+00 - loss_batch: 5.2265 - val_loss: 5.1810 - val_recall_at_30: 0.3442 - val_ndcg_at_30: 0.1284 - val_regularization_loss: 0.0000e+00 - val_loss_batch: 4.1086\n",
      "Epoch 3/15\n",
      "2058/2058 [==============================] - 90s 44ms/step - loss: 4.8108 - recall_at_30: 0.4268 - ndcg_at_30: 0.1526 - regularization_loss: 0.0000e+00 - loss_batch: 4.8106 - val_loss: 4.9178 - val_recall_at_30: 0.3810 - val_ndcg_at_30: 0.1519 - val_regularization_loss: 0.0000e+00 - val_loss_batch: 3.8806\n",
      "Epoch 4/15\n",
      "2058/2058 [==============================] - 91s 44ms/step - loss: 4.9173 - recall_at_30: 0.4429 - ndcg_at_30: 0.1640 - regularization_loss: 0.0000e+00 - loss_batch: 4.9170 - val_loss: 4.9187 - val_recall_at_30: 0.3938 - val_ndcg_at_30: 0.1699 - val_regularization_loss: 0.0000e+00 - val_loss_batch: 3.8896\n",
      "Epoch 5/15\n",
      "2058/2058 [==============================] - 91s 44ms/step - loss: 4.6716 - recall_at_30: 0.5381 - ndcg_at_30: 0.2173 - regularization_loss: 0.0000e+00 - loss_batch: 4.6713 - val_loss: 4.9451 - val_recall_at_30: 0.4059 - val_ndcg_at_30: 0.1683 - val_regularization_loss: 0.0000e+00 - val_loss_batch: 3.8560\n",
      "Epoch 6/15\n",
      "2058/2058 [==============================] - 90s 44ms/step - loss: 4.6415 - recall_at_30: 0.5823 - ndcg_at_30: 0.2418 - regularization_loss: 0.0000e+00 - loss_batch: 4.6412 - val_loss: 4.9992 - val_recall_at_30: 0.4019 - val_ndcg_at_30: 0.1633 - val_regularization_loss: 0.0000e+00 - val_loss_batch: 3.8835\n",
      "Epoch 7/15\n",
      "2058/2058 [==============================] - 91s 44ms/step - loss: 4.3853 - recall_at_30: 0.6194 - ndcg_at_30: 0.2620 - regularization_loss: 0.0000e+00 - loss_batch: 4.3852 - val_loss: 5.0040 - val_recall_at_30: 0.4131 - val_ndcg_at_30: 0.1641 - val_regularization_loss: 0.0000e+00 - val_loss_batch: 3.8280\n",
      "Epoch 8/15\n",
      "2058/2058 [==============================] - 91s 44ms/step - loss: 4.1637 - recall_at_30: 0.6379 - ndcg_at_30: 0.2703 - regularization_loss: 0.0000e+00 - loss_batch: 4.1636 - val_loss: 5.0674 - val_recall_at_30: 0.4194 - val_ndcg_at_30: 0.1702 - val_regularization_loss: 0.0000e+00 - val_loss_batch: 3.8503\n",
      "Epoch 9/15\n",
      "2058/2058 [==============================] - 91s 44ms/step - loss: 4.3266 - recall_at_30: 0.6311 - ndcg_at_30: 0.2659 - regularization_loss: 0.0000e+00 - loss_batch: 4.3264 - val_loss: 5.1038 - val_recall_at_30: 0.4229 - val_ndcg_at_30: 0.1737 - val_regularization_loss: 0.0000e+00 - val_loss_batch: 3.8605\n",
      "Epoch 10/15\n",
      "2058/2058 [==============================] - 90s 44ms/step - loss: 4.2898 - recall_at_30: 0.6650 - ndcg_at_30: 0.2888 - regularization_loss: 0.0000e+00 - loss_batch: 4.2896 - val_loss: 5.1889 - val_recall_at_30: 0.4078 - val_ndcg_at_30: 0.1633 - val_regularization_loss: 0.0000e+00 - val_loss_batch: 4.0104\n",
      "Epoch 11/15\n",
      "2058/2058 [==============================] - 89s 43ms/step - loss: 4.1804 - recall_at_30: 0.6849 - ndcg_at_30: 0.2991 - regularization_loss: 0.0000e+00 - loss_batch: 4.1804 - val_loss: 5.1410 - val_recall_at_30: 0.4197 - val_ndcg_at_30: 0.1677 - val_regularization_loss: 0.0000e+00 - val_loss_batch: 3.9008\n",
      "Epoch 12/15\n",
      "2058/2058 [==============================] - 90s 43ms/step - loss: 4.0054 - recall_at_30: 0.6899 - ndcg_at_30: 0.2990 - regularization_loss: 0.0000e+00 - loss_batch: 4.0053 - val_loss: 5.3146 - val_recall_at_30: 0.4173 - val_ndcg_at_30: 0.1681 - val_regularization_loss: 0.0000e+00 - val_loss_batch: 4.0605\n",
      "Epoch 13/15\n",
      "2058/2058 [==============================] - 91s 44ms/step - loss: 4.0784 - recall_at_30: 0.6945 - ndcg_at_30: 0.3046 - regularization_loss: 0.0000e+00 - loss_batch: 4.0783 - val_loss: 5.3468 - val_recall_at_30: 0.4082 - val_ndcg_at_30: 0.1686 - val_regularization_loss: 0.0000e+00 - val_loss_batch: 4.0925\n",
      "Epoch 14/15\n",
      "2058/2058 [==============================] - 90s 44ms/step - loss: 4.0249 - recall_at_30: 0.7069 - ndcg_at_30: 0.3141 - regularization_loss: 0.0000e+00 - loss_batch: 4.0247 - val_loss: 5.3654 - val_recall_at_30: 0.4186 - val_ndcg_at_30: 0.1702 - val_regularization_loss: 0.0000e+00 - val_loss_batch: 4.0843\n",
      "Epoch 15/15\n",
      "2058/2058 [==============================] - 90s 44ms/step - loss: 3.8931 - recall_at_30: 0.7189 - ndcg_at_30: 0.3224 - regularization_loss: 0.0000e+00 - loss_batch: 3.8930 - val_loss: 5.4409 - val_recall_at_30: 0.4038 - val_ndcg_at_30: 0.1656 - val_regularization_loss: 0.0000e+00 - val_loss_batch: 4.1505\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f6c2ae8e850>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer=\"adam\", run_eagerly=False, metrics=[mm.RecallAt(30), mm.NDCGAt(30)])\n",
    "model.fit(train, validation_data=valid, batch_size=4000, epochs=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  35/3707 [..............................] - ETA: 11s - loss: 3.7486 - recall_at_30: 0.7791 - ndcg_at_30: 0.3592 - regularization_loss: 0.0000e+00 - loss_batch: 3.7486"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-03 16:21:29.298362: I tensorflow/core/common_runtime/executor.cc:1209] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n",
      "\t [[{{node Placeholder/_0}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3707/3707 [==============================] - 10s 3ms/step - loss: 4.0384 - recall_at_30: 0.7330 - ndcg_at_30: 0.3295 - regularization_loss: 0.0000e+00 - loss_batch: 4.0371\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'loss': 4.038394451141357,\n",
       " 'recall_at_30': 0.7511730194091797,\n",
       " 'ndcg_at_30': 0.3364556133747101,\n",
       " 'regularization_loss': 0.0,\n",
       " 'loss_batch': 1.5547443628311157}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(valid, batch_size=1000, return_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  34/3089 [..............................] - ETA: 9s - loss: 7.0239 - recall_at_30: 0.0353 - ndcg_at_30: 0.0112 - regularization_loss: 0.0000e+00 - loss_batch: 7.0239"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-02 16:03:29.295270: I tensorflow/core/common_runtime/executor.cc:1209] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n",
      "\t [[{{node Placeholder/_0}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3089/3089 [==============================] - 9s 3ms/step - loss: 7.0094 - recall_at_30: 0.0374 - ndcg_at_30: 0.0119 - regularization_loss: 0.0000e+00 - loss_batch: 7.0092\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'loss': 7.009433269500732,\n",
       " 'recall_at_30': 0.03781740367412567,\n",
       " 'ndcg_at_30': 0.012081457301974297,\n",
       " 'regularization_loss': 0.0,\n",
       " 'loss_batch': 6.611358165740967}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(valid, batch_size=1000, return_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
