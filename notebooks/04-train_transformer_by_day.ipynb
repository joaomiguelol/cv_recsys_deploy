{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-09 17:04:59.758902: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-09-09 17:04:59.783039: I tensorflow/core/platform/cpu_feature_guard.cc:183] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE3 SSE4.1 SSE4.2 AVX, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/usr/local/lib/python3.8/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Please fix your imports. Module tensorflow.python.training.tracking.data_structures has been moved to tensorflow.python.trackable.data_structures. The old module will be deleted in version 2.11.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-09 17:05:01.313161: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:1013] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-09-09 17:05:01.313414: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:1013] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-09-09 17:05:01.313469: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:1013] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]: sparse_operation_kit is imported\n",
      "WARNING:tensorflow:Please fix your imports. Module tensorflow.python.training.tracking.base has been moved to tensorflow.python.trackable.base. The old module will be deleted in version 2.11.\n",
      "[SOK INFO] Import /usr/local/lib/python3.8/dist-packages/merlin_sok-1.2.0-py3.8-linux-x86_64.egg/sparse_operation_kit/lib/libsok_experiment.so\n",
      "[SOK INFO] Import /usr/local/lib/python3.8/dist-packages/merlin_sok-1.2.0-py3.8-linux-x86_64.egg/sparse_operation_kit/lib/libsok_experiment.so\n",
      "[SOK INFO] Initialize finished, communication tool: horovod\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-09 17:05:01.577471: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:1013] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-09-09 17:05:01.577600: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:1013] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-09-09 17:05:01.577650: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:1013] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-09-09 17:05:01.885371: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:1013] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-09-09 17:05:01.885481: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:1013] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-09-09 17:05:01.885535: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:1013] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-09-09 17:05:01.885568: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2023-09-09 17:05:01.885586: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1638] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6011 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "from transformers4rec import torch as tr\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import nvtabular as nvt\n",
    "import cudf\n",
    "import config\n",
    "import os\n",
    "# ignore warnings\n",
    "from merlin.core.dispatch import get_lib\n",
    "from merlin.schema.tags import Tags\n",
    "\n",
    "import time\n",
    "import merlin.models.tf as mm\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "from merlin.schema.tags import Tags\n",
    "from nvtabular.ops import *\n",
    "\n",
    "from merlin.schema.tags import Tags\n",
    "from merlin.io.dataset import Dataset\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = os.path.join(config.data_final,'time/',)\n",
    "train = Dataset(os.path.join(config.data_final,'transformer4rec','train.parquet'),engine=\"parquet\",  part_size=\"1GB\")\n",
    "# valid = Dataset(os.path.join(config.data_final,'transformer4rec', 'valid.parquet/*.parquet'),engine=\"parquet\", part_size=\"1GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>tags</th>\n",
       "      <th>dtype</th>\n",
       "      <th>is_list</th>\n",
       "      <th>is_ragged</th>\n",
       "      <th>properties.value_count.min</th>\n",
       "      <th>properties.value_count.max</th>\n",
       "      <th>properties.num_buckets</th>\n",
       "      <th>properties.freq_threshold</th>\n",
       "      <th>properties.max_size</th>\n",
       "      <th>properties.cat_path</th>\n",
       "      <th>properties.embedding_sizes.cardinality</th>\n",
       "      <th>properties.embedding_sizes.dimension</th>\n",
       "      <th>properties.domain.min</th>\n",
       "      <th>properties.domain.max</th>\n",
       "      <th>properties.domain.name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>price_list</td>\n",
       "      <td>(Tags.SEQUENCE, Tags.LIST, Tags.CONTINUOUS, Ta...</td>\n",
       "      <td>DType(name='float64', element_type=&lt;ElementTyp...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>article_id_count_norm</td>\n",
       "      <td>(Tags.CONTINUOUS, Tags.ITEM)</td>\n",
       "      <td>DType(name='float64', element_type=&lt;ElementTyp...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>.//categories/unique.article_id.parquet</td>\n",
       "      <td>55241.0</td>\n",
       "      <td>512.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>55240.0</td>\n",
       "      <td>article_id</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>price_max_norm</td>\n",
       "      <td>(Tags.CONTINUOUS, Tags.ITEM)</td>\n",
       "      <td>DType(name='float64', element_type=&lt;ElementTyp...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>price_mean_norm</td>\n",
       "      <td>(Tags.CONTINUOUS, Tags.ITEM)</td>\n",
       "      <td>DType(name='float64', element_type=&lt;ElementTyp...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>price_std_norm</td>\n",
       "      <td>(Tags.CONTINUOUS, Tags.ITEM)</td>\n",
       "      <td>DType(name='float64', element_type=&lt;ElementTyp...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "[{'name': 'price_list', 'tags': {<Tags.SEQUENCE: 'sequence'>, <Tags.LIST: 'list'>, <Tags.CONTINUOUS: 'continuous'>, <Tags.ITEM: 'item'>}, 'properties': {'value_count': {'min': 0, 'max': 30}}, 'dtype': DType(name='float64', element_type=<ElementType.Float: 'float'>, element_size=64, element_unit=None, signed=True, shape=Shape(dims=(Dimension(min=0, max=None), Dimension(min=0, max=30)))), 'is_list': True, 'is_ragged': True}, {'name': 'article_id_count_norm', 'tags': {<Tags.CONTINUOUS: 'continuous'>, <Tags.ITEM: 'item'>}, 'properties': {'num_buckets': None, 'freq_threshold': 0.0, 'max_size': 0.0, 'cat_path': './/categories/unique.article_id.parquet', 'embedding_sizes': {'cardinality': 55241.0, 'dimension': 512.0}, 'domain': {'min': 0.0, 'max': 55240, 'name': 'article_id'}}, 'dtype': DType(name='float64', element_type=<ElementType.Float: 'float'>, element_size=64, element_unit=None, signed=True, shape=Shape(dims=(Dimension(min=0, max=None),))), 'is_list': False, 'is_ragged': False}, {'name': 'price_max_norm', 'tags': {<Tags.CONTINUOUS: 'continuous'>, <Tags.ITEM: 'item'>}, 'properties': {}, 'dtype': DType(name='float64', element_type=<ElementType.Float: 'float'>, element_size=64, element_unit=None, signed=True, shape=Shape(dims=(Dimension(min=0, max=None),))), 'is_list': False, 'is_ragged': False}, {'name': 'price_mean_norm', 'tags': {<Tags.CONTINUOUS: 'continuous'>, <Tags.ITEM: 'item'>}, 'properties': {}, 'dtype': DType(name='float64', element_type=<ElementType.Float: 'float'>, element_size=64, element_unit=None, signed=True, shape=Shape(dims=(Dimension(min=0, max=None),))), 'is_list': False, 'is_ragged': False}, {'name': 'price_std_norm', 'tags': {<Tags.CONTINUOUS: 'continuous'>, <Tags.ITEM: 'item'>}, 'properties': {}, 'dtype': DType(name='float64', element_type=<ElementType.Float: 'float'>, element_size=64, element_unit=None, signed=True, shape=Shape(dims=(Dimension(min=0, max=None),))), 'is_list': False, 'is_ragged': False}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schema = train.schema.select_by_tag(Tags.CONTINUOUS)\n",
    "schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sequence_length, d_model = 64, 64\n",
    "# Define input module to process tabular input-features and to prepare masked inputs\n",
    "input_module = tr.TabularSequenceFeatures.from_schema(\n",
    "    train.schema,\n",
    "    max_sequence_length=max_sequence_length,\n",
    "    continuous_projection=320,\n",
    "    aggregation=\"concat\",\n",
    "    d_output=d_model,\n",
    "    masking=\"mlm\",\n",
    ")\n",
    "# Define Next item prediction-task \n",
    "prediction_task = tr.NextItemPredictionTask(weight_tying=True)\n",
    "\n",
    "# Define the config of the XLNet Transformer architecture\n",
    "transformer_config = tr.XLNetConfig.build(\n",
    "    d_model=d_model, n_head=8, n_layer=2, total_seq_length=max_sequence_length\n",
    ")\n",
    "\n",
    "# Get the end-to-end model \n",
    "model = transformer_config.to_torch_model(input_module, prediction_task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = tr.trainer.T4RecTrainingArguments(\n",
    "            output_dir=\"./tmp\",\n",
    "            max_sequence_length=20,\n",
    "            data_loader_engine='merlin',\n",
    "            num_train_epochs=10, \n",
    "            dataloader_drop_last=False,\n",
    "            per_device_train_batch_size = 500,\n",
    "            per_device_eval_batch_size = 500,\n",
    "            learning_rate=0.0005,\n",
    "            fp16=True,\n",
    "            report_to = [],\n",
    "            logging_steps=200\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cuda_amp half precision backend\n"
     ]
    }
   ],
   "source": [
    "recsys_trainer = tr.Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    schema=schema,\n",
    "    compute_metrics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/workspace/data/final/transformer4rec/time/'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/workspace/notebooks/04-train_transformer_by_day.ipynb Cell 7\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f72656373797332332d6d65726c696e2d31227d/workspace/notebooks/04-train_transformer_by_day.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mdir\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f72656373797332332d6d65726c696e2d31227d/workspace/notebooks/04-train_transformer_by_day.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m os\u001b[39m.\u001b[39;49mlistdir(\u001b[39mdir\u001b[39;49m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/workspace/data/final/transformer4rec/time/'"
     ]
    }
   ],
   "source": [
    "dir\n",
    "os.listdir(dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Launch training for day 61: *****\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "paths_or_dataset list must contain at least one filename",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/workspace/notebooks/04-train_transformer_by_day.ipynb Cell 7\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f72656373797332332d6d65726c696e2d31227d/workspace/notebooks/04-train_transformer_by_day.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers4rec\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mexamples_utils\u001b[39;00m \u001b[39mimport\u001b[39;00m fit_and_evaluate\n\u001b[0;32m----> <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f72656373797332332d6d65726c696e2d31227d/workspace/notebooks/04-train_transformer_by_day.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m OT_results \u001b[39m=\u001b[39m fit_and_evaluate(recsys_trainer, start_time_index\u001b[39m=\u001b[39;49m\u001b[39m61\u001b[39;49m , end_time_index\u001b[39m=\u001b[39;49m\u001b[39m68\u001b[39;49m ,input_dir\u001b[39m=\u001b[39;49m\u001b[39mdir\u001b[39;49m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers4rec/torch/utils/examples_utils.py:77\u001b[0m, in \u001b[0;36mfit_and_evaluate\u001b[0;34m(trainer, start_time_index, end_time_index, input_dir)\u001b[0m\n\u001b[1;32m     75\u001b[0m trainer\u001b[39m.\u001b[39mtrain_dataset_or_path \u001b[39m=\u001b[39m train_paths\n\u001b[1;32m     76\u001b[0m trainer\u001b[39m.\u001b[39mreset_lr_scheduler()\n\u001b[0;32m---> 77\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n\u001b[1;32m     79\u001b[0m \u001b[39m# 3. Evaluate on valid data of time_index+1\u001b[39;00m\n\u001b[1;32m     80\u001b[0m trainer\u001b[39m.\u001b[39meval_dataset_or_path \u001b[39m=\u001b[39m eval_paths\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/trainer.py:1543\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1538\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_wrapped \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\n\u001b[1;32m   1540\u001b[0m inner_training_loop \u001b[39m=\u001b[39m find_executable_batch_size(\n\u001b[1;32m   1541\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inner_training_loop, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_batch_size, args\u001b[39m.\u001b[39mauto_find_batch_size\n\u001b[1;32m   1542\u001b[0m )\n\u001b[0;32m-> 1543\u001b[0m \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   1544\u001b[0m     args\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m   1545\u001b[0m     resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[1;32m   1546\u001b[0m     trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[1;32m   1547\u001b[0m     ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[1;32m   1548\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/trainer.py:1555\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1553\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_batch_size \u001b[39m=\u001b[39m batch_size\n\u001b[1;32m   1554\u001b[0m \u001b[39m# Data loader and number of training steps\u001b[39;00m\n\u001b[0;32m-> 1555\u001b[0m train_dataloader \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_train_dataloader()\n\u001b[1;32m   1557\u001b[0m \u001b[39m# Setting up training control variables:\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[39m# number of training epochs: num_train_epochs\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[39m# number of training steps per epoch: num_update_steps_per_epoch\u001b[39;00m\n\u001b[1;32m   1560\u001b[0m \u001b[39m# total number of training steps to execute: max_steps\u001b[39;00m\n\u001b[1;32m   1561\u001b[0m total_train_batch_size \u001b[39m=\u001b[39m args\u001b[39m.\u001b[39mtrain_batch_size \u001b[39m*\u001b[39m args\u001b[39m.\u001b[39mgradient_accumulation_steps \u001b[39m*\u001b[39m args\u001b[39m.\u001b[39mworld_size\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers4rec/torch/trainer.py:151\u001b[0m, in \u001b[0;36mTrainer.get_train_dataloader\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    147\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_dataloader\n\u001b[1;32m    149\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mschema \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m, \u001b[39m\"\u001b[39m\u001b[39mschema is required to generate Train Dataloader\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 151\u001b[0m \u001b[39mreturn\u001b[39;00m T4RecDataLoader\u001b[39m.\u001b[39;49mparse(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49margs\u001b[39m.\u001b[39;49mdata_loader_engine)\u001b[39m.\u001b[39;49mfrom_schema(\n\u001b[1;32m    152\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mschema,\n\u001b[1;32m    153\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_dataset_or_path,\n\u001b[1;32m    154\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49margs\u001b[39m.\u001b[39;49mper_device_train_batch_size,\n\u001b[1;32m    155\u001b[0m     max_sequence_length\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49margs\u001b[39m.\u001b[39;49mmax_sequence_length,\n\u001b[1;32m    156\u001b[0m     drop_last\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49margs\u001b[39m.\u001b[39;49mdataloader_drop_last,\n\u001b[1;32m    157\u001b[0m     shuffle\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    158\u001b[0m     shuffle_buffer_size\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49margs\u001b[39m.\u001b[39;49mshuffle_buffer_size,\n\u001b[1;32m    159\u001b[0m     global_rank\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlocal_rank,\n\u001b[1;32m    160\u001b[0m     global_size\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mglobal_size,\n\u001b[1;32m    161\u001b[0m     device\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdevice,\n\u001b[1;32m    162\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers4rec/torch/utils/data_utils.py:472\u001b[0m, in \u001b[0;36mMerlinDataLoader.from_schema\u001b[0;34m(cls, schema, paths_or_dataset, batch_size, max_sequence_length, continuous_features, categorical_features, list_features, targets, collate_fn, shuffle, buffer_size, parts_per_chunk, transforms, **kwargs)\u001b[0m\n\u001b[1;32m    468\u001b[0m list_features \u001b[39m=\u001b[39m list_features \u001b[39mor\u001b[39;00m schema\u001b[39m.\u001b[39mselect_by_tag(Tags\u001b[39m.\u001b[39mLIST)\u001b[39m.\u001b[39mcolumn_names\n\u001b[1;32m    469\u001b[0m schema \u001b[39m=\u001b[39m schema\u001b[39m.\u001b[39mselect_by_name(\n\u001b[1;32m    470\u001b[0m     categorical_features \u001b[39m+\u001b[39m continuous_features \u001b[39m+\u001b[39m targets \u001b[39m+\u001b[39m list_features\n\u001b[1;32m    471\u001b[0m )\n\u001b[0;32m--> 472\u001b[0m loader \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m(\n\u001b[1;32m    473\u001b[0m     paths_or_dataset,\n\u001b[1;32m    474\u001b[0m     batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[1;32m    475\u001b[0m     labels\u001b[39m=\u001b[39;49mtargets,\n\u001b[1;32m    476\u001b[0m     max_sequence_length\u001b[39m=\u001b[39;49mmax_sequence_length,\n\u001b[1;32m    477\u001b[0m     cats\u001b[39m=\u001b[39;49mcategorical_features,\n\u001b[1;32m    478\u001b[0m     conts\u001b[39m=\u001b[39;49mcontinuous_features,\n\u001b[1;32m    479\u001b[0m     lists\u001b[39m=\u001b[39;49mlist_features,\n\u001b[1;32m    480\u001b[0m     collate_fn\u001b[39m=\u001b[39;49mcollate_fn,\n\u001b[1;32m    481\u001b[0m     engine\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mparquet\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    482\u001b[0m     shuffle\u001b[39m=\u001b[39;49mshuffle,\n\u001b[1;32m    483\u001b[0m     buffer_size\u001b[39m=\u001b[39;49mbuffer_size,  \u001b[39m# how many batches to load at once\u001b[39;49;00m\n\u001b[1;32m    484\u001b[0m     parts_per_chunk\u001b[39m=\u001b[39;49mparts_per_chunk,\n\u001b[1;32m    485\u001b[0m     schema\u001b[39m=\u001b[39;49mschema,\n\u001b[1;32m    486\u001b[0m     transforms\u001b[39m=\u001b[39;49mtransforms,\n\u001b[1;32m    487\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    488\u001b[0m )\n\u001b[1;32m    490\u001b[0m \u001b[39mreturn\u001b[39;00m loader\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers4rec/torch/utils/data_utils.py:320\u001b[0m, in \u001b[0;36mMerlinDataLoader.__init__\u001b[0;34m(self, paths_or_dataset, batch_size, max_sequence_length, conts, cats, labels, lists, collate_fn, engine, buffer_size, reader_kwargs, shuffle, seed_fn, parts_per_chunk, device, global_size, global_rank, drop_last, schema, row_groups_per_part, transforms, **kwargs)\u001b[0m\n\u001b[1;32m    318\u001b[0m reader_kwargs \u001b[39m=\u001b[39m reader_kwargs \u001b[39mor\u001b[39;00m {}\n\u001b[1;32m    319\u001b[0m reader_kwargs[\u001b[39m\"\u001b[39m\u001b[39mrow_groups_per_part\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m row_groups_per_part\n\u001b[0;32m--> 320\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mset_dataset(buffer_size, engine, reader_kwargs, schema\u001b[39m=\u001b[39;49mschema)\n\u001b[1;32m    322\u001b[0m \u001b[39mif\u001b[39;00m (global_rank \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m) \u001b[39mand\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39mnpartitions \u001b[39m<\u001b[39m global_size):\n\u001b[1;32m    323\u001b[0m     logger\u001b[39m.\u001b[39mwarning(\n\u001b[1;32m    324\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mUserWarning: User is advised to repartition the parquet file before training \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    325\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mso npartitions>=global_size. Cudf or pandas can be used for repartitioning \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    329\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mby number of GPUs to be used (eg. 2 or 4 partitions, if 2 GPUs will be used).\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    330\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers4rec/torch/utils/data_utils.py:417\u001b[0m, in \u001b[0;36mMerlinDataLoader.set_dataset\u001b[0;34m(self, buffer_size, engine, reader_kwargs, schema)\u001b[0m\n\u001b[1;32m    416\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mset_dataset\u001b[39m(\u001b[39mself\u001b[39m, buffer_size, engine, reader_kwargs, schema\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m--> 417\u001b[0m     dataset \u001b[39m=\u001b[39m validate_dataset(\n\u001b[1;32m    418\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpaths_or_dataset,\n\u001b[1;32m    419\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbatch_size,\n\u001b[1;32m    420\u001b[0m         buffer_size,\n\u001b[1;32m    421\u001b[0m         engine,\n\u001b[1;32m    422\u001b[0m         reader_kwargs,\n\u001b[1;32m    423\u001b[0m     )\n\u001b[1;32m    424\u001b[0m     \u001b[39mif\u001b[39;00m schema:\n\u001b[1;32m    425\u001b[0m         \u001b[39m# set the dataset schema based on the provided one to keep all original information\u001b[39;00m\n\u001b[1;32m    426\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(schema, Schema):\n\u001b[1;32m    427\u001b[0m             \u001b[39m# convert merlin-standardlib schema to merlin-core schema\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/merlin/models/utils/misc_utils.py:220\u001b[0m, in \u001b[0;36mvalidate_dataset\u001b[0;34m(paths_or_dataset, batch_size, buffer_size, engine, reader_kwargs)\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(files, \u001b[39mlist\u001b[39m)\n\u001b[1;32m    219\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(files) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m--> 220\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(_is_empty_msg)\n\u001b[1;32m    222\u001b[0m \u001b[39m# implement buffer size logic\u001b[39;00m\n\u001b[1;32m    223\u001b[0m \u001b[39m# TODO: IMPORTANT\u001b[39;00m\n\u001b[1;32m    224\u001b[0m \u001b[39m# should we divide everything by 3 to account\u001b[39;00m\n\u001b[1;32m    225\u001b[0m \u001b[39m# for extra copies laying around due to asynchronicity?\u001b[39;00m\n\u001b[1;32m    226\u001b[0m reader_kwargs \u001b[39m=\u001b[39m reader_kwargs \u001b[39mor\u001b[39;00m {}\n",
      "\u001b[0;31mValueError\u001b[0m: paths_or_dataset list must contain at least one filename"
     ]
    }
   ],
   "source": [
    "from transformers4rec.torch.utils.examples_utils import fit_and_evaluate\n",
    "\n",
    "OT_results = fit_and_evaluate(recsys_trainer, start_time_index=61 , end_time_index=68 ,input_dir=dir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
